<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>097计算机视觉领域的深度学习模型二VGGGoogleNet - Docs</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="" /><meta name="description" content="147 | 计算机视觉领域的深度学习模型（二）：VGG &amp; GoogleNet
在上第一期的分享中，我们通过一篇经典论文讲了 AlexNet 这个模型。可以说，这篇文章是深度学习在计算机视觉领域发挥作用的奠基之作。
" /><meta name="keywords" content="技术文档, docs, 极客时间" />






<meta name="generator" content="Hugo 0.140.2 with theme even" />


<link rel="canonical" href="https://politcloud.org/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/ai%E6%8A%80%E6%9C%AF%E5%86%85%E5%8F%82/097%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E4%BA%8Cvgggooglenet/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:url" content="https://politcloud.org/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/ai%E6%8A%80%E6%9C%AF%E5%86%85%E5%8F%82/097%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E4%BA%8Cvgggooglenet/">
  <meta property="og:site_name" content="Docs">
  <meta property="og:title" content="097计算机视觉领域的深度学习模型二VGGGoogleNet">
  <meta property="og:description" content="147 | 计算机视觉领域的深度学习模型（二）：VGG &amp; GoogleNet
在上第一期的分享中，我们通过一篇经典论文讲了 AlexNet 这个模型。可以说，这篇文章是深度学习在计算机视觉领域发挥作用的奠基之作。">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2024-01-10T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-01-10T00:00:00+00:00">
    <meta property="article:tag" content="AI技术内参">

  <meta itemprop="name" content="097计算机视觉领域的深度学习模型二VGGGoogleNet">
  <meta itemprop="description" content="147 | 计算机视觉领域的深度学习模型（二）：VGG &amp; GoogleNet
在上第一期的分享中，我们通过一篇经典论文讲了 AlexNet 这个模型。可以说，这篇文章是深度学习在计算机视觉领域发挥作用的奠基之作。">
  <meta itemprop="datePublished" content="2024-01-10T00:00:00+00:00">
  <meta itemprop="dateModified" content="2024-01-10T00:00:00+00:00">
  <meta itemprop="wordCount" content="2671">
  <meta itemprop="keywords" content="AI技术内参">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="097计算机视觉领域的深度学习模型二VGGGoogleNet">
  <meta name="twitter:description" content="147 | 计算机视觉领域的深度学习模型（二）：VGG &amp; GoogleNet
在上第一期的分享中，我们通过一篇经典论文讲了 AlexNet 这个模型。可以说，这篇文章是深度学习在计算机视觉领域发挥作用的奠基之作。">

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Docs</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Docs</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">097计算机视觉领域的深度学习模型二VGGGoogleNet</h1>

      <div class="post-meta">
        <span class="post-time"> 10100-01-10 </span>
        <div class="post-category">
            <a href="/categories/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/"> 极客时间 </a>
            </div>
          <span class="more-meta"> 约 2671 字 </span>
          <span class="more-meta"> 预计阅读 6 分钟 </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents"></nav>
  </div>
</div>
    <div class="post-content">
      <p>147 | 计算机视觉领域的深度学习模型（二）：VGG &amp; GoogleNet</p>
<p>在上第一期的分享中，我们通过一篇经典论文讲了 AlexNet 这个模型。可以说，这篇文章是深度学习在计算机视觉领域发挥作用的奠基之作。</p>
<p>AlexNet 在 2012 年发表之后，研究界对这个模型做了很多改进工作，使得这个模型得到了不断优化，特别是在 ImageNet 上的表现获得了显著提升。今天我们就来看看针对 AlexNet 模型的两个重要改进，分别是 VGG 和 GoogleNet。</p>
<p>VGG 网络</p>
<p>我们要分享的第一篇论文题目是《用于大规模图像识别的深度卷积网络》（Very Deep Convolutional Networks for Large-Scale Image Recognition）[1]。这篇文章的作者都来自于英国牛津大学的“视觉几何实验室”（Visual Geometry Group），简称 VGG，所以文章提出的模型也被叫作 VGG 网络。到目前为止，这篇论文的引用次数已经多达 1 万 4 千次。</p>
<p>首先，我们简单来了解一下这篇论文的作者。</p>
<p>第一作者叫卡伦·西蒙彦（Karen Simonyan），发表论文的时候他在牛津大学计算机系攻读博士学位。之后，西蒙彦加入了谷歌，在 DeepMind 任职，继续从事深度学习的研究。</p>
<p>第二作者叫安德鲁·兹泽曼（Andrew Zisserman），是牛津大学计算机系的教授，也是计算机视觉领域的学术权威。他曾经三次被授予计算机视觉最高荣誉“马尔奖”（Marr Prize）。</p>
<p>这篇论文的主要贡献是什么呢？一个重要贡献就是研究如何把之前的模型（例如 AlexNet）加深层次，从而能够拥有更好的模型泛化能力，最终实现更小的分类错误率。</p>
<p>为了更好地理解这篇文章的贡献，我们来回忆一下 AlexNet 的架构。AlexNet 拥有 8 层神经网络，分别是 5 层卷积层和 3 层全联通层。AlexNet 之所以能够有效地进行训练，是因为这个模型利用了“线性整流函数”（ReLu）、数据增强（Data Augmentation）以及 Dropout 等手段。这些方法让 AlexNet 能够达到 8 层。</p>
<p>但是，学术界一直以来都认为，从理论上看，神经网络应该是层数越多，泛化能力越好。而且在理论上，一个 8 层的神经网络完全可以加到 18 层或者 80 层。但是在现实中，梯度消失和过拟合等情况让加深神经网络变得非常困难。在这篇论文中，VGG 网络就尝试从 AlexNet 出发，看能否加入更多的神经网络层数，来达到更好的模型效果。</p>
<p>那 VGG 是怎么做到加深神经网络层数的呢？总体来说，VGG 对卷积层的“过滤器”（Filter）进行了更改，达到了 19 层的网络结构。从结果上看，和 AlexNet 相比，VGG 在 ImageNet 上的错误率要降低差不多一半。可以说，这是第一个真正意义上达到了“深层”的网络结构。</p>
<p>VGG 在“过滤器”上着手更改，那么具体的改变细节究竟有哪些呢？简单来说，就是在卷积层中仅仅使用“3*3”的“接受域”（Receptive Field），使得每一层都非常小。我们可以从整个形象上来理解，认为这是一组非常“瘦”的网络架构。在卷积层之后，是三层全联通层以及最后一层进行分类任务的层。一个细节是，VGG 放弃了我们之前介绍的 AlexNet 中引入的一个叫“局部响应归一化”（Local Response Normalization）的技术，原因是这个技巧并没有真正带来模型效果的提升。</p>
<p>VGG 架构在训练上的一个要点是先从一个小的结构开始，我们可以理解为首先训练一个 AlexNet，然后利用训练的结果来初始化更深结构的网络。作者们发现采用这种“初始训练”（Pre-Training）的办法要比完全从随机状态初始化模型训练得更加稳定。</p>
<p>GoogleNet</p>
<p>我们要分享的第二篇论文题目是《更深层的卷积》（Going deeper with convolutions）[2]。因为这篇论文的作者基本都来自于谷歌，所以文章提出的模型有时候又被叫作 GoogleNet。这篇论文拥有 8 千多次的引用数。</p>
<p>GoogleNet 不仅和 VGG 一样在把架构做“深”上下文章，而且在模型的效率上比 AlexNet 更加优秀。作者们利用了比 AlexNet 少 12 倍的参数，在更深的架构上达到了更好的效果。</p>
<p>GoogleNet 创新的重点是在网络架构上。和 AlexNet 以及 VGG 都不同的是，GoogleNet 的作者们认为更加合适的网络架构不是简单地把相同的卷积层叠加起来，然后再把相同的全联通层叠加。如果我们需要更深的架构，必须从原理上对网络架构有一个不同的理解。作者们认为，网络结构必须走向“稀疏化”（Sparsity），才能够达到更深层次、更高效的目的。</p>
<p>那么，能否直接用稀疏结构来进行网络的架构呢？过去的经验表明，这条路并不那么直观。第一，直接利用稀疏的结构所表达的网络结构效果并不好，第二，这样做就无法利用现代的硬件，特别是 GPU 的加速功能。现代的 GPU 之所以能够高效地处理视觉以及其他一系列类似的问题，主要的原因就是快速的紧密矩阵运算。所以，直接使用稀疏结构有一定的挑战。</p>
<p>这篇论文的核心思想就是希望用一组“局部的”（Local）紧密结构来逼近理想中的最优的稀疏化结构，从而能够在计算上达到高效率，同时在理论思想上，能够利用稀疏化结构来达到更深的网络架构。</p>
<p>这种局部模块被作者们称作是Inception 模块。什么意思呢？传统上，卷积层都是直接叠加起来的。而这篇论文提出的 Inception 模块，其实就是让卷积层能够在水平方向上排列起来，然后整个模块再进行垂直方向的叠加。至于水平方向排列多少个卷积层，垂直方向排列多少 Inception 模块，都是采用经验试错的方式来进行实验的。</p>
<p>这篇论文最终提出的 GoogleNet 有 22 层网络结构。如果把所有的平行结构都算上的话，整个网络超过了 100 层。为了能够在这么深的结构上训练模型，作者们还采用了一种方法，那就是在中间的一些层次中插入分类器。相比之下，我们之前遇到过的网络结构都是在最后一层才有一个分类器。分类器层的特点就是最终的标签信息会在这里被利用，也就是说，分类的准确性，或者说是图片中物体究竟是什么，都会被这个标签信息所涵盖。在中间层加入分类器，其实就是希望标签信息能够正确引导中间层的目标，并且能够让梯度依然有效经过。</p>
<p>在实验中，GoogleNet 模型可以说是达到了非常好的效果。在 2014 年的 ImageNet 竞赛中，GoogleNet 和 VGG 分列比赛的第一名和第二名。两个模型之间的差距仅有不到 1 个百分点。</p>
<p>小结</p>
<p>今天我为你讲了两篇基于深度学习的经典论文，讨论了两个模型 VGG 和 GoogleNet。这两个模型在 AlexNet 的基础上做了不少的革新。</p>
<p>一起回顾一下要点：第一，VGG 模型对卷积层的“过滤器”进行了更改，实现了 19 层的网络结构，可以说是第一个真正意义上达到了“深层”的网络结构；第二，GoogleNet 模型的创新是在网络架构上，利用稀疏化结构达到了更深的网络架构。</p>
<p>最后，给你留一个思考题，总结和比较 VGG 和 GoogleNet 这两个模型，我们看到了深度模型研发的一个什么趋势？</p>
<p>欢迎你给我留言，我们一起讨论。</p>
<p>参考文献</p>
<p>K. Simonyan, A. Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition. International Conference on Learning Representations, 2015.</p>
<p>C. Szegedy et al. Going deeper with convolutions. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Boston, MA, pp. 1-9, 2015.</p>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content"></span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
        10100-01-10
        
    </span>
  </p>
  
  
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/ai%E6%8A%80%E6%9C%AF%E5%86%85%E5%8F%82/">AI技术内参</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%95%86%E4%B8%9A%E6%A1%88%E4%BE%8B%E8%A7%A3%E8%AF%BB/097__%E8%B0%B7%E6%AD%8C%E7%9A%84%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%B7%AF%E8%B0%B7%E6%AD%8C%E7%9A%84%E9%BB%91%E7%A7%91%E6%8A%80/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">097__谷歌的大数据路：谷歌的“黑科技”</span>
            <span class="prev-text nav-mobile">上一篇</span>
          </a>
        <a class="next" href="/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%95%86%E4%B8%9A%E6%A1%88%E4%BE%8B%E8%A7%A3%E8%AF%BB/098__%E5%A6%82%E4%BD%95%E8%AF%BB%E6%87%82%E7%B1%BB%E4%BC%BC%E8%B0%B7%E6%AD%8C%E4%B8%89%E9%A9%BE%E9%A9%AC%E8%BD%A6%E8%BF%99%E6%A0%B7%E7%9A%84%E6%8A%80%E6%9C%AF%E8%AE%BA%E6%96%87/">
            <span class="next-text nav-default">098__如何读懂类似谷歌“三驾马车”这样的技术论文？</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
  
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2024 - 
    2025<span class="heart"><i class="iconfont icon-heart"></i></span><span></span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  <script src="/lib/highlight/highlight.pack.js?v=20171001"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js"></script>


      <script async src="https://www.googletagmanager.com/gtag/js?id=G-FVZ07KBD4X"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-FVZ07KBD4X');
        }
      </script>






</body>
</html>
