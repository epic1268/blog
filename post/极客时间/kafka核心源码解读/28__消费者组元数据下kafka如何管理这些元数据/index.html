<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>28__消费者组元数据（下）：Kafka如何管理这些元数据？ - Docs</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="" /><meta name="description" content="你好，我是胡夕。今天我们继续学习消费者组元数据。
学完上节课之后，我们知道，Kafka 定义了非常多的元数据，那么，这就必然涉及到对元数据的管理问题了。
" /><meta name="keywords" content="技术文档, docs, 极客时间" />






<meta name="generator" content="Hugo 0.140.2 with theme even" />


<link rel="canonical" href="https://politcloud.org/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/kafka%E6%A0%B8%E5%BF%83%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/28__%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%8Bkafka%E5%A6%82%E4%BD%95%E7%AE%A1%E7%90%86%E8%BF%99%E4%BA%9B%E5%85%83%E6%95%B0%E6%8D%AE/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:url" content="https://politcloud.org/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/kafka%E6%A0%B8%E5%BF%83%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/28__%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84%E5%85%83%E6%95%B0%E6%8D%AE%E4%B8%8Bkafka%E5%A6%82%E4%BD%95%E7%AE%A1%E7%90%86%E8%BF%99%E4%BA%9B%E5%85%83%E6%95%B0%E6%8D%AE/">
  <meta property="og:site_name" content="Docs">
  <meta property="og:title" content="28__消费者组元数据（下）：Kafka如何管理这些元数据？">
  <meta property="og:description" content="你好，我是胡夕。今天我们继续学习消费者组元数据。
学完上节课之后，我们知道，Kafka 定义了非常多的元数据，那么，这就必然涉及到对元数据的管理问题了。">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2024-01-10T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-01-10T00:00:00+00:00">
    <meta property="article:tag" content="Kafka核心源码解读">

  <meta itemprop="name" content="28__消费者组元数据（下）：Kafka如何管理这些元数据？">
  <meta itemprop="description" content="你好，我是胡夕。今天我们继续学习消费者组元数据。
学完上节课之后，我们知道，Kafka 定义了非常多的元数据，那么，这就必然涉及到对元数据的管理问题了。">
  <meta itemprop="datePublished" content="2024-01-10T00:00:00+00:00">
  <meta itemprop="dateModified" content="2024-01-10T00:00:00+00:00">
  <meta itemprop="wordCount" content="7008">
  <meta itemprop="keywords" content="Kafka核心源码解读">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="28__消费者组元数据（下）：Kafka如何管理这些元数据？">
  <meta name="twitter:description" content="你好，我是胡夕。今天我们继续学习消费者组元数据。
学完上节课之后，我们知道，Kafka 定义了非常多的元数据，那么，这就必然涉及到对元数据的管理问题了。">

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Docs</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Docs</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">28__消费者组元数据（下）：Kafka如何管理这些元数据？</h1>

      <div class="post-meta">
        <span class="post-time"> 10100-01-10 </span>
        <div class="post-category">
            <a href="/categories/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/"> 极客时间 </a>
            </div>
          <span class="more-meta"> 约 7008 字 </span>
          <span class="more-meta"> 预计阅读 14 分钟 </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#消费者组状态管理方法">消费者组状态管理方法</a></li>
        <li><a href="#成员管理方法">成员管理方法</a>
          <ul>
            <li><a href="#添加成员">添加成员</a></li>
            <li><a href="#移除成员">移除成员</a></li>
            <li><a href="#查询成员">查询成员</a></li>
          </ul>
        </li>
        <li><a href="#位移管理方法">位移管理方法</a>
          <ul>
            <li><a href="#添加位移值">添加位移值</a></li>
            <li><a href="#移除位移值">移除位移值</a></li>
          </ul>
        </li>
        <li><a href="#分区分配策略管理方法">分区分配策略管理方法</a>
          <ul>
            <li><a href="#确认消费者组支持的分区分配策略集">确认消费者组支持的分区分配策略集</a></li>
            <li><a href="#选出消费者组的分区消费分配策略">选出消费者组的分区消费分配策略</a></li>
          </ul>
        </li>
        <li><a href="#总结">总结</a></li>
        <li><a href="#课后讨论">课后讨论</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p>你好，我是胡夕。今天我们继续学习消费者组元数据。</p>
<p>学完上节课之后，我们知道，Kafka 定义了非常多的元数据，那么，这就必然涉及到对元数据的管理问题了。</p>
<p>这些元数据的类型不同，管理策略也就不一样。这节课，我将从消费者组状态、成员、位移和分区分配策略四个维度，对这些元数据进行拆解，带你一起了解下 Kafka 管理这些元数据的方法。</p>
<p>这些方法定义在 MemberMetadata 和 GroupMetadata 这两个类中，其中，GroupMetadata 类中的方法最为重要，是我们要重点学习的对象。在后面的课程中，你会看到，这些方法会被上层组件 GroupCoordinator 频繁调用，因此，它们是我们学习 Coordinator 组件代码的前提条件，你一定要多花些精力搞懂它们。</p>
<h2 id="消费者组状态管理方法">消费者组状态管理方法</h2>
<p>消费者组状态是很重要的一类元数据。管理状态的方法，要做的事情也就是设置和查询。这些方法大多比较简单，所以我把它们汇总在一起，直接介绍给你。</p>
<p>// GroupMetadata.scala<br>
// 设置/更新状态<br>
def transitionTo(groupState: GroupState): Unit = {<br>
assertValidTransition(groupState) // 确保是合法的状态转换<br>
state = groupState  // 设置状态到给定状态<br>
currentStateTimestamp = Some(time.milliseconds() // 更新状态变更时间戳<br>
// 查询状态<br>
def currentState = state<br>
// 判断消费者组状态是指定状态<br>
def is(groupState: GroupState) = state == groupState<br>
// 判断消费者组状态不是指定状态<br>
def not(groupState: GroupState) = state != groupState<br>
// 消费者组能否 Rebalance 的条件是当前状态是 PreparingRebalance 状态的合法前置状态<br>
def canRebalance = PreparingRebalance.validPreviousStates.contains(state)</p>
<p><strong>1.transitionTo 方法</strong></p>
<p>transitionTo 方法的作用是<strong>将消费者组状态变更成给定状态</strong>。在变更前，代码需要确保这次变更必须是合法的状态转换。这是依靠每个 GroupState 实现类定义的 <strong>validPreviousStates 集合</strong>来完成的。只有在这个集合中的状态，才是合法的前置状态。简单来说，只有集合中的这些状态，才能转换到当前状态。</p>
<p>同时，该方法还会<strong>更新状态变更的时间戳字段</strong>。Kafka 有个定时任务，会定期清除过期的消费者组位移数据，它就是依靠这个时间戳字段，来判断过期与否的。</p>
<p><strong>2.canRebalance 方法</strong></p>
<p>它用于判断消费者组是否能够开启 Rebalance 操作。判断依据是，<strong>当前状态是否是 PreparingRebalance 状态的合法前置状态</strong>。只有 <strong>Stable</strong>、<strong>CompletingRebalance</strong> 和 <strong>Empty</strong> 这 3 类状态的消费者组，才有资格开启 Rebalance。</p>
<p><strong>3.is 和 not 方法</strong></p>
<p>至于 is 和 not 方法，它们分别判断消费者组的状态与给定状态吻合还是不吻合，主要被用于<strong>执行状态校验</strong>。特别是 is 方法，被大量用于上层调用代码中，执行各类消费者组管理任务的前置状态校验工作。</p>
<p>总体来说，管理消费者组状态数据，依靠的就是这些方法，还是很简单的吧？</p>
<h2 id="成员管理方法">成员管理方法</h2>
<p>在介绍管理消费者组成员的方法之前，我先帮你回忆下 GroupMetadata 中保存成员的字段。GroupMetadata 中使用 members 字段保存所有的成员信息。该字段是一个 HashMap，Key 是成员的 member ID 字段，Value 是 MemberMetadata 类型，该类型保存了成员的元数据信息。</p>
<p>所谓的管理成员，也就是添加成员（add 方法）、移除成员（remove 方法）和查询成员（has、get、size 方法等）。接下来，我们就逐一来学习。</p>
<h3 id="添加成员">添加成员</h3>
<p>先说添加成员的方法：add。add 方法的主要逻辑，是将成员对象添加到 members 字段，同时更新其他一些必要的元数据，比如 Leader 成员字段、分区分配策略支持票数等。下面是 add 方法的源码：</p>
<p>def add(member: MemberMetadata, callback: JoinCallback = null): Unit = {<br>
// 如果是要添加的第一个消费者组成员<br>
if (members.isEmpty)<br>
// 就把该成员的 procotolType 设置为消费者组的 protocolType<br>
this.protocolType = Some(member.protocolType)<br>
// 确保成员元数据中的 groupId 和组 Id 相同<br>
assert(groupId == member.groupId)<br>
// 确保成员元数据中的 protoclType 和组 protocolType 相同<br>
assert(this.protocolType.orNull == member.protocolType)<br>
// 确保该成员选定的分区分配策略与组选定的分区分配策略相匹配<br>
assert(supportsProtocols(member.protocolType, MemberMetadata.plainProtocolSet(member.supportedProtocols)))<br>
// 如果尚未选出 Leader 成员<br>
if (leaderId.isEmpty)<br>
// 把该成员设定为 Leader 成员<br>
leaderId = Some(member.memberId)<br>
// 将该成员添加进 members<br>
members.put(member.memberId, member)<br>
// 更新分区分配策略支持票数<br>
member.supportedProtocols.foreach{ case (protocol, _) =&gt; supportedProtocols(protocol) += 1 }<br>
// 设置成员加入组后的回调逻辑<br>
member.awaitingJoinCallback = callback<br>
// 更新已加入组的成员数<br>
if (member.isAwaitingJoin)<br>
numMembersAwaitingJoin += 1<br>
}</p>
<p>我再画一张流程图，帮助你更直观地理解这个方法的作用。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/Kafka%E6%A0%B8%E5%BF%83%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/10a91bbcb3735579bff6a133eeed2bf3.png" alt=""></p>
<p>我再具体解释一下这个方法的执行逻辑。</p>
<p>第一步，add 方法要判断 members 字段是否包含已有成员。如果没有，就说明要添加的成员是该消费者组的第一个成员，那么，就令该成员协议类型（protocolType）成为组的 protocolType。我在上节课中讲过，对于普通的消费者而言，protocolType 就是字符串&quot;consumer&quot;。如果不是首个成员，就进入到下一步。</p>
<p>第二步，add 方法会连续进行三次校验，分别确保<strong>待添加成员的组 ID、protocolType</strong> 和组配置一致，以及该成员选定的分区分配策略与组选定的分区分配策略相匹配。如果这些校验有任何一个未通过，就会立即抛出异常。</p>
<p>第三步，判断消费者组的 Leader 成员是否已经选出了。如果还没有选出，就将该成员设置成 Leader 成员。当然了，如果 Leader 已经选出了，自然就不需要做这一步了。需要注意的是，这里的 Leader 和我们在学习副本管理器时学到的 Leader 副本是不同的概念。这里的 Leader 成员，是指<strong>消费者组下的一个成员</strong>。该成员负责为所有成员制定分区分配方案，制定方法的依据，就是消费者组选定的分区分配策略。</p>
<p>第四步，更新消费者组分区分配策略支持票数。关于 supportedProtocols 字段的含义，我在上节课的末尾用一个例子形象地进行了说明，这里就不再重复说了。如果你没有印象了，可以再复习一下。</p>
<p>最后一步，设置成员加入组后的回调逻辑，同时更新已加入组的成员数。至此，方法结束。</p>
<p>作为关键的成员管理方法之一，add 方法是实现消费者组 Rebalance 流程至关重要的一环。每当 Rebalance 开启第一大步——加入组的操作时，本质上就是在利用这个 add 方法实现新成员入组的逻辑。</p>
<h3 id="移除成员">移除成员</h3>
<p>有 add 方法，自然也就有 remove 方法，下面是 remove 方法的完整源码：</p>
<p>def remove(memberId: String): Unit = {<br>
// 从 members 中移除给定成员<br>
members.remove(memberId).foreach { member =&gt;<br>
// 更新分区分配策略支持票数<br>
member.supportedProtocols.foreach{ case (protocol, _) =&gt; supportedProtocols(protocol) -= 1 }<br>
// 更新已加入组的成员数<br>
if (member.isAwaitingJoin)<br>
numMembersAwaitingJoin -= 1<br>
}<br>
// 如果该成员是 Leader，选择剩下成员列表中的第一个作为新的 Leader 成员<br>
if (isLeader(memberId))<br>
leaderId = members.keys.headOption<br>
}</p>
<p>remove 方法比 add 要简单一些。<strong>首先</strong>，代码从 members 中移除给定成员。<strong>之后</strong>，更新分区分配策略支持票数，以及更新已加入组的成员数。<strong>最后</strong>，代码判断该成员是否是 Leader 成员，如果是的话，就选择成员列表中尚存的第一个成员作为新的 Leader 成员。</p>
<h3 id="查询成员">查询成员</h3>
<p>查询 members 的方法有很多，大多都是很简单的场景。我给你介绍 3 个比较常见的。</p>
<p>def has(memberId: String) = members.contains(memberId)<br>
def get(memberId: String) = members(memberId)<br>
def size = members.size</p>
<ol>
<li>has 方法，判断消费者组是否包含指定成员；</li>
<li>get 方法，获取指定成员对象；</li>
<li>size 方法，统计总成员数。</li>
</ol>
<p>其它的查询方法逻辑也都很简单，比如 allMemberMetadata、rebalanceTimeoutMs，等等，我就不多讲了。课后你可以自行阅读下，重点是体会这些方法利用 members 都做了什么事情。</p>
<h2 id="位移管理方法">位移管理方法</h2>
<p>除了组状态和成员管理之外，GroupMetadata 还有一大类管理功能，就是<strong>管理消费者组的提交位移</strong>（Committed Offsets），主要包括添加和移除位移值。</p>
<p>不过，在学习管理位移的功能之前，我再带你回顾一下保存位移的 offsets 字段的定义。毕竟，接下来我们要学习的方法，主要操作的就是这个字段。</p>
<p>private val offsets = new mutable.HashMap[TopicPartition, CommitRecordMetadataAndOffset]</p>
<p>它是 HashMap 类型，Key 是 TopicPartition 类型，表示一个主题分区，而 Value 是 CommitRecordMetadataAndOffset 类型。该类封装了位移提交消息的位移值。</p>
<p>在详细阅读位移管理方法之前，我先解释下这里的“位移”和“位移提交消息”。</p>
<p>消费者组需要向 Coordinator 提交已消费消息的进度，在 Kafka 中，这个进度有个专门的术语，叫作提交位移。Kafka 使用它来定位消费者组要消费的下一条消息。那么，提交位移在 Coordinator 端是如何保存的呢？它实际上是保存在内部位移主题中。提交的方式是，消费者组成员向内部主题写入符合特定格式的事件消息，这类消息就是所谓的位移提交消息（Commit Record）。关于位移提交消息的事件格式，我会在第 30 讲具体介绍，这里你可以暂时不用理会。而这里所说的 CommitRecordMetadataAndOffset 类，就是标识位移提交消息的地方。我们看下它的代码：</p>
<p>case class CommitRecordMetadataAndOffset(appendedBatchOffset: Option[Long], offsetAndMetadata: OffsetAndMetadata) {<br>
def olderThan(that: CommitRecordMetadataAndOffset): Boolean = appendedBatchOffset.get &lt; that.appendedBatchOffset.get<br>
}</p>
<p>这个类的构造函数有两个参数。</p>
<ol>
<li>appendedBatchOffset：保存的是位移主题消息自己的位移值；</li>
<li>offsetAndMetadata：保存的是位移提交消息中保存的消费者组的位移值。</li>
</ol>
<h3 id="添加位移值">添加位移值</h3>
<p>在 GroupMetadata 中，有 3 个向 offsets 中添加订阅分区的已消费位移值的方法，分别是 initializeOffsets、onOffsetCommitAppend 和 completePendingTxnOffsetCommit。</p>
<p>initializeOffsets 方法的代码非常简单，如下所示：</p>
<p>def initializeOffsets(<br>
offsets: collection.Map[TopicPartition, CommitRecordMetadataAndOffset],<br>
pendingTxnOffsets: Map[Long, mutable.Map[TopicPartition, CommitRecordMetadataAndOffset]]): Unit = {<br>
this.offsets ++= offsets<br>
this.pendingTransactionalOffsetCommits ++= pendingTxnOffsets<br>
}</p>
<p>它仅仅是将给定的一组订阅分区提交位移值加到 offsets 中。当然，同时它还会更新 pendingTransactionalOffsetCommits 字段。</p>
<p>不过，由于这个字段是给 Kafka 事务机制使用的，因此，你只需要关注这个方法的第一行语句就行了。当消费者组的协调者组件启动时，它会创建一个异步任务，定期地读取位移主题中相应消费者组的提交位移数据，并把它们加载到 offsets 字段中。</p>
<p>我们再来看第二个方法，onOffsetCommitAppend 的代码。</p>
<p>def onOffsetCommitAppend(topicPartition: TopicPartition, offsetWithCommitRecordMetadata: CommitRecordMetadataAndOffset): Unit = {<br>
if (pendingOffsetCommits.contains(topicPartition)) {<br>
if (offsetWithCommitRecordMetadata.appendedBatchOffset.isEmpty)<br>
throw new IllegalStateException(&ldquo;Cannot complete offset commit write without providing the metadata of the record &quot; +<br>
&ldquo;in the log.&rdquo;)<br>
// offsets 字段中没有该分区位移提交数据，或者<br>
// offsets 字段中该分区对应的提交位移消息在位移主题中的位移值小于待写入的位移值<br>
if (!offsets.contains(topicPartition) || offsets(topicPartition).olderThan(offsetWithCommitRecordMetadata))<br>
// 将该分区对应的提交位移消息添加到 offsets 中<br>
offsets.put(topicPartition, offsetWithCommitRecordMetadata)<br>
}<br>
pendingOffsetCommits.get(topicPartition) match {<br>
case Some(stagedOffset) if offsetWithCommitRecordMetadata.offsetAndMetadata == stagedOffset =&gt;<br>
pendingOffsetCommits.remove(topicPartition)<br>
case _ =&gt;<br>
}<br>
}</p>
<p>该方法在提交位移消息被成功写入后调用。主要判断的依据，是 offsets 中是否已包含该主题分区对应的消息值，或者说，offsets 字段中该分区对应的提交位移消息在位移主题中的位移值是否小于待写入的位移值。如果是的话，就把该主题已提交的位移值添加到 offsets 中。</p>
<p>第三个方法 completePendingTxnOffsetCommit 的作用是完成一个待决事务（Pending Transaction）的位移提交。所谓的待决事务，就是指正在进行中、还没有完成的事务。在处理待决事务的过程中，可能会出现将待决事务中涉及到的分区的位移值添加到 offsets 中的情况。不过，由于该方法是与 Kafka 事务息息相关的，你不需要重点掌握，这里我就不展开说了。</p>
<h3 id="移除位移值">移除位移值</h3>
<p>offsets 中订阅分区的已消费位移值也是能够被移除的。你还记得，Kafka 主题中的消息有默认的留存时间设置吗？位移主题是普通的 Kafka 主题，所以也要遵守相应的规定。如果当前时间与已提交位移消息时间戳的差值，超过了 Broker 端参数 offsets.retention.minutes 值，Kafka 就会将这条记录从 offsets 字段中移除。这就是方法 removeExpiredOffsets 要做的事情。</p>
<p>这个方法的代码有点长，为了方便你掌握，我分块给你介绍下。我先带你了解下它的内部嵌套类方法 getExpireOffsets，然后再深入了解它的实现逻辑，这样你就能很轻松地掌握 Kafka 移除位移值的代码原理了。</p>
<p>首先，该方法定义了一个内部嵌套方法 <strong>getExpiredOffsets</strong>，专门用于获取订阅分区过期的位移值。我们来阅读下源码，看看它是如何做到的。</p>
<p>def getExpiredOffsets(<br>
baseTimestamp: CommitRecordMetadataAndOffset =&gt; Long,<br>
subscribedTopics: Set[String] = Set.empty): Map[TopicPartition, OffsetAndMetadata] = {<br>
// 遍历 offsets 中的所有分区，过滤出同时满足以下 3 个条件的所有分区<br>
// 条件 1：分区所属主题不在订阅主题列表之内<br>
// 条件 2：该主题分区已经完成位移提交<br>
// 条件 3：该主题分区在位移主题中对应消息的存在时间超过了阈值<br>
offsets.filter {<br>
case (topicPartition, commitRecordMetadataAndOffset) =&gt;<br>
!subscribedTopics.contains(topicPartition.topic()) &amp;&amp;<br>
!pendingOffsetCommits.contains(topicPartition) &amp;&amp; {<br>
commitRecordMetadataAndOffset<br>
.offsetAndMetadata.expireTimestamp match {<br>
case None =&gt;<br>
currentTimestamp - baseTimestamp(commitRecordMetadataAndOffset) &gt;= offsetRetentionMs<br>
case Some(expireTimestamp) =&gt;<br>
currentTimestamp &gt;= expireTimestamp<br>
}<br>
}<br>
}.map {<br>
// 为满足以上 3 个条件的分区提取出 commitRecordMetadataAndOffset 中的位移值<br>
case (topicPartition, commitRecordOffsetAndMetadata) =&gt;<br>
(topicPartition, commitRecordOffsetAndMetadata.offsetAndMetadata)<br>
}.toMap<br>
}</p>
<p>该方法接收两个参数。</p>
<ol>
<li>baseTimestamp：它是一个函数类型，接收 CommitRecordMetadataAndOffset 类型的字段，然后计算时间戳，并返回；</li>
<li>subscribedTopics：即订阅主题集合，默认是空。</li>
</ol>
<p>方法开始时，代码从 offsets 字段中过滤出同时满足 3 个条件的所有分区。</p>
<p><strong>条件 1</strong>：分区所属主题不在订阅主题列表之内。当方法传入了不为空的主题集合时，就说明该消费者组此时正在消费中，正在消费的主题是不能执行过期位移移除的。</p>
<p><strong>条件 2</strong>：主题分区已经完成位移提交，那种处于提交中状态，也就是保存在 pendingOffsetCommits 字段中的分区，不予考虑。</p>
<p><strong>条件 3</strong>：该主题分区在位移主题中对应消息的存在时间超过了阈值。老版本的 Kafka 消息直接指定了过期时间戳，因此，只需要判断当前时间是否越过了这个过期时间。但是，目前，新版 Kafka 判断过期与否，主要是<strong>基于消费者组状态</strong>。如果是 Empty 状态，过期的判断依据就是当前时间与组变为 Empty 状态时间的差值，是否超过 Broker 端参数 offsets.retention.minutes 值；如果不是 Empty 状态，就看当前时间与提交位移消息中的时间戳差值是否超过了 offsets.retention.minutes 值。如果超过了，就视为已过期，对应的位移值需要被移除；如果没有超过，就不需要移除了。</p>
<p>当过滤出同时满足这 3 个条件的分区后，提取出它们对应的位移值对象并返回。</p>
<p>学过了 getExpiredOffsets 方法代码的实现之后，removeExpiredOffsets 方法剩下的代码就很容易理解了。</p>
<p>def removeExpiredOffsets(<br>
currentTimestamp: Long, offsetRetentionMs: Long): Map[TopicPartition, OffsetAndMetadata] = {<br>
// getExpiredOffsets 方法代码&hellip;&hellip;<br>
// 调用 getExpiredOffsets 方法获取主题分区的过期位移<br>
val expiredOffsets: Map[TopicPartition, OffsetAndMetadata] = protocolType match {<br>
case Some(_) if is(Empty) =&gt;<br>
getExpiredOffsets(<br>
commitRecordMetadataAndOffset =&gt; currentStateTimestamp    .getOrElse(commitRecordMetadataAndOffset.offsetAndMetadata.commitTimestamp)<br>
)<br>
case Some(ConsumerProtocol.PROTOCOL_TYPE) if subscribedTopics.isDefined =&gt;<br>
getExpiredOffsets(<br>
<em>.offsetAndMetadata.commitTimestamp,<br>
subscribedTopics.get<br>
)<br>
case None =&gt;<br>
getExpiredOffsets(</em>.offsetAndMetadata.commitTimestamp)<br>
case _ =&gt;<br>
Map()<br>
}<br>
if (expiredOffsets.nonEmpty)<br>
debug(s&quot;Expired offsets from group &lsquo;$groupId&rsquo;: ${expiredOffsets.keySet}&rdquo;)<br>
// 将过期位移对应的主题分区从 offsets 中移除<br>
offsets &ndash;= expiredOffsets.keySet<br>
// 返回主题分区对应的过期位移<br>
expiredOffsets<br>
}</p>
<p>代码根据消费者组的 protocolType 类型和组状态调用 getExpiredOffsets 方法，同时决定传入什么样的参数：</p>
<ol>
<li>如果消费者组状态是 Empty，就传入组变更为 Empty 状态的时间，若该时间没有被记录，则使用提交位移消息本身的写入时间戳，来获取过期位移；</li>
<li>如果是普通的消费者组类型，且订阅主题信息已知，就传入提交位移消息本身的写入时间戳和订阅主题集合共同确定过期位移值；</li>
<li>如果 protocolType 为 None，就表示，这个消费者组其实是一个 Standalone 消费者，依然是传入提交位移消息本身的写入时间戳，来决定过期位移值；</li>
<li>如果消费者组的状态不符合刚刚说的这些情况，那就说明，没有过期位移值需要被移除。</li>
</ol>
<p>当确定了要被移除的位移值集合后，代码会将它们从 offsets 中移除，然后返回这些被移除的位移值信息。至此，方法结束。</p>
<h2 id="分区分配策略管理方法">分区分配策略管理方法</h2>
<p>最后，我们讨论下消费者组分区分配策略的管理，也就是字段 supportedProtocols 的管理。supportedProtocols 是分区分配策略的支持票数，这个票数在添加成员、移除成员时，会进行相应的更新。</p>
<p>消费者组每次 Rebalance 的时候，都要重新确认本次 Rebalance 结束之后，要使用哪个分区分配策略，因此，就需要特定的方法来对这些票数进行统计，把票数最多的那个策略作为新的策略。</p>
<p>GroupMetadata 类中定义了两个方法来做这件事情，分别是 candidateProtocols 和 selectProtocol 方法。</p>
<h3 id="确认消费者组支持的分区分配策略集">确认消费者组支持的分区分配策略集</h3>
<p>首先来看 candidateProtocols 方法。它的作用是<strong>找出组内所有成员都支持的分区分配策略集</strong>。代码如下：</p>
<p>private def candidateProtocols: Set[String] = {<br>
val numMembers = members.size // 获取组内成员数<br>
// 找出支持票数=总成员数的策略，返回它们的名称<br>
supportedProtocols.filter(_.<em>2 == numMembers).map(</em>._1).toSet<br>
}</p>
<p>该方法首先会获取组内的总成员数，然后，找出 supportedProtocols 中那些支持票数等于总成员数的分配策略，并返回它们的名称。<strong>支持票数等于总成员数的意思，等同于所有成员都支持该策略</strong>。</p>
<h3 id="选出消费者组的分区消费分配策略">选出消费者组的分区消费分配策略</h3>
<p>接下来，我们看下 selectProtocol 方法，它的作用是<strong>选出消费者组的分区消费分配策略</strong>。</p>
<p>def selectProtocol: String = {<br>
// 如果没有任何成员，自然无法确定选用哪个策略<br>
if (members.isEmpty)<br>
throw new IllegalStateException(&ldquo;Cannot select protocol for empty group&rdquo;)<br>
// 获取所有成员都支持的策略集合<br>
val candidates = candidateProtocols<br>
// 让每个成员投票，票数最多的那个策略当选<br>
val (protocol, <em>) = allMemberMetadata<br>
.map(</em>.vote(candidates))<br>
.groupBy(identity)<br>
.maxBy { case (_, votes) =&gt; votes.size }<br>
protocol<br>
}</p>
<p>这个方法首先会判断组内是否有成员。如果没有任何成员，自然就无法确定选用哪个策略了，方法就会抛出异常，并退出。否则的话，代码会调用刚才的 candidateProtocols 方法，获取所有成员都支持的策略集合，然后让每个成员投票，票数最多的那个策略当选。</p>
<p>你可能会好奇，这里的 vote 方法是怎么实现的呢？其实，它就是简单地查找而已。我举一个简单的例子，来帮助你理解。</p>
<p>比如，candidates 字段的值是 [“策略 A”，“策略 B”]，成员 1 支持 [“策略 B”，“策略 A”]，成员 2 支持 [“策略 A”，“策略 B”，“策略 C”]，成员 3 支持 [“策略 D”，“策略 B”，“策略 A”]，那么，vote 方法会将 candidates 与每个成员的支持列表进行比对，找出成员支持列表中第一个包含在 candidates 中的策略。因此，对于这个例子来说，成员 1 投票策略 B，成员 2 投票策略 A，成员 3 投票策略 B。可以看到，投票的结果是，策略 B 是两票，策略 A 是 1 票。所以，selectProtocol 方法返回策略 B 作为新的策略。</p>
<p>有一点你需要注意，<strong>成员支持列表中的策略是有顺序的</strong>。这就是说，[“策略 B”，“策略 A”] 和 [“策略 A”，“策略 B”] 是不同的，成员会倾向于选择靠前的策略。</p>
<h2 id="总结">总结</h2>
<p>今天，我们结合 GroupMetadata 源码，学习了 Kafka 对消费者组元数据的管理，主要包括组状态、成员、位移和分区分配策略四个维度。我建议你在课下再仔细地阅读一下这些管理数据的方法，对照着源码和注释走一遍完整的操作流程。</p>
<p>另外，在这两节课中，我没有谈及待决成员列表（Pending Members）和待决位移（Pending Offsets）的管理，因为这两个元数据项属于中间临时状态，因此我没有展开讲，不理解这部分代码的话，也不会影响我们理解消费者组元数据以及 Coordinator 是如何使用它们的。不过，我建议你可以阅读下与它们相关的代码部分。要知道，Kafka 是非常喜欢引用中间状态变量来管理各类元数据或状态的。</p>
<p>现在，我们再来简单回顾下这节课的重点。</p>
<ol>
<li>消费者组元数据管理：主要包括对组状态、成员、位移和分区分配策略的管理。</li>
<li>组状态管理：transitionTo 方法负责设置状态，is、not 和 get 方法用于查询状态。</li>
<li>成员管理：add、remove 方法用于增减成员，has 和 get 方法用于查询特定成员。</li>
<li>分区分配策略管理：定义了专属方法 selectProtocols，用于在每轮 Rebalance 时选举分区分配策略。</li>
</ol>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/Kafka%E6%A0%B8%E5%BF%83%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/2efa0ac0012937979d2c706c6088afcb.png" alt=""></p>
<p>至此，我们花了两节课的时间，详细地学习了消费者组元数据及其管理方法的源码。这些操作元数据的方法被上层调用方 GroupCoordinator 大量使用，就像我在开头提到的，如果现在我们不彻底掌握这些元数据被操作的手法，等我们学到 GroupCoordinator 代码时，就会感到有些吃力，所以，你一定要好好地学习这两节课。有了这些基础，等到学习 GroupCoordinator 源码时，你就能更加深刻地理解它的底层实现原理了。</p>
<h2 id="课后讨论">课后讨论</h2>
<p>在讲到 MemberMetadata 时，我说过，每个成员都有自己的 Rebalance 超时时间设置，那么，Kafka 是怎么确认消费者组使用哪个成员的超时时间作为整个组的超时时间呢？</p>
<p>欢迎在留言区写下你的思考和答案，跟我交流讨论，也欢迎你把今天的内容分享给你的朋友。</p>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content"></span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
        10100-01-10
        
    </span>
  </p>
  
  
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/kafka%E6%A0%B8%E5%BF%83%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/">Kafka核心源码解读</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E6%80%A7%E8%83%BD%E5%B7%A5%E7%A8%8B%E9%AB%98%E6%89%8B%E8%AF%BE/28__%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93%E6%85%A2%E9%97%AE%E9%A2%98%E5%88%B0%E5%BA%95%E5%87%BA%E5%9C%A8%E5%93%AA%E4%BA%86/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">28__网络数据传输慢，问题到底出在哪了？</span>
            <span class="prev-text nav-mobile">上一篇</span>
          </a>
        <a class="next" href="/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%9330%E8%AE%B2/28__%E9%80%89%E5%9E%8B%E6%A1%88%E4%BE%8B%E9%93%B6%E8%A1%8C%E6%98%AF%E6%80%8E%E4%B9%88%E9%80%89%E6%8B%A9%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84/">
            <span class="next-text nav-default">28__选型案例：银行是怎么选择分布式数据库的？</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
  
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2024 - 
    2025<span class="heart"><i class="iconfont icon-heart"></i></span><span></span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  <script src="/lib/highlight/highlight.pack.js?v=20171001"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js"></script>


      <script async src="https://www.googletagmanager.com/gtag/js?id=G-FVZ07KBD4X"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-FVZ07KBD4X');
        }
      </script>






</body>
</html>
