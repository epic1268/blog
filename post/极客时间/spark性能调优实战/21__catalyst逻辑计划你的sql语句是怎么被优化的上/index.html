<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>21__Catalyst逻辑计划：你的SQL语句是怎么被优化的？（上） - Docs</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="" /><meta name="description" content="你好，我是吴磊。
上一讲我们说，Spark SQL 已经取代 Spark Core 成为了新一代的内核优化引擎，所有 Spark 子框架都能共享 Spark SQL 带来的性能红利，所以在 Spark 历次发布的新版本中，Spark SQL 占比最大。因此，Spark SQL 的优化过程是我们必须要掌握的。
" /><meta name="keywords" content="技术文档, docs, 极客时间" />






<meta name="generator" content="Hugo 0.140.2 with theme even" />


<link rel="canonical" href="https://politcloud.org/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/spark%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E5%AE%9E%E6%88%98/21__catalyst%E9%80%BB%E8%BE%91%E8%AE%A1%E5%88%92%E4%BD%A0%E7%9A%84sql%E8%AF%AD%E5%8F%A5%E6%98%AF%E6%80%8E%E4%B9%88%E8%A2%AB%E4%BC%98%E5%8C%96%E7%9A%84%E4%B8%8A/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:url" content="https://politcloud.org/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/spark%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E5%AE%9E%E6%88%98/21__catalyst%E9%80%BB%E8%BE%91%E8%AE%A1%E5%88%92%E4%BD%A0%E7%9A%84sql%E8%AF%AD%E5%8F%A5%E6%98%AF%E6%80%8E%E4%B9%88%E8%A2%AB%E4%BC%98%E5%8C%96%E7%9A%84%E4%B8%8A/">
  <meta property="og:site_name" content="Docs">
  <meta property="og:title" content="21__Catalyst逻辑计划：你的SQL语句是怎么被优化的？（上）">
  <meta property="og:description" content="你好，我是吴磊。
上一讲我们说，Spark SQL 已经取代 Spark Core 成为了新一代的内核优化引擎，所有 Spark 子框架都能共享 Spark SQL 带来的性能红利，所以在 Spark 历次发布的新版本中，Spark SQL 占比最大。因此，Spark SQL 的优化过程是我们必须要掌握的。">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2024-01-10T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-01-10T00:00:00+00:00">
    <meta property="article:tag" content="Spark性能调优实战">

  <meta itemprop="name" content="21__Catalyst逻辑计划：你的SQL语句是怎么被优化的？（上）">
  <meta itemprop="description" content="你好，我是吴磊。
上一讲我们说，Spark SQL 已经取代 Spark Core 成为了新一代的内核优化引擎，所有 Spark 子框架都能共享 Spark SQL 带来的性能红利，所以在 Spark 历次发布的新版本中，Spark SQL 占比最大。因此，Spark SQL 的优化过程是我们必须要掌握的。">
  <meta itemprop="datePublished" content="2024-01-10T00:00:00+00:00">
  <meta itemprop="dateModified" content="2024-01-10T00:00:00+00:00">
  <meta itemprop="wordCount" content="5633">
  <meta itemprop="keywords" content="Spark性能调优实战">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="21__Catalyst逻辑计划：你的SQL语句是怎么被优化的？（上）">
  <meta name="twitter:description" content="你好，我是吴磊。
上一讲我们说，Spark SQL 已经取代 Spark Core 成为了新一代的内核优化引擎，所有 Spark 子框架都能共享 Spark SQL 带来的性能红利，所以在 Spark 历次发布的新版本中，Spark SQL 占比最大。因此，Spark SQL 的优化过程是我们必须要掌握的。">

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Docs</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Docs</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">21__Catalyst逻辑计划：你的SQL语句是怎么被优化的？（上）</h1>

      <div class="post-meta">
        <span class="post-time"> 10100-01-10 </span>
        <div class="post-category">
            <a href="/categories/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/"> 极客时间 </a>
            </div>
          <span class="more-meta"> 约 5633 字 </span>
          <span class="more-meta"> 预计阅读 12 分钟 </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#案例小-q-变身记">案例：小 Q 变身记</a></li>
        <li><a href="#逻辑计划解析">逻辑计划解析</a></li>
        <li><a href="#逻辑计划优化">逻辑计划优化</a>
          <ul>
            <li><a href="#catalyst-的优化规则">Catalyst 的优化规则</a></li>
            <li><a href="#catalys-的优化过程">Catalys 的优化过程</a></li>
            <li><a href="#cache-manager-优化">Cache Manager 优化</a></li>
          </ul>
        </li>
        <li><a href="#小结">小结</a></li>
        <li><a href="#每日一练">每日一练</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p>你好，我是吴磊。</p>
<p>上一讲我们说，Spark SQL 已经取代 Spark Core 成为了新一代的内核优化引擎，所有 Spark 子框架都能共享 Spark SQL 带来的性能红利，所以在 Spark 历次发布的新版本中，Spark SQL 占比最大。因此，Spark SQL 的优化过程是我们必须要掌握的。</p>
<p>Spark SQL 端到端的完整优化流程主要包括两个阶段：Catalyst 优化器和 Tungsten。其中，Catalyst 优化器又包含逻辑优化和物理优化两个阶段。为了把开发者的查询优化到极致，整个优化过程的运作机制设计得都很精密，因此我会用三讲的时间带你详细探讨。</p>
<p>下图就是这个过程的完整图示，你可以先通过它对优化流程有一个整体的认知。然后随着我的讲解，逐渐去夯实其中的关键环节、重要步骤和核心知识点，在深入局部优化细节的同时，把握全局优化流程，做到既见树木、也见森林。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/Spark%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E5%AE%9E%E6%88%98/31ff6bf43ec44af7314082661584c902.png" alt=""></p>
<p>Spark SQL 的优化过程</p>
<p>今天这一讲，我们先来说说 Catalyst 优化器逻辑优化阶段的工作原理。</p>
<h2 id="案例小-q-变身记">案例：小 Q 变身记</h2>
<p>我们先来看一个例子，例子来自电子商务场景，业务需求很简单：给定交易事实表 transactions 和用户维度表 users，统计不同用户的交易额，数据源以 Parquet 的格式存储在分布式文件系统。因此，我们要先用 Parquet API 读取源文件。</p>
<p>val userFile: String = _<br>
val usersDf = spark.read.parquet(userFile)<br>
usersDf.printSchema<br>
/**<br>
root<br>
|&ndash; userId: integer (nullable = true)<br>
|&ndash; name: string (nullable = true)<br>
|&ndash; age: integer (nullable = true)<br>
|&ndash; gender: string (nullable = true)<br>
|&ndash; email: string (nullable = true)<br>
*/<br>
val users = usersDf<br>
.select(&ldquo;name&rdquo;, &ldquo;age&rdquo;, &ldquo;userId&rdquo;)<br>
.filter($&ldquo;age&rdquo; &lt; 30)<br>
.filter($&ldquo;gender&rdquo;.isin(&ldquo;M&rdquo;))</p>
<p>val txFile: String = _<br>
val txDf = spark.read.parquet(txFile)<br>
txDf.printSchema<br>
/**<br>
root<br>
|&ndash; itemId: integer (nullable = true)<br>
|&ndash; userId: integer (nullable = true)<br>
|&ndash; price: float (nullable = true)<br>
|&ndash; quantity: integer (nullable = true)<br>
*/</p>
<p>val result = txDF.select(&ldquo;price&rdquo;, &ldquo;volume&rdquo;, &ldquo;userId&rdquo;)<br>
.join(users, Seq(&ldquo;userId&rdquo;), &ldquo;inner&rdquo;)<br>
.groupBy(col(&ldquo;name&rdquo;), col(&ldquo;age&rdquo;)).agg(sum(col(&ldquo;price&rdquo;) * col(&ldquo;volume&rdquo;)).alias(&ldquo;revenue&rdquo;))</p>
<p>result.write.parquet(&quot;_&quot;)</p>
<p>代码示例如上图所示，为了实现业务逻辑，我们对过滤之后的用户表与交易表做内关联，然后再按照用户分组去计算交易额。不难发现，这个计算逻辑实际上就是星型数仓中典型的关联查询。为了叙述方便，我们给这个关联查询起个名字：小 Q。小 Q 的计算需要两个输入源，一个是交易表，另一个是过滤之后的用户表。今天这一讲，我们就去追随小 Q，看看它在 Catalyst 的逻辑优化阶段都会发生哪些变化。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/Spark%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E5%AE%9E%E6%88%98/ca2e1f6d665d4d018bae8c8ac9ecb6e8.png" alt=""></p>
<p>小 Q 的计算逻辑</p>
<p><strong>Catalyst 逻辑优化阶段分为两个环节：逻辑计划解析和逻辑计划优化。在逻辑计划解析中，Catalyst 把“Unresolved Logical Plan”转换为“Analyzed Logical Plan”；在逻辑计划优化中，Catalyst 基于一些既定的启发式规则（Heuristics Based Rules），把“Analyzed Logical Plan”转换为“Optimized Logical Plan”。</strong></p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/Spark%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E5%AE%9E%E6%88%98/2b61f395185fb4b6b204c6f2a9f7d885.png" alt=""></p>
<p>Catalyst 逻辑优化阶段</p>
<p>因为“Unresolved Logical Plan”是 Catalyst 优化的起点，所以在进入 Catalyst 优化器之前，小 Q 先是改头换面，从代码中的查询语句，摇身变成了“Unresolved Logical Plan”。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/Spark%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E5%AE%9E%E6%88%98/1dfc5bc21fe4a654bc34df90abfca58a.png" alt=""></p>
<p>小 Q 启程：Unresolved Logical Plan</p>
<h2 id="逻辑计划解析">逻辑计划解析</h2>
<p>小 Q 成功进入 Catalyst 优化器之后，就要开始执行逻辑计划解析，也就是要从“Unresolved Logical Plan”转换为“Analyzed Logical Plan”。那么，具体该怎么做呢？</p>
<p>从“小 Q 启程”那张图我们不难发现，“Unresolved Logical Plan”携带的信息相当有限，它只包含查询语句从 DSL 语法变换成 AST 语法树的信息。需要说明的是，不论是逻辑计划还是物理计划，执行的次序都是自下向上。因此，图中逻辑计划的计算顺序是从全表扫描到按性别过滤，每个步骤的含义都是准备“做什么”。</p>
<p>例如，在计划的最底层，Relation 节点“告诉”Catalyst：“你需要扫描一张表，这张表有 4 个字段，分别是 ABCD，文件格式是 Parquet”。但这些信息对于小 Q 的优化还远远不够，我们还需要知道这张表的 Schema 是啥？字段的类型都是什么？字段名是否真实存在？数据表中的字段名与计划中的字段名是一致的吗？</p>
<p>因此，**在逻辑计划解析环节，Catalyst 就是要结合 DataFrame 的 Schema 信息，来确认计划中的表名、字段名、字段类型与实际数据是否一致。**完成确认之后，Catalyst 会生成“Analyzed Logical Plan”。这个时候，小 Q 就会从“Unresolved Logical Plan”转换成“Analyzed Logical Plan”。</p>
<p>从下图中我们能够看到，逻辑计划已经完成了一致性检查，并且可以识别两张表的字段类型，比如 userId 的类型是 int，price 字段的类型是 double 等等。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/Spark%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E5%AE%9E%E6%88%98/5b041eeff293de0be4dcfb3666d57914.png" alt=""></p>
<p>小 Q 再变身：Analyzed Logical Plan</p>
<h2 id="逻辑计划优化">逻辑计划优化</h2>
<p>对于现在的小 Q 来说，如果我们不做任何优化，直接把它转换为物理计划也可以。但是，这种照搬开发者的计算步骤去制定物理计划的方式，它的执行效率往往不是最优的。</p>
<p>为什么这么说呢？在运行时，Spark 会先全量扫描 Parquet 格式的用户表，然后遴选出 userId、name、age、gender 四个字段，接着分别按照年龄和性别对数据进行过滤。</p>
<p>对于这样的执行计划来说，最开始的全量扫描显然是一种浪费。原因主要有两方面：一方面，查询实际上只涉及 4 个字段，并不需要 email 这一列数据；另一方面，字段 age 和 gender 上带有过滤条件，我们完全可以利用这些过滤条件减少需要扫描的数据量。</p>
<p>由此可见，<strong>对于同样一种计算逻辑，实现方式可以有多种，按照不同的顺序对算子做排列组合，我们就可以演化出不同的实现方式。最好的方式是，我们遵循“能省则省、能拖则拖”的开发原则，去选择所有实现方式中最优的那个</strong>。</p>
<p>同样，在面对这种“选择题”的时候，Catalyst 也有一套自己的“原则”和逻辑。因此，生成“Analyzed Logical Plan”之后，Catalyst 并不会止步于此，它会基于一套启发式的规则，把“Analyzed Logical Plan”转换为“Optimized Logical Plan”。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/Spark%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E5%AE%9E%E6%88%98/0d715ab0eb76b1ad4802484f984b7426.png" alt=""></p>
<p>逻辑优化环节</p>
<p>那么问题来了，Catalyst 都有哪些既定的规则和逻辑呢？基于这些规则，Catalyst 又是怎么做转换的呢？别着急，我们一个一个来解答，咱们先来说说 Catalyst 的优化规则，然后再去探讨逻辑计划的转换过程。</p>
<h3 id="catalyst-的优化规则">Catalyst 的优化规则</h3>
<p>和 Catalyst 相比，咱们总结出的开发原则简直就是小巫见大巫，为什么这么说呢？在新发布的 Spark 3.0 版本中，Catalyst 总共有 81 条优化规则（Rules），这 81 条规则会分成 27 组（Batches），其中有些规则会被收纳到多个分组里。因此，如果不考虑规则的重复性，27 组算下来总共会有 129 个优化规则。</p>
<p>对于如此多的优化规则，我们该怎么学呢？实际上，如果从优化效果的角度出发，这些规则可以归纳到以下 3 个范畴：</p>
<ol>
<li><strong>谓词下推（Predicate Pushdown）</strong></li>
<li><strong>列剪裁（Column Pruning）</strong></li>
<li><strong>常量替换（Constant Folding）</strong></li>
</ol>
<p>首先，我们来说说谓词下推谓词下推主要是围绕着查询中的过滤条件做文章。其中，<strong>“谓词”指代的是像用户表上“age &lt; 30”这样的过滤条件，“下推”指代的是把这些谓词沿着执行计划向下，推到离数据源最近的地方，从而在源头就减少数据扫描量</strong>。换句话说，让这些谓词越接近数据源越好。</p>
<p>不过，在下推之前，Catalyst 还会先对谓词本身做一些优化，比如像 OptimizeIn 规则，它会把“gender in‘M’”优化成“gender = ‘M’”，也就是把谓词 in 替换成等值谓词。再比如，CombineFilters 规则，它会把“age &lt; 30”和“gender = ‘M’”这两个谓词，捏合成一个谓词：“age != null AND gender != null AND age &lt;30 AND gender = ‘M’”。</p>
<p>完成谓词本身的优化之后，Catalyst 再用 PushDownPredicte 优化规则，把谓词推到逻辑计划树最下面的数据源上。对于 Parquet、ORC 这类存储格式，结合文件注脚（Footer）中的统计信息，下推的谓词能够大幅减少数据扫描量，降低磁盘 I/O 开销。</p>
<p>再来说说列剪裁。**列剪裁就是扫描数据源的时候，只读取那些与查询相关的字段。**以小 Q 为例，用户表的 Schema 是（userId、name、age、gender、email），但是查询中压根就没有出现过 email 的引用，因此，Catalyst 会使用 ColumnPruning 规则，把 email 这一列“剪掉”。经过这一步优化，Spark 在读取 Parquet 文件的时候就会跳过 email 这一列，从而节省 I/O 开销。</p>
<p>不难发现，谓词下推与列剪裁的优化动机，其实和“能省则省”的原则一样。核心思想都是用尽一切办法，减少需要扫描和处理的数据量，降低后续计算的负载。</p>
<p>最后一类优化是常量替换，它的逻辑比较简单。假设我们在年龄上加的过滤条件是“age &lt; 12 + 18”，Catalyst 会使用 ConstantFolding 规则，自动帮我们把条件变成“age &lt; 30”。再比如，我们在 select 语句中，掺杂了一些常量表达式，Catalyst 也会自动地用表达式的结果进行替换。</p>
<p>到此为止，咱们从功用和效果的角度，探讨了 Catalyst 逻辑优化规则的 3 大范畴。你可能说：“拢共就做了这么 3 件事，至于兴师动众地制定 81 条规则吗？”我们划分这 3 大范畴，主要是为了叙述和理解上的方便。实际上，对于开发者写出的五花八门、千奇百怪的查询语句，正是因为 Catalyst 不断丰富的优化规则，才让这些查询都能够享有不错的执行性能。如果没有这些优化规则的帮忙，小 Q 的执行性能一定会惨不忍睹。</p>
<p>最终，被 Catalyst 优化过后的小 Q，就从“Analyzed Logical Plan”转换为“Optimized Logical Plan”，如下图所示。我们可以看到，谓词下推和列剪裁都体现到了 Optimized Logical Plan 中。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/Spark%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E5%AE%9E%E6%88%98/71e090b16606c7cb9ca3aefad1982848.png" alt=""></p>
<p>小 Q 再变身：Optimized Logical Plan</p>
<h3 id="catalys-的优化过程">Catalys 的优化过程</h3>
<p>接下来，我继续来回答刚刚提出的第二个问题：基于这么多优化规则，Catalyst 具体是怎么把“Analyzed Logical Plan”转换成“Optimized Logical Plan”的呢？其实，不管是逻辑计划（Logical Plan）还是物理计划（Physical Plan），它们都继承自 QueryPlan。</p>
<p>QueryPlan 的父类是 TreeNode，TreeNode 就是语法树中对于节点的抽象。TreeNode 有一个名叫 children 的字段，类型是 Seq[TreeNode]，利用 TreeNode 类型，Catalyst 可以很容易地构建一个树结构。</p>
<p>除了 children 字段，TreeNode 还定义了很多高阶函数，其中最值得关注的是一个叫做 transformDown 的方法。transformDown 的形参，正是 Catalyst 定义的各种优化规则，方法的返回类型还是 TreeNode。另外，transformDown 是个递归函数，参数的优化规则会先作用（Apply）于当前节点，然后依次作用到 children 中的子节点，直到整棵树的叶子节点。</p>
<p>**总的来说，从“Analyzed Logical Plan”到“Optimized Logical Plan”的转换，就是从一个 TreeNode 生成另一个 TreeNode 的过程。**Analyzed Logical Plan 的根节点，通过调用 transformDown 方法，不停地把各种优化规则作用到整棵树，直到把所有 27 组规则尝试完毕，且树结构不再发生变化为止。这个时候，生成的 TreeNode 就是 Optimized Logical Plan。</p>
<p>为了把复杂问题简单化，我们使用 Expression，也就是表达式来解释一下这个过程。因为 Expression 本身也继承自 TreeNode，所以明白了这个例子，TreeNode 之间的转换我们也就清楚了。</p>
<p>//Expression 的转换<br>
import org.apache.spark.sql.catalyst.expressions._<br>
val myExpr: Expression = Multiply(Subtract(Literal(6), Literal(4)), Subtract(Literal(1), Literal(9)))<br>
val transformed: Expression = myExpr transformDown {<br>
case BinaryOperator(l, r) =&gt; Add(l, r)<br>
case IntegerLiteral(i) if i &gt; 5 =&gt; Literal(1)<br>
case IntegerLiteral(i) if i &lt; 5 =&gt; Literal(0)<br>
}</p>
<p>首先，我们定义了一个表达式：（（6 - 4）*（1 - 9）），然后我们调用这个表达式的 transformDown 高阶函数。在高阶函数中，我们提供了一个用 case 定义的匿名函数。显然，这是一个偏函数（Partial Functions），你可以把这个匿名函数理解成“自定义的优化规则”。在这个优化规则中，我们仅考虑 3 种情况：</p>
<ol>
<li>对于所有的二元操作符，我们都把它转化成加法操作</li>
<li>对于所有大于 5 的数字，我们都把它变成 1</li>
<li>对于所有小于 5 的数字，我们都把它变成 0</li>
</ol>
<p>虽然我们的优化规则没有任何实质性的意义，仅仅是一种转换规则而已，但是这并不妨碍你去理解 Catalyst 中 TreeNode 之间的转换。当我们把这个规则应用到表达式（（6 - 4）*（1 - 9））之后，得到的结果是另外一个表达式（（1 + 0）+（0 + 1）），下面的示意图直观地展示了这个过程。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/Spark%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E5%AE%9E%E6%88%98/babce2fdb4eef0fcfa64601a0a74558d.png" alt=""></p>
<p>自顶向下对执行计划进行转换</p>
<p>从“Analyzed Logical Plan”到“Optimized Logical Plan”的转换，与示例中表达式的转换过程如出一辙。最主要的区别在于，Catalyst 的优化规则要复杂、精密得多。</p>
<h3 id="cache-manager-优化">Cache Manager 优化</h3>
<p>从“Analyzed Logical Plan”到“Optimized Logical Plan”的转换，Catalyst 除了使用启发式的规则以外，还会利用 Cache Manager 做进一步的优化。</p>
<p><strong>这里的 Cache 指的就是我们常说的分布式数据缓存。想要对数据进行缓存，你可以调用 DataFrame 的.cache 或.persist，或是在 SQL 语句中使用“cache table”关键字</strong>。</p>
<p>Cache Manager 其实很简单，它的主要职责是维护与缓存有关的信息。具体来说，Cache Manager 维护了一个 Mapping 映射字典，字典的 Key 是逻辑计划，Value 是对应的 Cache 元信息。</p>
<p>当 Catalyst 尝试对逻辑计划做优化时，会先尝试对 Cache Manager 查找，看看当前的逻辑计划或是逻辑计划分支，是否已经被记录在 Cache Manager 的字典里。如果在字典中可以查到当前计划或是分支，Catalyst 就用 InMemoryRelation 节点来替换整个计划或是计划的一部分，从而充分利用已有的缓存数据做优化。</p>
<h2 id="小结">小结</h2>
<p>今天这一讲，我们主要探讨了 Catalyst 优化器的逻辑优化阶段。这个阶段包含两个环节：逻辑计划解析和逻辑计划优化。</p>
<p>在逻辑计划解析环节，Catalyst 结合 Schema 信息，对于仅仅记录语句字符串的 Unresolved Logical Plan，验证表名、字段名与实际数据的一致性。解析后的执行计划称为 Analyzed Logical Plan。</p>
<p>在逻辑计划优化环节，Catalyst 会同时利用 3 方面的力量优化 Analyzed Logical Plan，分别是 AQE、Cache Manager 和启发式的规则。它们当中，Catalyst 最倚重的是启发式的规则。</p>
<p>尽管启发式的规则多达 81 项，但我们把它们归纳为 3 大范畴：谓词下推、列剪裁和常量替换。我们要重点掌握谓词下推和列剪裁，它们的优化动机和“能省则省”的开发原则一样，核心思想都是用尽一切办法，减少需要扫描和处理的数据量，降低后续计算的负载。</p>
<p>针对所有的优化规则，Catalyst 优化器会通过调用 TreeNode 中的 transformDown 高阶函数，分别把它们作用到逻辑计划的每一个节点上，直到逻辑计划的结构不再改变为止，这个时候生成的逻辑计划就是 Optimized Logical Plan。</p>
<p>最后，Cache Manager 的作用是提供逻辑计划与数据缓存的映射关系，当现有逻辑计划或是分支出现在 Cache Manager 维护的映射字典的时候，Catalyst 可以充分利用已有的缓存数据来优化。</p>
<h2 id="每日一练">每日一练</h2>
<ol>
<li>既然 Catalyst 在逻辑优化阶段有 81 条优化规则，我们还需要遵循“能省则省、能拖则拖”的开发原则吗？</li>
<li>你能说说 Spark 为什么用偏函数，而不是普通函数来定义 Catalyst 的优化规则吗？</li>
</ol>
<p>期待在留言区看到你的思考和答案，我们下一讲见！</p>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content"></span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
        10100-01-10
        
    </span>
  </p>
  
  
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/spark%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E5%AE%9E%E6%88%98/">Spark性能调优实战</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A/21__akf%E7%AB%8B%E6%96%B9%E4%BD%93%E6%80%8E%E6%A0%B7%E9%80%9A%E8%BF%87%E5%8F%AF%E6%89%A9%E5%B1%95%E6%80%A7%E6%9D%A5%E6%8F%90%E9%AB%98%E6%80%A7%E8%83%BD/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">21__AKF立方体：怎样通过可扩展性来提高性能？</span>
            <span class="prev-text nav-mobile">上一篇</span>
          </a>
        <a class="next" href="/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E6%B5%8F%E8%A7%88%E5%99%A8%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/21__chrome%E5%BC%80%E5%8F%91%E8%80%85%E5%B7%A5%E5%85%B7%E5%88%A9%E7%94%A8%E7%BD%91%E7%BB%9C%E9%9D%A2%E6%9D%BF%E5%81%9A%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/">
            <span class="next-text nav-default">21__Chrome开发者工具：利用网络面板做性能分析</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
  
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2024 - 
    2025<span class="heart"><i class="iconfont icon-heart"></i></span><span></span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  <script src="/lib/highlight/highlight.pack.js?v=20171001"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js"></script>


      <script async src="https://www.googletagmanager.com/gtag/js?id=G-FVZ07KBD4X"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-FVZ07KBD4X');
        }
      </script>






</body>
</html>
