<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>13__容器磁盘限速：我的容器里磁盘读写为什么不稳定_ - Docs</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="" /><meta name="description" content="你好，我是程远。今天我们聊一聊磁盘读写不稳定的问题。
上一讲，我给你讲了如何通过 XFS Quota 来限制容器文件系统的大小，这是静态容量大小的一个限制。
你也许会马上想到，磁盘除了容量的划分，还有一个读写性能的问题。
" /><meta name="keywords" content="技术文档, docs, 极客时间" />






<meta name="generator" content="Hugo 0.140.2 with theme even" />


<link rel="canonical" href="https://politcloud.org/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E5%AE%B9%E5%99%A8%E5%AE%9E%E6%88%98%E9%AB%98%E6%89%8B%E8%AF%BE/13__%E5%AE%B9%E5%99%A8%E7%A3%81%E7%9B%98%E9%99%90%E9%80%9F%E6%88%91%E7%9A%84%E5%AE%B9%E5%99%A8%E9%87%8C%E7%A3%81%E7%9B%98%E8%AF%BB%E5%86%99%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E7%A8%B3%E5%AE%9A_/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:url" content="https://politcloud.org/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E5%AE%B9%E5%99%A8%E5%AE%9E%E6%88%98%E9%AB%98%E6%89%8B%E8%AF%BE/13__%E5%AE%B9%E5%99%A8%E7%A3%81%E7%9B%98%E9%99%90%E9%80%9F%E6%88%91%E7%9A%84%E5%AE%B9%E5%99%A8%E9%87%8C%E7%A3%81%E7%9B%98%E8%AF%BB%E5%86%99%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E7%A8%B3%E5%AE%9A_/">
  <meta property="og:site_name" content="Docs">
  <meta property="og:title" content="13__容器磁盘限速：我的容器里磁盘读写为什么不稳定_">
  <meta property="og:description" content="你好，我是程远。今天我们聊一聊磁盘读写不稳定的问题。
上一讲，我给你讲了如何通过 XFS Quota 来限制容器文件系统的大小，这是静态容量大小的一个限制。
你也许会马上想到，磁盘除了容量的划分，还有一个读写性能的问题。">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2024-01-10T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-01-10T00:00:00+00:00">
    <meta property="article:tag" content="容器实战高手课">

  <meta itemprop="name" content="13__容器磁盘限速：我的容器里磁盘读写为什么不稳定_">
  <meta itemprop="description" content="你好，我是程远。今天我们聊一聊磁盘读写不稳定的问题。
上一讲，我给你讲了如何通过 XFS Quota 来限制容器文件系统的大小，这是静态容量大小的一个限制。
你也许会马上想到，磁盘除了容量的划分，还有一个读写性能的问题。">
  <meta itemprop="datePublished" content="2024-01-10T00:00:00+00:00">
  <meta itemprop="dateModified" content="2024-01-10T00:00:00+00:00">
  <meta itemprop="wordCount" content="4772">
  <meta itemprop="keywords" content="容器实战高手课">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="13__容器磁盘限速：我的容器里磁盘读写为什么不稳定_">
  <meta name="twitter:description" content="你好，我是程远。今天我们聊一聊磁盘读写不稳定的问题。
上一讲，我给你讲了如何通过 XFS Quota 来限制容器文件系统的大小，这是静态容量大小的一个限制。
你也许会马上想到，磁盘除了容量的划分，还有一个读写性能的问题。">

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Docs</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Docs</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">13__容器磁盘限速：我的容器里磁盘读写为什么不稳定_</h1>

      <div class="post-meta">
        <span class="post-time"> 10100-01-10 </span>
        <div class="post-category">
            <a href="/categories/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/"> 极客时间 </a>
            </div>
          <span class="more-meta"> 约 4772 字 </span>
          <span class="more-meta"> 预计阅读 10 分钟 </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#场景再现">场景再现</a></li>
        <li><a href="#知识详解">知识详解</a>
          <ul>
            <li><a href="#blkio-cgroup">Blkio Cgroup</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#ls--l-devvdb--l">ls -l /dev/vdb -l</a></li>
    <li><a href="#to-get-the-device-major-and-minor-id-from-dev-for-the-device-that-tmptest1-is-on">To get the device major and minor id from /dev for the device that /tmp/test1 is on.</a></li>
    <li><a href="#to-get-the-device-major-and-minor-id-from-dev-for-the-device-that-tmptest1-is-on-1">To get the device major and minor id from /dev for the device that /tmp/test1 is on.</a></li>
    <li><a href="#to-get-the-device-major-and-minor-id-from-dev-for-the-device-that-tmptest1-is-on-2">To get the device major and minor id from /dev for the device that /tmp/test1 is on.</a>
      <ul>
        <li>
          <ul>
            <li><a href="#direct-io-和-buffered-io">Direct I/O 和 Buffered I/O</a></li>
          </ul>
        </li>
        <li><a href="#cgroup-v2">Cgroup V2</a></li>
      </ul>
    </li>
    <li><a href="#create-a-new-control-group">Create a new control group</a></li>
    <li><a href="#enable-the-io-and-memory-controller-subsystem">enable the io and memory controller subsystem</a></li>
    <li><a href="#add-current-bash-pid-in-iotest-control-group">Add current bash pid in iotest control group.</a></li>
    <li><a href="#then-all-child-processes-of-the-bash-will-be-in-iotest-group-too">Then all child processes of the bash will be in iotest group too,</a></li>
    <li><a href="#including-the-fio">including the fio</a></li>
    <li><a href="#25616-are-device-major-and-minor-ids-mnt-is-on-the-device">256:16 are device major and minor ids, /mnt is on the device.</a>
      <ul>
        <li><a href="#重点总结">重点总结</a></li>
        <li><a href="#思考题">思考题</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p>你好，我是程远。今天我们聊一聊磁盘读写不稳定的问题。</p>
<p>上一讲，我给你讲了如何通过 XFS Quota 来限制容器文件系统的大小，这是静态容量大小的一个限制。</p>
<p>你也许会马上想到，磁盘除了容量的划分，还有一个读写性能的问题。</p>
<p>具体来说，就是如果多个容器同时读写节点上的同一块磁盘，那么它们的磁盘读写相互之间影响吗？如果容器之间读写磁盘相互影响，我们有什么办法解决呢？</p>
<p>接下来，我们就带着问题一起学习今天的内容。</p>
<h2 id="场景再现">场景再现</h2>
<p>我们先用这里的代码，运行一下 <code>make image</code> 来做一个带 fio 的容器镜像，fio 在我们之前的课程里提到过，它是用来测试磁盘文件系统读写性能的工具。</p>
<p>有了这个带 fio 的镜像，我们可以用它启动一个容器，在容器中运行 fio，就可以得到只有一个容器读写磁盘时的性能数据。</p>
<p>mkdir -p /tmp/test1<br>
docker stop fio_test1;docker rm fio_test1<br>
docker run &ndash;name fio_test1 &ndash;volume /tmp/test1:/tmp  registery/fio:v1 fio -direct=1 -rw=write -ioengine=libaio -bs=4k -size=1G -numjobs=1  -name=/tmp/fio_test1.log</p>
<p>上面的这个 Docker 命令，我给你简单地解释一下：在这里我们第一次用到了&quot;&ndash;volume&quot;这个参数。之前我们讲过容器文件系统，比如 OverlayFS。</p>
<p>不过容器文件系统并不适合频繁地读写。对于频繁读写的数据，容器需要把他们到放到&quot;volume&quot;中。这里的 volume 可以是一个本地的磁盘，也可以是一个网络磁盘。</p>
<p>在这个例子里我们就使用了宿主机本地磁盘，把磁盘上的 /tmp/test1 目录作为 volume 挂载到容器的 /tmp 目录下。</p>
<p>然后在启动容器之后，我们直接运行 fio 的命令，这里的参数和我们第 11 讲最开始的例子差不多，只是这次我们运行的是 write，也就是写磁盘的操作，而写的目标盘就是挂载到 /tmp 目录的 volume。</p>
<p>可以看到，fio 的运行结果如下图所示，IOPS 是 18K，带宽 (BW) 是 70MB/s 左右。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%AE%B9%E5%99%A8%E5%AE%9E%E6%88%98%E9%AB%98%E6%89%8B%E8%AF%BE/23db24d7da44d9dcf00f4020b9056b2e.png" alt=""></p>
<p>好了，刚才我们模拟了一个容器写磁盘的性能。那么如果这时候有两个容器，都在往同一个磁盘上写数据又是什么情况呢？我们可以再用下面的这个脚本试一下：</p>
<p>mkdir -p /tmp/test1<br>
mkdir -p /tmp/test2</p>
<p>docker stop fio_test1;docker rm fio_test1<br>
docker stop fio_test2;docker rm fio_test2</p>
<p>docker run &ndash;name fio_test1 &ndash;volume /tmp/test1:/tmp  registery/fio:v1 fio -direct=1 -rw=write -ioengine=libaio -bs=4k -size=1G -numjobs=1  -name=/tmp/fio_test1.log &amp;</p>
<p>docker run &ndash;name fio_test2 &ndash;volume /tmp/test2:/tmp  registery/fio:v1 fio -direct=1 -rw=write -ioengine=libaio -bs=4k -size=1G -numjobs=1  -name=/tmp/fio_test2.log &amp;</p>
<p>这时候，我们看到的结果，在容器 fio_test1 里，IOPS 是 15K 左右，带宽是 59MB/s 了，比之前单独运行的时候性能下降了不少。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%AE%B9%E5%99%A8%E5%AE%9E%E6%88%98%E9%AB%98%E6%89%8B%E8%AF%BE/f45312b8153cf4f2e4206cfaf6e302cc.png" alt=""></p>
<p>显然从这个例子中，我们可以看到多个容器同时写一块磁盘的时候，它的性能受到了干扰。那么有什么办法可以保证每个容器的磁盘读写性能呢？</p>
<p>之前，我们讨论过用 Cgroups 来保证容器的 CPU 使用率，以及控制 Memroy 的可用大小。那么你肯定想到了，我们是不是也可以用 Cgroups 来保证每个容器的磁盘读写性能？</p>
<p>没错，在 Cgroup v1 中有 blkio 子系统，它可以来限制磁盘的 I/O。不过 blkio 子系统对于磁盘 I/O 的限制，并不像 CPU，Memory 那么直接，下面我会详细讲解。</p>
<h2 id="知识详解">知识详解</h2>
<h3 id="blkio-cgroup">Blkio Cgroup</h3>
<p>在讲解 blkio Cgroup 前，我们先简单了解一下衡量磁盘性能的**两个常见的指标 IOPS 和吞吐量（Throughput）**是什么意思，后面讲 Blkio Cgroup 的参数配置时会用到。</p>
<p>IOPS 是 Input/Output Operations Per Second 的简称，也就是每秒钟磁盘读写的次数，这个数值越大，当然也就表示性能越好。</p>
<p>吞吐量（Throughput）是指每秒钟磁盘中数据的读取量，一般以 MB/s 为单位。这个读取量可以叫作吞吐量，有时候也被称为带宽（Bandwidth）。刚才我们用到的 fio 显示结果就体现了带宽。</p>
<p>IOPS 和吞吐量之间是有关联的，在 IOPS 固定的情况下，如果读写的每一个数据块越大，那么吞吐量也越大，它们的关系大概是这样的：吞吐量 = 数据块大小 *IOPS。</p>
<p>好，那么我们再回到 blkio Cgroup 这个概念上，blkio Cgroup 也是 Cgroups 里的一个子系统。在 Cgroups v1 里，blkio Cgroup 的虚拟文件系统挂载点一般在&quot;/sys/fs/cgroup/blkio/&quot;。</p>
<p>和我之前讲过的 CPU，memory Cgroup 一样，我们在这个&quot;/sys/fs/cgroup/blkio/&ldquo;目录下创建子目录作为控制组，再把需要做 I/O 限制的进程 pid 写到控制组的 cgroup.procs 参数中就可以了。</p>
<p>在 blkio Cgroup 中，有四个最主要的参数，它们可以用来限制磁盘 I/O 性能，我列在了下面。</p>
<p>blkio.throttle.read_iops_device<br>
blkio.throttle.read_bps_device<br>
blkio.throttle.write_iops_device<br>
blkio.throttle.write_bps_device</p>
<p>前面我们刚说了磁盘 I/O 的两个主要性能指标 IOPS 和吞吐量，在这里，根据这四个参数的名字，估计你已经大概猜到它们的意思了。</p>
<p>没错，它们分别表示：磁盘读取 IOPS 限制，磁盘读取吞吐量限制，磁盘写入 IOPS 限制，磁盘写入吞吐量限制。</p>
<p>对于每个参数写入值的格式，你可以参考内核blkio 的文档。为了让你更好地理解，在这里我给你举个例子。</p>
<p>如果我们要对一个控制组做限制，限制它对磁盘 /dev/vdb 的写入吞吐量不超过 10MB/s，那么我们对 blkio.throttle.write_bps_device 参数的配置就是下面这个命令。</p>
<p>echo &ldquo;252:16 10485760&rdquo; &gt; $CGROUP_CONTAINER_PATH/blkio.throttle.write_bps_device</p>
<p>在这个命令中，&ldquo;252:16&quot;是 /dev/vdb 的主次设备号，你可以通过 <code>ls -l /dev/vdb</code> 看到这两个值，而后面的&quot;10485760&quot;就是 10MB 的每秒钟带宽限制。</p>
<h1 id="ls--l-devvdb--l">ls -l /dev/vdb -l</h1>
<p>brw-rw&mdash;- 1 root disk 252, 16 Nov  2 08:02 /dev/vdb</p>
<p>了解了 blkio Cgroup 的参数配置，我们再运行下面的这个例子，限制一个容器 blkio 的读写磁盘吞吐量，然后在这个容器里运行一下 fio，看看结果是什么。</p>
<p>mkdir -p /tmp/test1<br>
rm -f /tmp/test1/*</p>
<p>docker stop fio_test1;docker rm fio_test1</p>
<p>docker run -d &ndash;name fio_test1 &ndash;volume /tmp/test1:/tmp  registery/fio:v1 sleep 3600</p>
<p>sleep 2</p>
<p>CONTAINER_ID=$(sudo docker ps &ndash;format &ldquo;{{.ID}}\t{{.Names}}&rdquo; | grep -i fio_test1 | awk &lsquo;{print $1}&rsquo;)</p>
<p>echo $CONTAINER_ID</p>
<p>CGROUP_CONTAINER_PATH=$(find /sys/fs/cgroup/blkio/ -name &ldquo;<em>$CONTAINER_ID</em>&rdquo;)</p>
<p>echo $CGROUP_CONTAINER_PATH</p>
<h1 id="to-get-the-device-major-and-minor-id-from-dev-for-the-device-that-tmptest1-is-on">To get the device major and minor id from /dev for the device that /tmp/test1 is on.</h1>
<p>echo &ldquo;253:0 10485760&rdquo; &gt; $CGROUP_CONTAINER_PATH/blkio.throttle.read_bps_device</p>
<p>echo &ldquo;253:0 10485760&rdquo; &gt; $CGROUP_CONTAINER_PATH/blkio.throttle.write_bps_device</p>
<p>docker exec fio_test1 fio -direct=1 -rw=write -ioengine=libaio -bs=4k -size=100MB -numjobs=1  -name=/tmp/fio_test1.log</p>
<p>docker exec fio_test1 fio -direct=1 -rw=read -ioengine=libaio -bs=4k -size=100MB -numjobs=1  -name=/tmp/fio_test1.log</p>
<p>在这里，我的机器上 /tmp/test1 所在磁盘主次设备号是”253:0”，你在自己运行这组命令的时候，需要把主次设备号改成你自己磁盘的对应值。</p>
<p>还有一点我要提醒一下，不同数据块大小，在性能测试中可以适用于不同的测试目的。但因为这里不是我们要讲的重点，所以为了方便你理解概念，这里就用固定值。</p>
<p>在我们后面的例子里，fio 读写的数据块都固定在 4KB。所以对于磁盘的性能限制，我们在 blkio Cgroup 里就只设置吞吐量限制了。</p>
<p>在加了 blkio Cgroup 限制 10MB/s 后，从 fio 运行后的输出结果里，我们可以看到这个容器对磁盘无论是读还是写，它的最大值就不会再超过 10MB/s 了。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%AE%B9%E5%99%A8%E5%AE%9E%E6%88%98%E9%AB%98%E6%89%8B%E8%AF%BE/40bd4d165d174a6a4563651e6ac77322.png" alt=""></p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%AE%B9%E5%99%A8%E5%AE%9E%E6%88%98%E9%AB%98%E6%89%8B%E8%AF%BE/d20f09f220d19ec82ce7203e788ad950.png" alt=""></p>
<p>在给每个容器都加了 blkio Cgroup 限制，限制为 10MB/s 后，即使两个容器同时在一个磁盘上写入文件，那么每个容器的写入磁盘的最大吞吐量，也不会互相干扰了。</p>
<p>我们可以用下面的这个脚本来验证一下。</p>
<p>#!/bin/bash</p>
<p>mkdir -p /tmp/test1<br>
rm -f /tmp/test1/*<br>
docker stop fio_test1;docker rm fio_test1</p>
<p>mkdir -p /tmp/test2<br>
rm -f /tmp/test2/*<br>
docker stop fio_test2;docker rm fio_test2</p>
<p>docker run -d &ndash;name fio_test1 &ndash;volume /tmp/test1:/tmp  registery/fio:v1 sleep 3600<br>
docker run -d &ndash;name fio_test2 &ndash;volume /tmp/test2:/tmp  registery/fio:v1 sleep 3600</p>
<p>sleep 2</p>
<p>CONTAINER_ID1=$(sudo docker ps &ndash;format &ldquo;{{.ID}}\t{{.Names}}&rdquo; | grep -i fio_test1 | awk &lsquo;{print $1}&rsquo;)<br>
echo $CONTAINER_ID1</p>
<p>CGROUP_CONTAINER_PATH1=$(find /sys/fs/cgroup/blkio/ -name &ldquo;<em>$CONTAINER_ID1</em>&rdquo;)<br>
echo $CGROUP_CONTAINER_PATH1</p>
<h1 id="to-get-the-device-major-and-minor-id-from-dev-for-the-device-that-tmptest1-is-on-1">To get the device major and minor id from /dev for the device that /tmp/test1 is on.</h1>
<p>echo &ldquo;253:0 10485760&rdquo; &gt; $CGROUP_CONTAINER_PATH1/blkio.throttle.read_bps_device</p>
<p>echo &ldquo;253:0 10485760&rdquo; &gt; $CGROUP_CONTAINER_PATH1/blkio.throttle.write_bps_device</p>
<p>CONTAINER_ID2=$(sudo docker ps &ndash;format &ldquo;{{.ID}}\t{{.Names}}&rdquo; | grep -i fio_test2 | awk &lsquo;{print $1}&rsquo;)<br>
echo $CONTAINER_ID2</p>
<p>CGROUP_CONTAINER_PATH2=$(find /sys/fs/cgroup/blkio/ -name &ldquo;<em>$CONTAINER_ID2</em>&rdquo;)<br>
echo $CGROUP_CONTAINER_PATH2</p>
<h1 id="to-get-the-device-major-and-minor-id-from-dev-for-the-device-that-tmptest1-is-on-2">To get the device major and minor id from /dev for the device that /tmp/test1 is on.</h1>
<p>echo &ldquo;253:0 10485760&rdquo; &gt; $CGROUP_CONTAINER_PATH2/blkio.throttle.read_bps_device</p>
<p>echo &ldquo;253:0 10485760&rdquo; &gt; $CGROUP_CONTAINER_PATH2/blkio.throttle.write_bps_device</p>
<p>docker exec fio_test1 fio -direct=1 -rw=write -ioengine=libaio -bs=4k -size=100MB -numjobs=1  -name=/tmp/fio_test1.log &amp;</p>
<p>docker exec fio_test2 fio -direct=1 -rw=write -ioengine=libaio -bs=4k -size=100MB -numjobs=1  -name=/tmp/fio_test2.log &amp;</p>
<p>我们还是看看 fio 运行输出的结果，这时候，fio_test1 和 fio_test2 两个容器里执行的结果都是 10MB/s 了。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%AE%B9%E5%99%A8%E5%AE%9E%E6%88%98%E9%AB%98%E6%89%8B%E8%AF%BE/d9ca21124a285b8f75323060089c6630.png" alt=""></p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%AE%B9%E5%99%A8%E5%AE%9E%E6%88%98%E9%AB%98%E6%89%8B%E8%AF%BE/d91cbb027126df5608329a3e44bacf07.png" alt=""></p>
<p>那么做到了这一步，我们是不是就可以认为，blkio Cgroup 可以完美地对磁盘 I/O 做限制了呢？</p>
<p>你先别急，我们可以再做个试验，把前面脚本里 fio 命令中的“-direct=1”给去掉，也就是不让 fio 运行在 Direct I/O 模式了，而是用 Buffered I/O 模式再运行一次，看看 fio 执行的输出。</p>
<p>同时我们也可以运行 iostat 命令，查看实际的磁盘写入速度。</p>
<p>这时候你会发现，即使我们设置了 blkio Cgroup，也根本不能限制磁盘的吞吐量了。</p>
<h3 id="direct-io-和-buffered-io">Direct I/O 和 Buffered I/O</h3>
<p>为什么会这样的呢？这就要提到 Linux 的两种文件 I/O 模式了：Direct I/O 和 Buffered I/O。</p>
<p>Direct I/O 模式，用户进程如果要写磁盘文件，就会通过 Linux 内核的文件系统层 (filesystem) -&gt; 块设备层 (block layer) -&gt; 磁盘驱动 -&gt; 磁盘硬件，这样一路下去写入磁盘。</p>
<p>而如果是 Buffered I/O 模式，那么用户进程只是把文件数据写到内存中（Page Cache）就返回了，而 Linux 内核自己有线程会把内存中的数据再写入到磁盘中。<strong>在 Linux 里，由于考虑到性能问题，绝大多数的应用都会使用 Buffered I/O 模式。</strong></p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%AE%B9%E5%99%A8%E5%AE%9E%E6%88%98%E9%AB%98%E6%89%8B%E8%AF%BE/a0f67ebbbbc5557dcdaf5c99c82a566b.png" alt=""></p>
<p>我们通过前面的测试，发现 Direct I/O 可以通过 blkio Cgroup 来限制磁盘 I/O，但是 Buffered I/O 不能被限制。</p>
<p>那通过上面的两种 I/O 模式的解释，你是不是可以想到原因呢？是的，原因就是被 Cgroups v1 的架构限制了。</p>
<p>我们已经学习过了 v1 的 CPU Cgroup，memory Cgroup 和 blkio Cgroup，那么 Cgroup v1 的一个整体结构，你应该已经很熟悉了。它的每一个子系统都是独立的，资源的限制只能在子系统中发生。</p>
<p>就像下面图里的进程 pid_y，它可以分别属于 memory Cgroup 和 blkio Cgroup。但是在 blkio Cgroup 对进程 pid_y 做磁盘 I/O 做限制的时候，blkio 子系统是不会去关心 pid_y 用了哪些内存，哪些内存是不是属于 Page Cache，而这些 Page Cache 的页面在刷入磁盘的时候，产生的 I/O 也不会被计算到进程 pid_y 上面。</p>
<p>就是这个原因，导致了 blkio 在 Cgroups v1 里不能限制 Buffered I/O。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%AE%B9%E5%99%A8%E5%AE%9E%E6%88%98%E9%AB%98%E6%89%8B%E8%AF%BE/e6a89d666b2420293d69509554ec3785.png" alt=""></p>
<p>这个 Buffered I/O 限速的问题，在 Cgroup V2 里得到了解决，其实这个问题也是促使 Linux 开发者重新设计 Cgroup V2 的原因之一。</p>
<h2 id="cgroup-v2">Cgroup V2</h2>
<p>Cgroup v2 相比 Cgroup v1 做的最大的变动就是一个进程属于一个控制组，而每个控制组里可以定义自己需要的多个子系统。</p>
<p>比如下面的 Cgroup V2 示意图里，进程 pid_y 属于控制组 group2，而在 group2 里同时打开了 io 和 memory 子系统（Cgroup V2 里的 io 子系统就等同于 Cgroup v1 里的 blkio 子系统）。</p>
<p>那么，Cgroup 对进程 pid_y 的磁盘 I/O 做限制的时候，就可以考虑到进程 pid_y 写入到 Page Cache 内存的页面了，这样 buffered I/O 的磁盘限速就实现了。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%AE%B9%E5%99%A8%E5%AE%9E%E6%88%98%E9%AB%98%E6%89%8B%E8%AF%BE/9e335f68ae8d9872080b733e04b33038.png" alt=""></p>
<p>下面我们在 Cgroup v2 里，尝试一下设置了 blkio Cgroup+Memory Cgroup 之后，是否可以对 Buffered I/O 进行磁盘限速。</p>
<p>我们要做的第一步，就是在 Linux 系统里打开 Cgroup v2 的功能。因为目前即使最新版本的 Ubuntu Linux 或者 Centos Linux，仍然在使用 Cgroup v1 作为缺省的 Cgroup。</p>
<p>打开方法就是配置一个 kernel 参数&quot;cgroup_no_v1=blkio,memory&rdquo;，这表示把 Cgroup v1 的 blkio 和 Memory 两个子系统给禁止，这样 Cgroup v2 的 io 和 Memory 这两个子系统就打开了。</p>
<p>我们可以把这个参数配置到 grub 中，然后我们重启 Linux 机器，这时 Cgroup v2 的 io 还有 Memory 这两个子系统，它们的功能就打开了。</p>
<p>系统重启后，我们会看到 Cgroup v2 的虚拟文件系统被挂载到了 /sys/fs/cgroup/unified 目录下。</p>
<p>然后，我们用下面的这个脚本做 Cgroup v2 io 的限速配置，并且运行 fio，看看 buffered I/O 是否可以被限速。</p>
<h1 id="create-a-new-control-group">Create a new control group</h1>
<p>mkdir -p /sys/fs/cgroup/unified/iotest</p>
<h1 id="enable-the-io-and-memory-controller-subsystem">enable the io and memory controller subsystem</h1>
<p>echo &ldquo;+io +memory&rdquo; &gt; /sys/fs/cgroup/unified/cgroup.subtree_control</p>
<h1 id="add-current-bash-pid-in-iotest-control-group">Add current bash pid in iotest control group.</h1>
<h1 id="then-all-child-processes-of-the-bash-will-be-in-iotest-group-too">Then all child processes of the bash will be in iotest group too,</h1>
<h1 id="including-the-fio">including the fio</h1>
<p>echo $$ &gt;/sys/fs/cgroup/unified/iotest/cgroup.procs</p>
<h1 id="25616-are-device-major-and-minor-ids-mnt-is-on-the-device">256:16 are device major and minor ids, /mnt is on the device.</h1>
<p>echo &ldquo;252:16 wbps=10485760&rdquo; &gt; /sys/fs/cgroup/unified/iotest/io.max<br>
cd /mnt<br>
#Run the fio in non direct I/O mode<br>
fio -iodepth=1 -rw=write -ioengine=libaio -bs=4k -size=1G -numjobs=1  -name=./fio.test</p>
<p>在这个例子里，我们建立了一个名叫 iotest 的控制组，并且在这个控制组里加入了 io 和 Memory 两个控制子系统，对磁盘最大吞吐量的设置为 10MB。运行 fio 的时候不加&rdquo;-direct=1&quot;，也就是让 fio 运行在 buffered I/O 模式下。</p>
<p>运行 fio 写入 1GB 的数据后，你会发现 fio 马上就执行完了，因为系统上有足够的内存，fio 把数据写入内存就返回了，不过只要你再运行”iostat -xz 10”这个命令，你就可以看到磁盘 vdb 上稳定的写入速率是 10240wkB/s，也就是我们在 io Cgroup 里限制的 10MB/s。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%AE%B9%E5%99%A8%E5%AE%9E%E6%88%98%E9%AB%98%E6%89%8B%E8%AF%BE/330d7c4fe7224c81b635a7b8139aaa0b.png" alt=""></p>
<p>看到这个结果，我们证实了 Cgoupv2 io+Memory 两个子系统一起使用，就可以对 buffered I/O 控制磁盘写入速率。</p>
<h2 id="重点总结">重点总结</h2>
<p>这一讲，我们主要想解决的问题是如何保证容器读写磁盘速率的稳定，特别是当多个容器同时读写同一个磁盘的时候，需要减少相互的干扰。</p>
<p>Cgroup V1 的 blkiio 控制子系统，可以用来限制容器中进程的读写的 IOPS 和吞吐量（Throughput），但是它只能对于 Direct I/O 的读写文件做磁盘限速，对 Buffered I/O 的文件读写，它无法进行磁盘限速。</p>
<p><strong>这是因为 Buffered I/O 会把数据先写入到内存 Page Cache 中，然后由内核线程把数据写入磁盘，而 Cgroup v1 blkio 的子系统独立于 memory 子系统，无法统计到由 Page Cache 刷入到磁盘的数据量。</strong></p>
<p>这个 Buffered I/O 无法被限速的问题，在 Cgroup v2 里被解决了。Cgroup v2 从架构上允许一个控制组里有多个子系统协同运行，这样在一个控制组里只要同时有 io 和 Memory 子系统，就可以对 Buffered I/O 作磁盘读写的限速。</p>
<p>虽然 Cgroup v2 解决了 Buffered I/O 磁盘读写限速的问题，但是在现实的容器平台上也不是能够立刻使用的，还需要等待一段时间。目前从 runC、containerd 到 Kubernetes 都是刚刚开始支持 Cgroup v2，而对生产环境中原有运行 Cgroup v1 的节点要迁移转化成 Cgroup v2 需要一个过程。</p>
<h2 id="思考题">思考题</h2>
<p>最后呢，我给你留一道思考题。其实这是一道操作题，通过这个操作你可以再理解一下 blkio Cgroup 与 Buffered I/O 的关系。</p>
<p>在 Cgroup v1 的环境里，我们在 blkio Cgroup v1 的例子基础上，把 fio 中&quot;direct=1&quot;参数去除之后，再运行 fio，同时运行 iostat 查看实际写入磁盘的速率，确认 Cgroup v1 blkio 无法对 Buffered I/O 限速。</p>
<p>欢迎你在留言区分享你的收获和疑问。如果这篇文章给你带来了启发，也欢迎转发给你的朋友，一起学习和交流。</p>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content"></span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
        10100-01-10
        
    </span>
  </p>
  
  
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/%E5%AE%B9%E5%99%A8%E5%AE%9E%E6%88%98%E9%AB%98%E6%89%8B%E8%AF%BE/">容器实战高手课</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/java%E4%B8%9A%E5%8A%A1%E5%BC%80%E5%8F%91%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF100%E4%BE%8B/13__%E6%97%A5%E5%BF%97%E6%97%A5%E5%BF%97%E8%AE%B0%E5%BD%95%E7%9C%9F%E6%B2%A1%E4%BD%A0%E6%83%B3%E8%B1%A1%E7%9A%84%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">13__日志：日志记录真没你想象的那么简单</span>
            <span class="prev-text nav-mobile">上一篇</span>
          </a>
        <a class="next" href="/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E5%AE%9E%E7%94%A8%E5%AF%86%E7%A0%81%E5%AD%A6/13__%E5%A6%82%E4%BD%95%E9%98%B2%E6%AD%A2%E6%95%B0%E6%8D%AE%E8%A2%AB%E8%B0%83%E5%8C%85/">
            <span class="next-text nav-default">13__如何防止数据被调包？</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
  
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2024 - 
    2025<span class="heart"><i class="iconfont icon-heart"></i></span><span></span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  <script src="/lib/highlight/highlight.pack.js?v=20171001"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js"></script>


      <script async src="https://www.googletagmanager.com/gtag/js?id=G-FVZ07KBD4X"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-FVZ07KBD4X');
        }
      </script>






</body>
</html>
