<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>27｜多模态交互：替代触屏的交互新可能？ - Docs</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="" /><meta name="description" content="你好，我是 Rocky。
今天我们来聊聊多模态交互。到底什么是多模态交互呢？人和一个智能系统交互的时候，存在双方相互理解的过程，也就是双方都通过各种通道去表达，然后也都通过各种通道去分析对方的意图。多模态是站在智能系统一方来表达，它更多强调的是智能系统通过多个通道去捕获人和环境的信息，或者通过多个通道去呈现信息。
" /><meta name="keywords" content="技术文档, docs, 极客时间" />






<meta name="generator" content="Hugo 0.140.2 with theme even" />


<link rel="canonical" href="https://politcloud.org/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E5%9F%BA%E4%BA%8E%E4%BA%BA%E5%9B%A0%E7%9A%84%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C%E8%AE%BE%E8%AE%A1%E8%AF%BE/27%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BA%A4%E4%BA%92%E6%9B%BF%E4%BB%A3%E8%A7%A6%E5%B1%8F%E7%9A%84%E4%BA%A4%E4%BA%92%E6%96%B0%E5%8F%AF%E8%83%BD/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:url" content="https://politcloud.org/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E5%9F%BA%E4%BA%8E%E4%BA%BA%E5%9B%A0%E7%9A%84%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C%E8%AE%BE%E8%AE%A1%E8%AF%BE/27%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BA%A4%E4%BA%92%E6%9B%BF%E4%BB%A3%E8%A7%A6%E5%B1%8F%E7%9A%84%E4%BA%A4%E4%BA%92%E6%96%B0%E5%8F%AF%E8%83%BD/">
  <meta property="og:site_name" content="Docs">
  <meta property="og:title" content="27｜多模态交互：替代触屏的交互新可能？">
  <meta property="og:description" content="你好，我是 Rocky。
今天我们来聊聊多模态交互。到底什么是多模态交互呢？人和一个智能系统交互的时候，存在双方相互理解的过程，也就是双方都通过各种通道去表达，然后也都通过各种通道去分析对方的意图。多模态是站在智能系统一方来表达，它更多强调的是智能系统通过多个通道去捕获人和环境的信息，或者通过多个通道去呈现信息。">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2024-01-10T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-01-10T00:00:00+00:00">
    <meta property="article:tag" content="基于人因的用户体验设计课">

  <meta itemprop="name" content="27｜多模态交互：替代触屏的交互新可能？">
  <meta itemprop="description" content="你好，我是 Rocky。
今天我们来聊聊多模态交互。到底什么是多模态交互呢？人和一个智能系统交互的时候，存在双方相互理解的过程，也就是双方都通过各种通道去表达，然后也都通过各种通道去分析对方的意图。多模态是站在智能系统一方来表达，它更多强调的是智能系统通过多个通道去捕获人和环境的信息，或者通过多个通道去呈现信息。">
  <meta itemprop="datePublished" content="2024-01-10T00:00:00+00:00">
  <meta itemprop="dateModified" content="2024-01-10T00:00:00+00:00">
  <meta itemprop="wordCount" content="4669">
  <meta itemprop="keywords" content="基于人因的用户体验设计课">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="27｜多模态交互：替代触屏的交互新可能？">
  <meta name="twitter:description" content="你好，我是 Rocky。
今天我们来聊聊多模态交互。到底什么是多模态交互呢？人和一个智能系统交互的时候，存在双方相互理解的过程，也就是双方都通过各种通道去表达，然后也都通过各种通道去分析对方的意图。多模态是站在智能系统一方来表达，它更多强调的是智能系统通过多个通道去捕获人和环境的信息，或者通过多个通道去呈现信息。">

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Docs</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Docs</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">27｜多模态交互：替代触屏的交互新可能？</h1>

      <div class="post-meta">
        <span class="post-time"> 10100-01-10 </span>
        <div class="post-category">
            <a href="/categories/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/"> 极客时间 </a>
            </div>
          <span class="more-meta"> 约 4669 字 </span>
          <span class="more-meta"> 预计阅读 10 分钟 </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#多模态交互通道">多模态交互通道</a></li>
        <li><a href="#非接触式交互">非接触式交互</a>
          <ul>
            <li><a href="#避免手势导致劳累">避免手势导致劳累</a></li>
            <li><a href="#手势识别设计受技术的约束">手势识别设计受技术的约束</a></li>
          </ul>
        </li>
        <li><a href="#多模态多通道并行输入">多模态多通道并行输入</a></li>
        <li><a href="#总结">总结</a></li>
        <li><a href="#作业">作业</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p>你好，我是 Rocky。</p>
<p>今天我们来聊聊多模态交互。到底什么是多模态交互呢？人和一个智能系统交互的时候，存在双方相互理解的过程，也就是双方都通过各种通道去表达，然后也都通过各种通道去分析对方的意图。<strong>多模态是站在智能系统一方来表达，它更多强调的是智能系统通过多个通道去捕获人和环境的信息，或者通过多个通道去呈现信息。</strong></p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%9F%BA%E4%BA%8E%E4%BA%BA%E5%9B%A0%E7%9A%84%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C%E8%AE%BE%E8%AE%A1%E8%AF%BE/a429bd2120ed848faa0c44b1e4a5bcff.png" alt=""></p>
<p>从多个通道呈现信息并非是个新鲜概念，比如我们的电影就是同时有画面和声音的多通道呈现的。我在前面的第 9 课讲到视听触的协同性时也提到，手机反馈要让用户通过视觉、听觉和触觉三个反馈通道去感知，这些也是多通道呈现信息的交互。而我们今天这节课要聊的重点是多模态交互的另一个方面：<strong>智能系统如何从更多个通道获取用户的意图</strong>。</p>
<h2 id="多模态交互通道">多模态交互通道</h2>
<p>我们先来简单看看智能系统的交互通道。智能系统应该从哪些通道来去理解人呢？我在第 3 课讲重新认识感觉时提到，要从人的角度来看我们有哪些感知觉在感知世界。相应地，我们理解智能系统也可以站在人的角度。</p>
<p>我们来做一下类比，列如计算机视觉就是通过摄像头模拟人的视觉，从而帮助智能系统来感知世界。但这不是简单的模拟，机器视觉有其独特的、并且还在不断高速演进中的技术优势。比如在分辨率、景深、可见光和非可见光光谱范围、多自由度视觉捕获能力等方面，以及 AI 和大数据加持的图像识别、海量摄像头数据分析及挖掘能力上，都会让计算机视觉表现出惊人的力量和生命力。</p>
<p>我下面列出了几种类似的感知觉技术优势和演进方向的表格，你可以对照着感受一下。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%9F%BA%E4%BA%8E%E4%BA%BA%E5%9B%A0%E7%9A%84%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C%E8%AE%BE%E8%AE%A1%E8%AF%BE/a874725c899753a29f2176961a605ddd.png" alt=""></p>
<p>因为智能系统的演进速度非常快，技术的能力边界也在不断提升，很多已经远远超过人类。所以对于 UX 设计师而言，深入理解技术的当下能力和演进路径，对如何去设计一个多模态创新的交互界面会变得至关重要。我下面用一些案例来带着你来理解一下多模态交互创新设计。</p>
<h2 id="非接触式交互">非接触式交互</h2>
<p>首先是非接触式交互。非接触式交互（Touchless UI，简称 TUI）顾名思义就是指人不需要去触碰键盘、鼠标或者屏幕，直接通过身体运动、隔空手势、眼动凝视等手段完成与设备的交互。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%9F%BA%E4%BA%8E%E4%BA%BA%E5%9B%A0%E7%9A%84%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C%E8%AE%BE%E8%AE%A1%E8%AF%BE/609dd8074f52dc84b97d37c73b0c0193.png" alt=""></p>
<p>非接触式交互的技术有很多种，有的需要普通的可见光摄像头，有的是需要感知深度的摄像头，有的是 3D 立体摄像头，有的是利用了毫米波雷达技术的摄像头，还有些是穿戴在人身上的设备，比如手套、手表、护腕等等。技术肯定是不断在演进的，但作用都是殊途同归。</p>
<p>和触屏交互不同，非接触式交互最近几年才在不同场合被热点关注（包括手机、VR/AR、智能驾驶等）。对任何交互而言，它的适用场景都非常重要。作为一个以大触摸屏为主的智能设备，手机是很难被人想到要进行非接触式交互的。</p>
<p>在当初设计华为的隔空手势时，我和同事们也曾激烈讨论过这一点。当时我们想到的一些非接触式交互的典型场景，就只有吃小龙虾、厨房做菜、油画创作等手脏需要操作手机的情况。</p>
<p>**不过在 2020 年新冠病毒疫情爆发后，非接触式交互已经变成一个非常重要的交互手段。**特别在公共场合，人们非常担心触摸公共屏幕，而喜欢通过扫码在本机上处理，但这也是一种治标不治本的方式。相信未来，真正的 TUI 会越来越变成一个被广泛运用的、重要的交互手段。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%9F%BA%E4%BA%8E%E4%BA%BA%E5%9B%A0%E7%9A%84%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C%E8%AE%BE%E8%AE%A1%E8%AF%BE/0c389986dc9f39697ba44ee7d7b0bd0b.png" alt=""></p>
<h3 id="避免手势导致劳累">避免手势导致劳累</h3>
<p>你在很多科幻电影比如《少数派报告》和《钢铁侠》中，都会看到隔空手势的设计。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%9F%BA%E4%BA%8E%E4%BA%BA%E5%9B%A0%E7%9A%84%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C%E8%AE%BE%E8%AE%A1%E8%AF%BE/21017b1af80c9ab1478a66210b2d1385.png" alt=""></p>
<p>隔空手势在非接触式交互里是一种很炫酷的设计。但其实隔空手势有一点被经常忽视掉，就是大猩猩臂效应（大猩猩臂是指人们根本无法在手臂往外展开的姿势下长时间操作，这是灵长类动物身体结构的限制）。</p>
<p>如果你的隔空手势需要长时间抬起整个手臂进行，那这就是一种不符合人体工学、效率低并且极容易疲惫的交互手势，并不是一种理想的设计。<strong>理想的手势设计应该如操作鼠标一般，不需要大幅度的动作就可以轻松完成任务。</strong></p>
<p>比如下图左半部分的动作整个手臂放在椅子扶手上，手的动作稳定，而且运动过程放松，不至于劳累。但是右半部分就不同了，相信人操作一会儿胳膊就会酸了。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%9F%BA%E4%BA%8E%E4%BA%BA%E5%9B%A0%E7%9A%84%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C%E8%AE%BE%E8%AE%A1%E8%AF%BE/b10ccb7567f7004eb1f9143088234c39.png" alt=""></p>
<p>你在设计手势的时候，手势的疲劳检测必须是一个重要的决定性指标，除非我们要做的设计是一个需要刻意耗费体力的竞技类游戏。</p>
<h3 id="手势识别设计受技术的约束">手势识别设计受技术的约束</h3>
<p>再酷炫的设计，没有一个稳定先进的技术支撑也是不行的。如果我们手势的识别准确率过低，隔空手势功能也就会沦为用户一时冲动的尝鲜，无法成为有生命力、能够持续使用的特性。</p>
<p>手势识别也分为静态手势识别和动态手势识别，前者技术需求较为简单，但是要求手势交互要有停顿，不能够流畅、自然地识别，后者对摄像头和智能系统图像实时动态捕获精度和分析准确度的要求更高，需要能够识别运动的方向、速度以及加速度。</p>
<p>第一种<strong>静态手势识别</strong>，就是通过 2D 外观建模，对摄像头捕获的图像进行手势分析，不需要深度信息。这种技术对应的手势设计，要特别注意减少遮挡的发生，或者至少要考虑遮挡情况，在设计上考虑如何在发生遮挡时也能保证正常识别。</p>
<p>比如某些手势中手的侧边对着摄像头，摄像头可能就无法分辨是哪个手指的变化。这种普通摄像头的手势识别，一般多用于某些典型的标准静态动作、静态表情的识别，不要尝试去做变化很多、带着加速度的手势和表情，比如下图的几种手势就是合适的。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%9F%BA%E4%BA%8E%E4%BA%BA%E5%9B%A0%E7%9A%84%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C%E8%AE%BE%E8%AE%A1%E8%AF%BE/0d08fcfedc2a9a13dc08c800df81a419.png" alt=""></p>
<p>第二种<strong>动态手势识别</strong>，是最高效简单的做法。它其实就是利用骨骼绑定和分析技术，把人体的身体根据骨关节进行简单的分段并做好对应的分段建模，进而检测、识别这些关节的变化来推导出来人具体的手势和体态变化。这种技术下的手势设计，必须要有清晰的骨骼关节变化。比如下面谷歌的 MediaPipe 的手势识别就是在一只手中定义了 21 个 3D 的关键关节点，从而提供了精准、高保真的手部动作跟踪。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%9F%BA%E4%BA%8E%E4%BA%BA%E5%9B%A0%E7%9A%84%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C%E8%AE%BE%E8%AE%A1%E8%AF%BE/f517e8d25e42e43922207e5f7b6abe8a.png" alt=""></p>
<p>第三种是<strong>肌电（EMG）信号来实现手势识别</strong>。这个需要一些非侵入式的穿戴设备来配合（比如手臂或腿部上戴上带着传感器的绑带或者特殊的手套），这些设备再通过蓝牙方式连接智能系统。肌电信号有其特殊的优点，它不依赖于摄像头，有着最大的移动自由度，也不必在乎遮挡问题。对于设计师而言，手势发挥的空间更大，不过缺点就是需要穿戴在用户身上。</p>
<p>Myo 臂环就是一款典型的穿戴在手臂上的手势识别设备。这种设备有一种特别的好处就是可以帮助失去前臂和手的残障人士。因为人即便失去了前臂，人脑还是会尝试控制胳膊的肌肉。把这种设备穿戴在胳膊上，即使是残障人士也是可以控制假肢实现张开手、握手、握拳、拾起物体、旋转手臂等复杂动作。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%9F%BA%E4%BA%8E%E4%BA%BA%E5%9B%A0%E7%9A%84%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C%E8%AE%BE%E8%AE%A1%E8%AF%BE/35a32876f6b5d784dc817fdfe25c0b07.png" alt=""></p>
<p>最后一种我认为是精度最高的一种方式，就是<strong>毫米波雷达技术</strong>。这种技术完美规避大猩猩手臂的那种大幅度动作交互设计，能够对人体和手势的轻微的动作作出精准的跟踪，已经不仅仅是骨骼绑定的那种粗狂的颗粒度了。设计师可以进行更加细腻的手势设计，比如下图，我们通过搓捏拇指和食指的指腹就能够灵活调整手表的时间了。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%9F%BA%E4%BA%8E%E4%BA%BA%E5%9B%A0%E7%9A%84%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C%E8%AE%BE%E8%AE%A1%E8%AF%BE/e6e32a5661f82ff33d5a12349db17345.png" alt=""></p>
<p>不过从目前这个技术量产化的情况来看，它的商用效果和技术潜能之间还是有一定的距离的。对于设计师而言，最大的挑战在于以下三点：</p>
<ol>
<li>用户无法精准地判断到底距离屏幕多远做手势是合理的；</li>
<li>什么时间可以启动手势操作；</li>
<li>不同人对同一动作的理解有偏差，如何去兼容这些理解差异性。</li>
</ol>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%9F%BA%E4%BA%8E%E4%BA%BA%E5%9B%A0%E7%9A%84%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C%E8%AE%BE%E8%AE%A1%E8%AF%BE/d306aafe1d4bee5996a79386e4d3e185.png" alt=""></p>
<p>针对以上问题，在设计上要注意到以下三点：</p>
<ol>
<li>要帮助用户找到合适距离并给予清晰的动态反馈指引，而且这种指引不能被误触发。</li>
<li>在明显判定用户准备做隔空手势时，需要在屏幕上有明显的启动状态提醒，比如一个手的形状，或者其他和业务相关的特殊界面。</li>
<li>要能在感知用户动作不合适时候，在后台做最大的算法兼容，而不是去纠正用户的手势。手势操作失败，还被指出错误，用户会产生很强的挫败感，也就无法建立其使用业务的粘性。</li>
</ol>
<p>从刚刚说的这些点你会发现，<strong>不同的技术对手势交互设计也有完全不同的要求</strong>，这也叫量体裁衣的设计。</p>
<h2 id="多模态多通道并行输入">多模态多通道并行输入</h2>
<p>如果我们要在一个严格的意义上来说，非接触式交互其实也只是单类型的模态交互，并不是真正的多模态交互。</p>
<p><strong>多通道获取信息不仅仅是指切换不同的通道来获取数据，也指同时从多个通道获取信息，并且能够针对上述信息进行进一步的整合，更加精准地获取用户的意图。</strong></p>
<p>比如智能系统通过计算机视觉技术感知到了用户正在盯着屏幕上的某个 App，然后嘴巴说出“打开”就直接打开了这个应用。这个过程其实就是两个通道同时在配合理解用户的意图的过程，这就是多模态交互。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%9F%BA%E4%BA%8E%E4%BA%BA%E5%9B%A0%E7%9A%84%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C%E8%AE%BE%E8%AE%A1%E8%AF%BE/bbfddde4d34af16073f8fbefbffed9a4.png" alt=""></p>
<p>再举一个例子，假设一个电视节目增加字幕，仅仅是把通过录音设备采集到的语音数据通过语音识别算法转换为文字，那这只是一种单模态的场景。</p>
<p>但是如果也同时通过图像中的唇语读取技术来修正语音中的干扰及背景噪音，即视频通道的输入加上语音通道的输入进行双重的信息采集，两个一起做校对性的文字转换，这就是多模态交互了。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%9F%BA%E4%BA%8E%E4%BA%BA%E5%9B%A0%E7%9A%84%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C%E8%AE%BE%E8%AE%A1%E8%AF%BE/1c50c459c537540da63a10bc73ed8405.png" alt=""></p>
<p>因为汽车拥有更多的传感器，所以在驾驶场景中多模态交互的运用更为常见。如果驾驶者通过说“请把温度调高 5 度”的语音交互指令来触发车内空调的温度调节功能，那这只是单模态的交互。</p>
<p>但如果汽车通过用户穿戴设备检测用户的体温，同时也通过摄像头来观察用户是否打寒颤、通过麦克风检测用户是否打喷嚏，这样将多个通道的数据进行整合分析，判断要赶紧调整温度，直到判断用户体感舒适为止，那这就是多模态交互。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%9F%BA%E4%BA%8E%E4%BA%BA%E5%9B%A0%E7%9A%84%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C%E8%AE%BE%E8%AE%A1%E8%AF%BE/5caebbd3d3504da705798797a77c8a90.png" alt=""></p>
<p>监测用户是否疲劳驾驶也是一样的。我们通过同时检测用户的生物电信号（比如呼吸和心跳速度）、驾驶行为（比如是否出现驾驶者驾驶状况突变、违规异常，或者突然紧握方向盘等情况），还有用户面部状况的疲劳特征（比如是否闭眼、是否眼神游离、是否眼睑下垂、是否伴随不停瞌睡或者打哈欠的情况），综合判断疲劳状况后再给予针对性的交互输出和应对，这就是多模态交互。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%9F%BA%E4%BA%8E%E4%BA%BA%E5%9B%A0%E7%9A%84%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C%E8%AE%BE%E8%AE%A1%E8%AF%BE/0556f691602eb5d46ed1fde495e2f758.png" alt=""></p>
<p>在开始设计多模态交互之前，我们要先对这个场景设计有充分的认知和理解，然后再考虑可能调动的通道数量和技术准备度等情况，综合判断以提供不一样的创新体验。</p>
<p>你可能会问，多模态交互是不是可替代触屏交互的新可能？这其实是对多模态交互最大的误解。触屏交互本身就是模态之一，而我们的**多模态交互更像是触屏交互的升级版。我们会从更多维度来深度理解用户意图和对环境的认知，更加精准地为用户提供服务，从这个维度上来看，多模态必然是单模态的下一跳和未来。**只是这种未来的交互体验的具体形态未必唯一，也许会出现更加丰富多彩的可能。</p>
<h2 id="总结">总结</h2>
<p>好了，讲到这里，今天的内容也就基本结束了。最后我来给你总结一下今天讲的要点。</p>
<p>今天我们重点谈了多模态交互。多模态交互包括多通道信息呈现和多通道信息收集，我们这一课主要聚焦的是后者。我们的智能系统有着丰富的传感器，是可以从非常多的维度来获取信息的。</p>
<p>从场景上来看，疫情导致我们对非接触交互的需求量激增，非接触交互会成为未来重点的交互手段。非接触式交互可以通过包括隔空手势、眼动凝视跟踪、身体姿态等手段来实现。关于非接触式交互你可以注意以下几个点。</p>
<ol>
<li>从体验上来说，不要设计幅度过大的隔空手势动作，因为会产生大猩猩手臂效应，引起疲劳。</li>
<li>对于 2D 的外观建模技术，需要重点考虑遮挡发生，以及采用简单的不容易误判别的静态手势进行交互。</li>
<li>骨骼绑定是非常好的一种手势识别技术，需要设计师去根据关键关节点去设计合适的动作。</li>
<li>肌电信号穿戴设备更适合实现无障碍的设计，但存在需要用户穿戴的缺点。</li>
<li>毫米波雷达是比较理想的小幅度交互的精准交互技术，但它的商用效果和技术潜能之间还是有一定距离。</li>
</ol>
<p>从严格意义上来讲，非接触式交互并不能称为多模态交互。你要注意的是多模态交互要具备同时从多个通道获取信息的能力。例如在驾驶场景中由于传感器更为丰富，多模态能够得到更多地运用。多模态交互方式是触屏交互的升维交互，必然会成为其未来的发展方向，但是具体表现形态可能会非常丰富多彩。</p>
<h2 id="作业">作业</h2>
<p>最后我给你留了一个小作业，结合今天我讲的内容，试着思考一个你最喜欢的应用，它的哪些交互特性适合通过隔空手势的方式改造？如果适合的话，什么样的手势设计会让它更加自然流畅呢？</p>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content"></span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
        10100-01-10
        
    </span>
  </p>
  
  
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/%E5%9F%BA%E4%BA%8E%E4%BA%BA%E5%9B%A0%E7%9A%84%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C%E8%AE%BE%E8%AE%A1%E8%AF%BE/">基于人因的用户体验设计课</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/android%E5%BC%80%E5%8F%91%E9%AB%98%E6%89%8B%E8%AF%BE/27-android%E5%BC%80%E5%8F%91%E9%AB%98%E6%89%8B%E8%AF%BE/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">27-Android开发高手课</span>
            <span class="prev-text nav-mobile">上一篇</span>
          </a>
        <a class="next" href="/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/kubernetes%E5%8E%9F%E7%90%86%E5%89%96%E6%9E%90%E4%B8%8E%E5%AE%9E%E6%88%98%E5%BA%94%E7%94%A8/27k8crd%E5%A6%82%E4%BD%95%E6%A0%B9%E6%8D%AE%E9%9C%80%E6%B1%82%E8%87%AA%E5%AE%9A%E4%B9%89%E4%BD%A0%E7%9A%84api/">
            <span class="next-text nav-default">27K8CRD：如何根据需求自定义你的API？</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
  
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2024 - 
    2025<span class="heart"><i class="iconfont icon-heart"></i></span><span></span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  <script src="/lib/highlight/highlight.pack.js?v=20171001"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js"></script>


      <script async src="https://www.googletagmanager.com/gtag/js?id=G-FVZ07KBD4X"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-FVZ07KBD4X');
        }
      </script>






</body>
</html>
