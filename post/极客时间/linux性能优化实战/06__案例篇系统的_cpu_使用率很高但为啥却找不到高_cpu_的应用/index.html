<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>06__案例篇：系统的_CPU_使用率很高，但为啥却找不到高_CPU_的应用？ - Docs</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="" /><meta name="description" content="你好，我是倪朋飞。
上一节我讲了 CPU 使用率是什么，并通过一个案例教你使用 top、vmstat、pidstat 等工具，排查高 CPU 使用率的进程，然后再使用 perf top 工具，定位应用内部函数的问题。不过就有人留言了，说似乎感觉高 CPU 使用率的问题，还是挺容易排查的。
" /><meta name="keywords" content="技术文档, docs, 极客时间" />






<meta name="generator" content="Hugo 0.140.2 with theme even" />


<link rel="canonical" href="https://politcloud.org/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/linux%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%AE%9E%E6%88%98/06__%E6%A1%88%E4%BE%8B%E7%AF%87%E7%B3%BB%E7%BB%9F%E7%9A%84_cpu_%E4%BD%BF%E7%94%A8%E7%8E%87%E5%BE%88%E9%AB%98%E4%BD%86%E4%B8%BA%E5%95%A5%E5%8D%B4%E6%89%BE%E4%B8%8D%E5%88%B0%E9%AB%98_cpu_%E7%9A%84%E5%BA%94%E7%94%A8/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:url" content="https://politcloud.org/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/linux%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%AE%9E%E6%88%98/06__%E6%A1%88%E4%BE%8B%E7%AF%87%E7%B3%BB%E7%BB%9F%E7%9A%84_cpu_%E4%BD%BF%E7%94%A8%E7%8E%87%E5%BE%88%E9%AB%98%E4%BD%86%E4%B8%BA%E5%95%A5%E5%8D%B4%E6%89%BE%E4%B8%8D%E5%88%B0%E9%AB%98_cpu_%E7%9A%84%E5%BA%94%E7%94%A8/">
  <meta property="og:site_name" content="Docs">
  <meta property="og:title" content="06__案例篇：系统的_CPU_使用率很高，但为啥却找不到高_CPU_的应用？">
  <meta property="og:description" content="你好，我是倪朋飞。
上一节我讲了 CPU 使用率是什么，并通过一个案例教你使用 top、vmstat、pidstat 等工具，排查高 CPU 使用率的进程，然后再使用 perf top 工具，定位应用内部函数的问题。不过就有人留言了，说似乎感觉高 CPU 使用率的问题，还是挺容易排查的。">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2024-01-10T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-01-10T00:00:00+00:00">
    <meta property="article:tag" content="Linux性能优化实战">

  <meta itemprop="name" content="06__案例篇：系统的_CPU_使用率很高，但为啥却找不到高_CPU_的应用？">
  <meta itemprop="description" content="你好，我是倪朋飞。
上一节我讲了 CPU 使用率是什么，并通过一个案例教你使用 top、vmstat、pidstat 等工具，排查高 CPU 使用率的进程，然后再使用 perf top 工具，定位应用内部函数的问题。不过就有人留言了，说似乎感觉高 CPU 使用率的问题，还是挺容易排查的。">
  <meta itemprop="datePublished" content="2024-01-10T00:00:00+00:00">
  <meta itemprop="dateModified" content="2024-01-10T00:00:00+00:00">
  <meta itemprop="wordCount" content="5854">
  <meta itemprop="keywords" content="Linux性能优化实战">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="06__案例篇：系统的_CPU_使用率很高，但为啥却找不到高_CPU_的应用？">
  <meta name="twitter:description" content="你好，我是倪朋飞。
上一节我讲了 CPU 使用率是什么，并通过一个案例教你使用 top、vmstat、pidstat 等工具，排查高 CPU 使用率的进程，然后再使用 perf top 工具，定位应用内部函数的问题。不过就有人留言了，说似乎感觉高 CPU 使用率的问题，还是挺容易排查的。">

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Docs</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Docs</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">06__案例篇：系统的_CPU_使用率很高，但为啥却找不到高_CPU_的应用？</h1>

      <div class="post-meta">
        <span class="post-time"> 10100-01-10 </span>
        <div class="post-category">
            <a href="/categories/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/"> 极客时间 </a>
            </div>
          <span class="more-meta"> 约 5854 字 </span>
          <span class="more-meta"> 预计阅读 12 分钟 </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#案例分析">案例分析</a>
          <ul>
            <li><a href="#你的准备">你的准备</a></li>
            <li><a href="#操作和分析">操作和分析</a></li>
          </ul>
        </li>
        <li><a href="#execsnoop">execsnoop</a></li>
        <li><a href="#小结">小结</a></li>
        <li><a href="#思考">思考</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p>你好，我是倪朋飞。</p>
<p>上一节我讲了 CPU 使用率是什么，并通过一个案例教你使用 top、vmstat、pidstat 等工具，排查高 CPU 使用率的进程，然后再使用 perf top 工具，定位应用内部函数的问题。不过就有人留言了，说似乎感觉高 CPU 使用率的问题，还是挺容易排查的。</p>
<p>那是不是所有 CPU 使用率高的问题，都可以这么分析呢？我想，你的答案应该是否定的。</p>
<p>回顾前面的内容，我们知道，系统的 CPU 使用率，不仅包括进程用户态和内核态的运行，还包括中断处理、等待 I/O 以及内核线程等。所以，<strong>当你发现系统的 CPU 使用率很高的时候，不一定能找到相对应的高 CPU 使用率的进程</strong>。</p>
<p>今天，我就用一个 Nginx + PHP 的 Web 服务的案例，带你来分析这种情况。</p>
<h2 id="案例分析">案例分析</h2>
<h3 id="你的准备">你的准备</h3>
<p>今天依旧探究系统 CPU 使用率高的情况，所以这次实验的准备工作，与上节课的准备工作基本相同，差别在于案例所用的 Docker 镜像不同。</p>
<p>本次案例还是基于 Ubuntu 18.04，同样适用于其他的 Linux 系统。我使用的案例环境如下所示：</p>
<ul>
<li>机器配置：2 CPU，8GB 内存</li>
<li>预先安装 docker、sysstat、perf、ab 等工具，如 apt install <a href="./docker.io.md">docker.io</a> sysstat linux-tools-common apache2-utils</li>
</ul>
<p>前面我们讲到过，ab（apache bench）是一个常用的 HTTP 服务性能测试工具，这里同样用来模拟 Nginx 的客户端。由于 Nginx 和 PHP 的配置比较麻烦，我把它们打包成了两个 <a href="./nginx-short-process.md">Docker 镜像</a>，这样只需要运行两个容器，就可以得到模拟环境。</p>
<p>注意，这个案例要用到两台虚拟机，如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/Linux%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%AE%9E%E6%88%98/ea6315bcd1467b86b5d5e9016c3368a5.png" alt=""></p>
<p>你可以看到，其中一台用作 Web 服务器，来模拟性能问题；另一台用作 Web 服务器的客户端，来给 Web 服务增加压力请求。使用两台虚拟机是为了相互隔离，避免“交叉感染”。</p>
<p>接下来，我们打开两个终端，分别 SSH 登录到两台机器上，并安装上述工具。</p>
<p>同样注意，下面所有命令都默认以 root 用户运行，如果你是用普通用户身份登陆系统，请运行 sudo su root 命令切换到 root 用户。</p>
<p>走到这一步，准备工作就完成了。接下来，我们正式进入操作环节。</p>
<blockquote>
<p>温馨提示：案例中 PHP 应用的核心逻辑比较简单，你可能一眼就能看出问题，但实际生产环境中的源码就复杂多了。所以，我依旧建议，<strong>操作之前别看源码</strong>，避免先入为主，而要把它当成一个黑盒来分析。这样，你可以更好把握，怎么从系统的资源使用问题出发，分析出瓶颈所在的应用，以及瓶颈在应用中大概的位置。</p>
</blockquote>
<h3 id="操作和分析">操作和分析</h3>
<p>首先，我们在第一个终端，执行下面的命令运行 Nginx 和 PHP 应用：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ docker run --name nginx -p 10000:80 -itd feisky/nginx:sp
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ docker run --name phpfpm -itd --network container:nginx feisky/php-fpm:sp
</span></span></code></pre></td></tr></table>
</div>
</div><p>然后，在第二个终端，使用 curl 访问 http://[VM1 的 IP]:10000，确认 Nginx 已正常启动。你应该可以看到 It works! 的响应。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"># 192.168.0.10 是第一台虚拟机的 IP 地址
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ curl http://192.168.0.10:10000/
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">It works!
</span></span></code></pre></td></tr></table>
</div>
</div><p>接着，我们来测试一下这个 Nginx 服务的性能。在第二个终端运行下面的 ab 命令。要注意，与上次操作不同的是，这次我们需要并发 100 个请求测试 Nginx 性能，总共测试 1000 个请求。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"># 并发 100 个请求测试 Nginx 性能，总共测试 1000 个请求
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ ab -c 100 -n 1000 http://192.168.0.10:10000/
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">This is ApacheBench, Version 2.3 &lt;$Revision: 1706008 $&gt;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Copyright 1996 Adam Twiss, Zeus Technology Ltd, 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Requests per second:    87.86 [#/sec] (mean)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Time per request:       1138.229 [ms] (mean)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">...
</span></span></code></pre></td></tr></table>
</div>
</div><p>从 ab 的输出结果我们可以看到，Nginx 能承受的每秒平均请求数，只有 87 多一点，是不是感觉它的性能有点差呀。那么，到底是哪里出了问题呢？我们再用 top 和 pidstat 来观察一下。</p>
<p>这次，我们在第二个终端，将测试的并发请求数改成 5，同时把请求时长设置为 10 分钟（-t 600）。这样，当你在第一个终端使用性能分析工具时，Nginx 的压力还是继续的。</p>
<p>继续在第二个终端运行 ab 命令：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ ab -c 5 -t 600 http://192.168.0.10:10000/
</span></span></code></pre></td></tr></table>
</div>
</div><p>然后，我们在第一个终端运行 top 命令，观察系统的 CPU 使用情况：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ top
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">%Cpu(s): 80.8 us, 15.1 sy,  0.0 ni,  2.8 id,  0.0 wa,  0.0 hi,  1.3 si,  0.0 st
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> 6882 root      20   0    8456   5052   3884 S   2.7  0.1   0:04.78 docker-containe
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> 6947 systemd+  20   0   33104   3716   2340 S   2.7  0.0   0:04.92 nginx
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> 7494 daemon    20   0  336696  15012   7332 S   2.0  0.2   0:03.55 php-fpm
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> 7495 daemon    20   0  336696  15160   7480 S   2.0  0.2   0:03.55 php-fpm
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">10547 daemon    20   0  336696  16200   8520 S   2.0  0.2   0:03.13 php-fpm
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">10155 daemon    20   0  336696  16200   8520 S   1.7  0.2   0:03.12 php-fpm
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">10552 daemon    20   0  336696  16200   8520 S   1.7  0.2   0:03.12 php-fpm
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">15006 root      20   0 1168608  66264  37536 S   1.0  0.8   9:39.51 dockerd
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> 4323 root      20   0       0      0      0 I   0.3  0.0   0:00.87 kworker/u4:1
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">...
</span></span></code></pre></td></tr></table>
</div>
</div><p>观察 top 输出的进程列表可以发现，CPU 使用率最高的进程也只不过才 2.7%，看起来并不高。</p>
<p>然而，再看系统 CPU 使用率（ %Cpu）这一行，你会发现，系统的整体 CPU 使用率是比较高的：用户 CPU 使用率（us）已经到了 80%，系统 CPU 为 15.1%，而空闲 CPU（id）则只有 2.8%。</p>
<p>为什么用户 CPU 使用率这么高呢？我们再重新分析一下进程列表，看看有没有可疑进程：</p>
<ul>
<li>docker-containerd 进程是用来运行容器的，2.7% 的 CPU 使用率看起来正常；</li>
<li>Nginx 和 php-fpm 是运行 Web 服务的，它们会占用一些 CPU 也不意外，并且 2% 的 CPU 使用率也不算高；</li>
<li>再往下看，后面的进程呢，只有 0.3% 的 CPU 使用率，看起来不太像会导致用户 CPU 使用率达到 80%。</li>
</ul>
<p>那就奇怪了，明明用户 CPU 使用率都 80% 了，可我们挨个分析了一遍进程列表，还是找不到高 CPU 使用率的进程。看来 top 是不管用了，那还有其他工具可以查看进程 CPU 使用情况吗？不知道你记不记得我们的老朋友 pidstat，它可以用来分析进程的 CPU 使用情况。</p>
<p>接下来，我们还是在第一个终端，运行 pidstat 命令：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"># 间隔 1 秒输出一组数据（按 Ctrl+C 结束）
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ pidstat 1
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">04:36:24      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">04:36:25        0      6882    1.00    3.00    0.00    0.00    4.00     0  docker-containe
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">04:36:25      101      6947    1.00    2.00    0.00    1.00    3.00     1  nginx
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">04:36:25        1     14834    1.00    1.00    0.00    1.00    2.00     0  php-fpm
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">04:36:25        1     14835    1.00    1.00    0.00    1.00    2.00     0  php-fpm
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">04:36:25        1     14845    0.00    2.00    0.00    2.00    2.00     1  php-fpm
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">04:36:25        1     14855    0.00    1.00    0.00    1.00    1.00     1  php-fpm
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">04:36:25        1     14857    1.00    2.00    0.00    1.00    3.00     0  php-fpm
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">04:36:25        0     15006    0.00    1.00    0.00    0.00    1.00     0  dockerd
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">04:36:25        0     15801    0.00    1.00    0.00    0.00    1.00     1  pidstat
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">04:36:25        1     17084    1.00    0.00    0.00    2.00    1.00     0  stress
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">04:36:25        0     31116    0.00    1.00    0.00    0.00    1.00     0  atopacctd
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">...
</span></span></code></pre></td></tr></table>
</div>
</div><p>观察一会儿，你是不是发现，所有进程的 CPU 使用率也都不高啊，最高的 Docker 和 Nginx 也只有 4% 和 3%，即使所有进程的 CPU 使用率都加起来，也不过是 21%，离 80% 还差得远呢！</p>
<p>最早的时候，我碰到这种问题就完全懵了：明明用户 CPU 使用率已经高达 80%，但我却怎么都找不到是哪个进程的问题。到这里，你也可以想想，你是不是也遇到过这种情况？还能不能再做进一步的分析呢？</p>
<p>后来我发现，会出现这种情况，很可能是因为前面的分析漏了一些关键信息。你可以先暂停一下，自己往上翻，重新操作检查一遍。或者，我们一起返回去分析 top 的输出，看看能不能有新发现。</p>
<p>现在，我们回到第一个终端，重新运行 top 命令，并观察一会儿：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-gdscript3" data-lang="gdscript3"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">$</span> <span class="n">top</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">top</span> <span class="o">-</span> <span class="mi">04</span><span class="p">:</span><span class="mi">58</span><span class="p">:</span><span class="mi">24</span> <span class="n">up</span> <span class="mi">14</span> <span class="n">days</span><span class="p">,</span> <span class="mi">15</span><span class="p">:</span><span class="mi">47</span><span class="p">,</span>  <span class="mi">1</span> <span class="n">user</span><span class="p">,</span>  <span class="nb">load</span> <span class="n">average</span><span class="p">:</span> <span class="mf">3.39</span><span class="p">,</span> <span class="mf">3.82</span><span class="p">,</span> <span class="mf">2.74</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">Tasks</span><span class="p">:</span> <span class="mi">149</span> <span class="n">total</span><span class="p">,</span>   <span class="mi">6</span> <span class="n">running</span><span class="p">,</span>  <span class="mi">93</span> <span class="n">sleeping</span><span class="p">,</span>   <span class="mi">0</span> <span class="n">stopped</span><span class="p">,</span>   <span class="mi">0</span> <span class="n">zombie</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">%</span><span class="n">Cpu</span><span class="p">(</span><span class="n">s</span><span class="p">):</span> <span class="mf">77.7</span> <span class="n">us</span><span class="p">,</span> <span class="mf">19.3</span> <span class="n">sy</span><span class="p">,</span>  <span class="mf">0.0</span> <span class="n">ni</span><span class="p">,</span>  <span class="mf">2.0</span> <span class="n">id</span><span class="p">,</span>  <span class="mf">0.0</span> <span class="n">wa</span><span class="p">,</span>  <span class="mf">0.0</span> <span class="n">hi</span><span class="p">,</span>  <span class="mf">1.0</span> <span class="n">si</span><span class="p">,</span>  <span class="mf">0.0</span> <span class="n">st</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">KiB</span> <span class="n">Mem</span> <span class="p">:</span>  <span class="mi">8169348</span> <span class="n">total</span><span class="p">,</span>  <span class="mi">2543916</span> <span class="n">free</span><span class="p">,</span>   <span class="mi">457976</span> <span class="n">used</span><span class="p">,</span>  <span class="mi">5167456</span> <span class="n">buff</span><span class="o">/</span><span class="n">cache</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">KiB</span> <span class="n">Swap</span><span class="p">:</span>        <span class="mi">0</span> <span class="n">total</span><span class="p">,</span>        <span class="mi">0</span> <span class="n">free</span><span class="p">,</span>        <span class="mi">0</span> <span class="n">used</span><span class="o">.</span>  <span class="mi">7363908</span> <span class="n">avail</span> <span class="n">Mem</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   <span class="n">PID</span> <span class="n">USER</span>      <span class="n">PR</span>  <span class="n">NI</span>    <span class="n">VIRT</span>    <span class="n">RES</span>    <span class="n">SHR</span> <span class="n">S</span>  <span class="o">%</span><span class="n">CPU</span> <span class="o">%</span><span class="n">MEM</span>     <span class="n">TIME</span><span class="o">+</span> <span class="n">COMMAND</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> <span class="mi">6947</span> <span class="n">systemd</span><span class="o">+</span>  <span class="mi">20</span>   <span class="mi">0</span>   <span class="mi">33104</span>   <span class="mi">3764</span>   <span class="mi">2340</span> <span class="n">S</span>   <span class="mf">4.0</span>  <span class="mf">0.0</span>   <span class="mi">0</span><span class="p">:</span><span class="mf">32.69</span> <span class="n">nginx</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> <span class="mi">6882</span> <span class="n">root</span>      <span class="mi">20</span>   <span class="mi">0</span>   <span class="mi">12108</span>   <span class="mi">8360</span>   <span class="mi">3884</span> <span class="n">S</span>   <span class="mf">2.0</span>  <span class="mf">0.1</span>   <span class="mi">0</span><span class="p">:</span><span class="mf">31.40</span> <span class="n">docker</span><span class="o">-</span><span class="n">containe</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="mi">15465</span> <span class="n">daemon</span>    <span class="mi">20</span>   <span class="mi">0</span>  <span class="mi">336696</span>  <span class="mi">15256</span>   <span class="mi">7576</span> <span class="n">S</span>   <span class="mf">2.0</span>  <span class="mf">0.2</span>   <span class="mi">0</span><span class="p">:</span><span class="mf">00.62</span> <span class="n">php</span><span class="o">-</span><span class="n">fpm</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="mi">15466</span> <span class="n">daemon</span>    <span class="mi">20</span>   <span class="mi">0</span>  <span class="mi">336696</span>  <span class="mi">15196</span>   <span class="mi">7516</span> <span class="n">S</span>   <span class="mf">2.0</span>  <span class="mf">0.2</span>   <span class="mi">0</span><span class="p">:</span><span class="mf">00.62</span> <span class="n">php</span><span class="o">-</span><span class="n">fpm</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="mi">15489</span> <span class="n">daemon</span>    <span class="mi">20</span>   <span class="mi">0</span>  <span class="mi">336696</span>  <span class="mi">16200</span>   <span class="mi">8520</span> <span class="n">S</span>   <span class="mf">2.0</span>  <span class="mf">0.2</span>   <span class="mi">0</span><span class="p">:</span><span class="mf">00.62</span> <span class="n">php</span><span class="o">-</span><span class="n">fpm</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> <span class="mi">6948</span> <span class="n">systemd</span><span class="o">+</span>  <span class="mi">20</span>   <span class="mi">0</span>   <span class="mi">33104</span>   <span class="mi">3764</span>   <span class="mi">2340</span> <span class="n">S</span>   <span class="mf">1.0</span>  <span class="mf">0.0</span>   <span class="mi">0</span><span class="p">:</span><span class="mf">00.95</span> <span class="n">nginx</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="mi">15006</span> <span class="n">root</span>      <span class="mi">20</span>   <span class="mi">0</span> <span class="mi">1168608</span>  <span class="mi">65632</span>  <span class="mi">37536</span> <span class="n">S</span>   <span class="mf">1.0</span>  <span class="mf">0.8</span>   <span class="mi">9</span><span class="p">:</span><span class="mf">51.09</span> <span class="n">dockerd</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="mi">15476</span> <span class="n">daemon</span>    <span class="mi">20</span>   <span class="mi">0</span>  <span class="mi">336696</span>  <span class="mi">16200</span>   <span class="mi">8520</span> <span class="n">S</span>   <span class="mf">1.0</span>  <span class="mf">0.2</span>   <span class="mi">0</span><span class="p">:</span><span class="mf">00.61</span> <span class="n">php</span><span class="o">-</span><span class="n">fpm</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="mi">15477</span> <span class="n">daemon</span>    <span class="mi">20</span>   <span class="mi">0</span>  <span class="mi">336696</span>  <span class="mi">16200</span>   <span class="mi">8520</span> <span class="n">S</span>   <span class="mf">1.0</span>  <span class="mf">0.2</span>   <span class="mi">0</span><span class="p">:</span><span class="mf">00.61</span> <span class="n">php</span><span class="o">-</span><span class="n">fpm</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="mi">24340</span> <span class="n">daemon</span>    <span class="mi">20</span>   <span class="mi">0</span>    <span class="mi">8184</span>   <span class="mi">1616</span>    <span class="mi">536</span> <span class="n">R</span>   <span class="mf">1.0</span>  <span class="mf">0.0</span>   <span class="mi">0</span><span class="p">:</span><span class="mf">00.01</span> <span class="n">stress</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="mi">24342</span> <span class="n">daemon</span>    <span class="mi">20</span>   <span class="mi">0</span>    <span class="mi">8196</span>   <span class="mi">1580</span>    <span class="mi">492</span> <span class="n">R</span>   <span class="mf">1.0</span>  <span class="mf">0.0</span>   <span class="mi">0</span><span class="p">:</span><span class="mf">00.01</span> <span class="n">stress</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="mi">24344</span> <span class="n">daemon</span>    <span class="mi">20</span>   <span class="mi">0</span>    <span class="mi">8188</span>   <span class="mi">1056</span>    <span class="mi">492</span> <span class="n">R</span>   <span class="mf">1.0</span>  <span class="mf">0.0</span>   <span class="mi">0</span><span class="p">:</span><span class="mf">00.01</span> <span class="n">stress</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="mi">24347</span> <span class="n">daemon</span>    <span class="mi">20</span>   <span class="mi">0</span>    <span class="mi">8184</span>   <span class="mi">1356</span>    <span class="mi">540</span> <span class="n">R</span>   <span class="mf">1.0</span>  <span class="mf">0.0</span>   <span class="mi">0</span><span class="p">:</span><span class="mf">00.01</span> <span class="n">stress</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">...</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>这次从头开始看 top 的每行输出，咦？Tasks 这一行看起来有点奇怪，就绪队列中居然有 6 个 Running 状态的进程（6 running），是不是有点多呢？</p>
<p>回想一下 ab 测试的参数，并发请求数是 5。再看进程列表里，php-fpm 的数量也是 5，再加上 Nginx，好像同时有 6 个进程也并不奇怪。但真的是这样吗？</p>
<p>再仔细看进程列表，这次主要看 Running（R）状态的进程。你有没有发现，Nginx 和所有的 php-fpm 都处于 Sleep（S）状态，而真正处于 Running（R）状态的，却是几个 stress 进程。这几个 stress 进程就比较奇怪了，需要我们做进一步的分析。</p>
<p>我们还是使用 pidstat 来分析这几个进程，并且使用 -p 选项指定进程的 PID。首先，从上面 top 的结果中，找到这几个进程的 PID。比如，先随便找一个 24344，然后用 pidstat 命令看一下它的 CPU 使用情况：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ pidstat -p 24344
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> 16:14:55      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
</span></span></code></pre></td></tr></table>
</div>
</div><p>奇怪，居然没有任何输出。难道是 pidstat 命令出问题了吗？之前我说过，<strong>在怀疑性能工具出问题前，最好还是先用其他工具交叉确认一下</strong>。那用什么工具呢？ps 应该是最简单易用的。我们在终端里运行下面的命令，看看 24344 进程的状态：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"># 从所有进程中查找 PID 是 24344 的进程
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ ps aux | grep 24344
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">root      9628  0.0  0.0  14856  1096 pts/0    S+   16:15   0:00 grep --color=auto 24344
</span></span></code></pre></td></tr></table>
</div>
</div><p>还是没有输出。现在终于发现问题，原来这个进程已经不存在了，所以 pidstat 就没有任何输出。既然进程都没了，那性能问题应该也跟着没了吧。我们再用 top 命令确认一下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ top
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">%Cpu(s): 80.9 us, 14.9 sy,  0.0 ni,  2.8 id,  0.0 wa,  0.0 hi,  1.3 si,  0.0 st
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> 6882 root      20   0   12108   8360   3884 S   2.7  0.1   0:45.63 docker-containe
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> 6947 systemd+  20   0   33104   3764   2340 R   2.7  0.0   0:47.79 nginx
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> 3865 daemon    20   0  336696  15056   7376 S   2.0  0.2   0:00.15 php-fpm
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  6779 daemon    20   0    8184   1112    556 R   0.3  0.0   0:00.01 stress
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">...
</span></span></code></pre></td></tr></table>
</div>
</div><p>好像又错了。结果还跟原来一样，用户 CPU 使用率还是高达 80.9%，系统 CPU 接近 15%，而空闲 CPU 只有 2.8%，Running 状态的进程有 Nginx、stress 等。</p>
<p>可是，刚刚我们看到 stress 进程不存在了，怎么现在还在运行呢？再细看一下 top 的输出，原来，这次 stress 进程的 PID 跟前面不一样了，原来的 PID 24344 不见了，现在的是 6779。</p>
<p>进程的 PID 在变，这说明什么呢？在我看来，要么是这些进程在不停地重启，要么就是全新的进程，这无非也就两个原因：</p>
<ul>
<li>第一个原因，进程在不停地崩溃重启，比如因为段错误、配置错误等等，这时，进程在退出后可能又被监控系统自动重启了。</li>
<li>第二个原因，这些进程都是短时进程，也就是在其他应用内部通过 exec 调用的外面命令。这些命令一般都只运行很短的时间就会结束，你很难用 top 这种间隔时间比较长的工具发现（上面的案例，我们碰巧发现了）。</li>
</ul>
<p>至于 stress，我们前面提到过，它是一个常用的压力测试工具。它的 PID 在不断变化中，看起来像是被其他进程调用的短时进程。要想继续分析下去，还得找到它们的父进程。</p>
<p>要怎么查找一个进程的父进程呢？没错，用 pstree 就可以用树状形式显示所有进程之间的关系：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ pstree | grep stress
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        |-docker-containe-+-php-fpm-+-php-fpm---sh---stress
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        |         |-3*[php-fpm---sh---stress---stress]
</span></span></code></pre></td></tr></table>
</div>
</div><p>从这里可以看到，stress 是被 php-fpm 调用的子进程，并且进程数量不止一个（这里是 3 个）。找到父进程后，我们能进入 app 的内部分析了。</p>
<p>首先，当然应该去看看它的源码。运行下面的命令，把案例应用的源码拷贝到 app 目录，然后再执行 grep 查找是不是有代码再调用 stress 命令：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"># 拷贝源码到本地
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ docker cp phpfpm:/app .
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> # grep 查找看看是不是有代码在调用 stress 命令
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ grep stress -r app
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">app/index.php:// fake I/O with stress (via write()/unlink()).
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">app/index.php:$result = exec(&#34;/usr/local/bin/stress -t 1 -d 1 2&gt;&amp;1&#34;, $output, $status);
</span></span></code></pre></td></tr></table>
</div>
</div><p>找到了，果然是 app/index.php 文件中直接调用了 stress 命令。</p>
<p>再来看看<a href="./index.php.md"> app/index.php </a>的源代码：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ cat app/index.php
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">&lt;?php
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">// fake I/O with stress (via write()/unlink()).
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$result = exec(&#34;/usr/local/bin/stress -t 1 -d 1 2&gt;&amp;1&#34;, $output, $status);
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">if (isset($_GET[&#34;verbose&#34;]) &amp;&amp; $_GET[&#34;verbose&#34;]==1 &amp;&amp; $status != 0) {
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  echo &#34;Server internal error: &#34;;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  print_r($output);
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">} else {
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  echo &#34;It works!&#34;;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">}
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">?&gt;
</span></span></code></pre></td></tr></table>
</div>
</div><p>可以看到，源码里对每个请求都会调用一个 stress 命令，模拟 I/O 压力。从注释上看，stress 会通过 write() 和 unlink() 对 I/O 进程进行压测，看来，这应该就是系统 CPU 使用率升高的根源了。</p>
<p>不过，stress 模拟的是 I/O 压力，而之前在 top 的输出中看到的，却一直是用户 CPU 和系统 CPU 升高，并没见到 iowait 升高。这又是怎么回事呢？stress 到底是不是 CPU 使用率升高的原因呢？</p>
<p>我们还得继续往下走。从代码中可以看到，给请求加入 verbose=1 参数后，就可以查看 stress 的输出。你先试试看，在第二个终端运行：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ curl http://192.168.0.10:10000?verbose=1
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Server internal error: Array
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">(
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    [0] =&gt; stress: info: [19607] dispatching hogs: 0 cpu, 0 io, 0 vm, 1 hdd
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    [1] =&gt; stress: FAIL: [19608] (563) mkstemp failed: Permission denied
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    [2] =&gt; stress: FAIL: [19607] (394) &lt;-- worker 19608 returned error 1
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    [3] =&gt; stress: WARN: [19607] (396) now reaping child worker processes
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    [4] =&gt; stress: FAIL: [19607] (400) kill error: No such process
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    [5] =&gt; stress: FAIL: [19607] (451) failed run completed in 0s
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">)
</span></span></code></pre></td></tr></table>
</div>
</div><p>看错误消息 mkstemp failed: Permission denied ，以及 failed run completed in 0s。原来 stress 命令并没有成功，它因为权限问题失败退出了。看来，我们发现了一个 PHP 调用外部 stress 命令的 bug：没有权限创建临时文件。</p>
<p>从这里我们可以猜测，正是由于权限错误，大量的 stress 进程在启动时初始化失败，进而导致用户 CPU 使用率的升高。</p>
<p>分析出问题来源，下一步是不是就要开始优化了呢？当然不是！既然只是猜测，那就需要再确认一下，这个猜测到底对不对，是不是真的有大量的 stress 进程。该用什么工具或指标呢？</p>
<p>我们前面已经用了 top、pidstat、pstree 等工具，没有发现大量的 stress 进程。那么，还有什么其他的工具可以用吗？</p>
<p>还记得上一期提到的 perf 吗？它可以用来分析 CPU 性能事件，用在这里就很合适。依旧在第一个终端中运行 perf record -g 命令，并等待一会儿（比如 15 秒）后按 Ctrl+C 退出。然后再运行 perf report 查看报告：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"># 记录性能事件，等待大约 15 秒后按 Ctrl+C 退出
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ perf record -g
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> # 查看报告
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ perf report
</span></span></code></pre></td></tr></table>
</div>
</div><p>这样，你就可以看到下图这个性能报告：</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/Linux%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%AE%9E%E6%88%98/fd0ad6402bf3c0ee454ab2515dd7b7d4.png" alt=""></p>
<p>你看，stress 占了所有 CPU 时钟事件的 77%，而 stress 调用调用栈中比例最高的，是随机数生成函数 random()，看来它的确就是 CPU 使用率升高的元凶了。随后的优化就很简单了，只要修复权限问题，并减少或删除 stress 的调用，就可以减轻系统的 CPU 压力。</p>
<p>当然，实际生产环境中的问题一般都要比这个案例复杂，在你找到触发瓶颈的命令行后，却可能发现，这个外部命令的调用过程是应用核心逻辑的一部分，并不能轻易减少或者删除。</p>
<p>这时，你就得继续排查，为什么被调用的命令，会导致 CPU 使用率升高或 I/O 升高等问题。这些复杂场景的案例，我会在后面的综合实战里详细分析。</p>
<p>最后，在案例结束时，不要忘了清理环境，执行下面的 Docker 命令，停止案例中用到的 Nginx 进程：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ docker rm -f nginx phpfpm
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="execsnoop">execsnoop</h2>
<p>在这个案例中，我们使用了 top、pidstat、pstree 等工具分析了系统 CPU 使用率高的问题，并发现 CPU 升高是短时进程 stress 导致的，但是整个分析过程还是比较复杂的。对于这类问题，有没有更好的方法监控呢？</p>
<p><a href="./execsnoop.md">execsnoop</a> 就是一个专为短时进程设计的工具。它通过 ftrace 实时监控进程的 exec() 行为，并输出短时进程的基本信息，包括进程 PID、父进程 PID、命令行参数以及执行的结果。</p>
<p>比如，用 execsnoop 监控上述案例，就可以直接得到 stress 进程的父进程 PID 以及它的命令行参数，并可以发现大量的 stress 进程在不停启动：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"># 按 Ctrl+C 结束
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ execsnoop
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">PCOMM            PID    PPID   RET ARGS
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">sh               30394  30393    0
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">stress           30396  30394    0 /usr/local/bin/stress -t 1 -d 1
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">sh               30398  30393    0
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">stress           30399  30398    0 /usr/local/bin/stress -t 1 -d 1
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">sh               30402  30400    0
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">stress           30403  30402    0 /usr/local/bin/stress -t 1 -d 1
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">sh               30405  30393    0
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">stress           30407  30405    0 /usr/local/bin/stress -t 1 -d 1
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">...
</span></span></code></pre></td></tr></table>
</div>
</div><p>execsnoop 所用的 ftrace 是一种常用的动态追踪技术，一般用于分析 Linux 内核的运行时行为，后面课程我也会详细介绍并带你使用。</p>
<h2 id="小结">小结</h2>
<p>碰到常规问题无法解释的 CPU 使用率情况时，首先要想到有可能是短时应用导致的问题，比如有可能是下面这两种情况。</p>
<ul>
<li>第一，<strong>应用里直接调用了其他二进制程序，这些程序通常运行时间比较短，通过 top 等工具也不容易发现</strong>。</li>
<li>第二，<strong>应用本身在不停地崩溃重启，而启动过程的资源初始化，很可能会占用相当多的 CPU</strong>。</li>
</ul>
<p>对于这类进程，我们可以用 pstree 或者 execsnoop 找到它们的父进程，再从父进程所在的应用入手，排查问题的根源。</p>
<h2 id="思考">思考</h2>
<p>最后，我想邀请你一起来聊聊，你所碰到的 CPU 性能问题。有没有哪个印象深刻的经历可以跟我分享呢？或者，在今天的案例操作中，你遇到了什么问题，又解决了哪些呢？你可以结合我的讲述，总结自己的思路。</p>
<p>欢迎在留言区和我讨论，也欢迎把这篇文章分享给你的同事、朋友。我们一起在实战中演练，在交流中进步。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/Linux%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%AE%9E%E6%88%98/f3ab291e71ad0a9d7fe2c894ccb9706a.png" alt=""></p>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content"></span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
        10100-01-10
        
    </span>
  </p>
  
  
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/linux%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%AE%9E%E6%88%98/">Linux性能优化实战</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/tob%E5%B8%82%E5%9C%BA%E5%93%81%E7%89%8C%E5%AE%9E%E6%88%98%E8%AF%BE/06__%E6%A1%88%E4%BE%8B%E6%80%8E%E6%A0%B70%E6%88%90%E6%9C%AC%E6%8A%8A%E4%B8%80%E4%B8%AA%E6%96%B0%E4%BA%A7%E5%93%81%E7%9A%84%E5%B8%82%E5%9C%BA%E8%AE%A4%E7%9F%A5%E7%8E%87%E5%81%9A%E5%88%B076/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">06__案例：怎样0成本把一个新产品的市场认知率做到76%？</span>
            <span class="prev-text nav-mobile">上一篇</span>
          </a>
        <a class="next" href="/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90kubernetes/06__%E7%99%BD%E8%AF%9D%E5%AE%B9%E5%99%A8%E5%9F%BA%E7%A1%80%E4%BA%8C%E9%9A%94%E7%A6%BB%E4%B8%8E%E9%99%90%E5%88%B6/">
            <span class="next-text nav-default">06__白话容器基础（二）：隔离与限制</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
  
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2024 - 
    2025<span class="heart"><i class="iconfont icon-heart"></i></span><span></span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  <script src="/lib/highlight/highlight.pack.js?v=20171001"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js"></script>


      <script async src="https://www.googletagmanager.com/gtag/js?id=G-FVZ07KBD4X"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-FVZ07KBD4X');
        }
      </script>






</body>
</html>
