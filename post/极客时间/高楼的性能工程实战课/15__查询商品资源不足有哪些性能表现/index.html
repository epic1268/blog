<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>15__查询商品：资源不足有哪些性能表现？ - Docs</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="" /><meta name="description" content="你好，我是高楼。
这节课，我们来收拾“查询商品”这个接口。虽然这次的现象同样是 TPS 低、响应时间长，但是，这个接口走的路径和之前的不一样，所以在分析过程中会有些新鲜的东西，你将看到在资源真的不足的情况下，我们只有增加相应节点的资源才能提升性能。
" /><meta name="keywords" content="技术文档, docs, 极客时间" />






<meta name="generator" content="Hugo 0.140.2 with theme even" />


<link rel="canonical" href="https://politcloud.org/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E9%AB%98%E6%A5%BC%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E8%AF%BE/15__%E6%9F%A5%E8%AF%A2%E5%95%86%E5%93%81%E8%B5%84%E6%BA%90%E4%B8%8D%E8%B6%B3%E6%9C%89%E5%93%AA%E4%BA%9B%E6%80%A7%E8%83%BD%E8%A1%A8%E7%8E%B0/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:url" content="https://politcloud.org/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E9%AB%98%E6%A5%BC%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E8%AF%BE/15__%E6%9F%A5%E8%AF%A2%E5%95%86%E5%93%81%E8%B5%84%E6%BA%90%E4%B8%8D%E8%B6%B3%E6%9C%89%E5%93%AA%E4%BA%9B%E6%80%A7%E8%83%BD%E8%A1%A8%E7%8E%B0/">
  <meta property="og:site_name" content="Docs">
  <meta property="og:title" content="15__查询商品：资源不足有哪些性能表现？">
  <meta property="og:description" content="你好，我是高楼。
这节课，我们来收拾“查询商品”这个接口。虽然这次的现象同样是 TPS 低、响应时间长，但是，这个接口走的路径和之前的不一样，所以在分析过程中会有些新鲜的东西，你将看到在资源真的不足的情况下，我们只有增加相应节点的资源才能提升性能。">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2024-01-10T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-01-10T00:00:00+00:00">
    <meta property="article:tag" content="高楼的性能工程实战课">

  <meta itemprop="name" content="15__查询商品：资源不足有哪些性能表现？">
  <meta itemprop="description" content="你好，我是高楼。
这节课，我们来收拾“查询商品”这个接口。虽然这次的现象同样是 TPS 低、响应时间长，但是，这个接口走的路径和之前的不一样，所以在分析过程中会有些新鲜的东西，你将看到在资源真的不足的情况下，我们只有增加相应节点的资源才能提升性能。">
  <meta itemprop="datePublished" content="2024-01-10T00:00:00+00:00">
  <meta itemprop="dateModified" content="2024-01-10T00:00:00+00:00">
  <meta itemprop="wordCount" content="6113">
  <meta itemprop="keywords" content="高楼的性能工程实战课">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="15__查询商品：资源不足有哪些性能表现？">
  <meta name="twitter:description" content="你好，我是高楼。
这节课，我们来收拾“查询商品”这个接口。虽然这次的现象同样是 TPS 低、响应时间长，但是，这个接口走的路径和之前的不一样，所以在分析过程中会有些新鲜的东西，你将看到在资源真的不足的情况下，我们只有增加相应节点的资源才能提升性能。">

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Docs</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Docs</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">15__查询商品：资源不足有哪些性能表现？</h1>

      <div class="post-meta">
        <span class="post-time"> 10100-01-10 </span>
        <div class="post-category">
            <a href="/categories/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/"> 极客时间 </a>
            </div>
          <span class="more-meta"> 约 6113 字 </span>
          <span class="more-meta"> 预计阅读 13 分钟 </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#压力场景数据">压力场景数据</a></li>
        <li><a href="#先看架构图">先看架构图</a></li>
        <li><a href="#拆分响应时间">拆分响应时间</a></li>
        <li><a href="#全局监控">全局监控</a></li>
        <li><a href="#定向分析">定向分析</a>
          <ul>
            <li><a href="#es-client-请求不均衡">ES client 请求不均衡</a></li>
            <li><a href="#es-data-cpu-使用率高">ES Data CPU 使用率高</a></li>
          </ul>
        </li>
        <li><a href="#总结">总结</a></li>
        <li><a href="#课后作业">课后作业</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p>你好，我是高楼。</p>
<p>这节课，我们来收拾“查询商品”这个接口。虽然这次的现象同样是 TPS 低、响应时间长，但是，这个接口走的路径和之前的不一样，所以在分析过程中会有些新鲜的东西，你将看到在资源真的不足的情况下，我们只有增加相应节点的资源才能提升性能。</p>
<p>在我的项目中，我一直都在讲，<strong>不要轻易给出资源不足的结论。因为但凡有优化的空间，我们都要尝试做优化，而不是直接告诉客户加资源。而给出“增加资源”这个结论，也必须建立在有足够证据的基础上</strong>。在这节课中，你也将看到这一点。</p>
<p>话不多说，我们直接开始今天的内容。</p>
<h2 id="压力场景数据">压力场景数据</h2>
<p>对于查询商品接口，我们第一次试执行性能场景的结果如下：</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E9%AB%98%E6%A5%BC%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E8%AF%BE/338da95552ad42493b9580d252b62106.png" alt=""></p>
<p>你看，TPS 只有 250 左右，并且响应时间也明显随着压力的增加而增加了，看起来瓶颈已经出现了，对吧？根据哥的逻辑，下一步就是看架构图啦。</p>
<h2 id="先看架构图">先看架构图</h2>
<p>我们用 APM 工具来看看这个接口的架构。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E9%AB%98%E6%A5%BC%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E8%AF%BE/cd5b0f0fb1894c8399d938133590757d.png" alt=""></p>
<p>你看，从压力机到 Gateway 服务、到 Search 服务、再到 ES-Client，这个 APM 工具也只能帮我们到这里了。因为我们用的是 ElasticSearch 7 来做的搜索服务的支撑，而这个 skywalking 工具也没有对应的 Agent，所以后面并没有配置 skywalking。</p>
<p>在这里，我要多啰嗦几句。现在的 APM 工具大多是基于应用层来做的，有些运维 APM 采集的数据会更多一些，但也是响应时间、吞吐量等这样的信息。对于性能分析而言，现在的 APM 工具有减少排查时间的能力，但是在组件级的细化定位上还有待提高。虽然 AI OPS 也被提到了台面，但是也没见过哪个公司上了 AIOPS 产品后，就敢不让人看着。</p>
<p>总之，从细化分析的角度，我们在定位问题的根本原因时，手头有什么工具就可以用什么工具，即使什么工具都没有，撸日志也是照样能做到的，所以我建议你不要迷信工具，要“迷信”思路。</p>
<p>下面我们来拆分下这个接口的响应时间，看看这个案例的问题点在哪里。</p>
<h2 id="拆分响应时间">拆分响应时间</h2>
<p><strong>“在 RESAR 性能分析逻辑中，拆分响应时间只是一个分析的起点</strong>。”这是我一直在强调的一句话。如果性能工程师连这个都不会做，就只能好好学习天天向上了。</p>
<p>根据架构图，我们拆分响应时间如下。</p>
<ol>
<li>Gateway 服务上的响应时间：</li>
</ol>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E9%AB%98%E6%A5%BC%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E8%AF%BE/4f0c4d290d383d9c5e90aa81fc346c8e.png" alt=""></p>
<ol>
<li>Search 服务上的响应时间：</li>
</ol>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E9%AB%98%E6%A5%BC%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E8%AF%BE/e80338b5729d1c606bed5eedef87e90a.png" alt=""></p>
<ol>
<li>ES Client 的响应时间：</li>
</ol>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E9%AB%98%E6%A5%BC%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E8%AF%BE/306915dfba55b3d8801a1c85c753fa08.png" alt=""></p>
<p>一层层看过之后，我们发现查询商品这个接口的响应时间消耗在了 ES client 上面。而在我们这个查询的路径上，在 gateway/search 服务上，我们并没有做什么复杂的动作。</p>
<p>既然知道了响应时间消耗在哪里，下面我们就来定位它，看能不能把 TPS 优化起来。</p>
<h2 id="全局监控">全局监控</h2>
<p>根据高老师的经验，我们还是从全局监控开始，看全局监控可以让我们更加有的放矢。在分析的过程中，经常有人往下走了几步之后，就开始思维混乱、步伐飘逸。因为数据有很多，所以分析时很容易从一个数据走到不重要的分支上去了。而这时候，如果你心里有全局监控数据，思路就会更清晰，不会在无关的分支上消耗时间。</p>
<p>回到我们这个例子中，从下面的 k8s worker（也就是 k8s 中的 node，在我们的环境中我习惯叫成 worker，就是为了体现：在我的地盘，我爱叫啥就叫啥）的数据上来看，似乎没有一个 worker 的资源使用率是特别高的。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E9%AB%98%E6%A5%BC%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E8%AF%BE/555bbd311d17d6629d4b6871356cf92c.png" alt=""></p>
<p>请你注意，在 k8s 中看资源消耗，一定不要只看 worker 这个层面，因为这个层面还不够，一个 worker 上可能会运行多个 pod。从上图来看，由于 worker 层面没有资源消耗，但是时间又明显是消耗在 ES client 上的，所以，接下来我们要看一下每一个 pod 的资源使用情况。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E9%AB%98%E6%A5%BC%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E8%AF%BE/4f551338dd3fcff921e98cee9b3af023.png" alt=""></p>
<p>咦，有红色。你看，有两个与 ES 相关的 POD，它们的 CPU 都飘红了，这下可有得玩了。既然是与 ES 相关的 POD，那我们就把 ES 所有的 POD 排个序看看。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E9%AB%98%E6%A5%BC%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E8%AF%BE/89ab726138587e523ff127031ea7f74c.png" alt=""></p>
<p>从上图的数据来看，有一个 ES Client 消耗了 67% 的 CPU，有两个 ES Data 消耗了 99% 的 CPU，ES 本来就是吃 CPU 的大户，所以我们接下来要着重分析它。</p>
<p>这里我再说明一点，我们从前面的 worker 资源使用率一步一步走到这里，在分析方向上是合情合理的，因为这些都是属于我提到的全局监控的内容。</p>
<h2 id="定向分析">定向分析</h2>
<p>现在我们就来扒一扒 ES，看看它在哪个 worker 节点上。罗列 Pod 信息如下：</p>
<p>[root@k8s-master-1 ~]# kubectl get pods -o wide | grep elasticsearch<br>
elasticsearch-client-0                      1/1     Running   0          6h43m   10.100.230.2     k8s-worker-1   <none>           <none><br>
elasticsearch-client-1                      1/1     Running   0          6h45m   10.100.140.8     k8s-worker-2   <none>           <none><br>
elasticsearch-data-0                        1/1     Running   0          7h8m    10.100.18.197    k8s-worker-5   <none>           <none><br>
elasticsearch-data-1                        1/1     Running   0          7h8m    10.100.5.5       k8s-worker-7   <none>           <none><br>
elasticsearch-data-2                        1/1     Running   0          7h8m    10.100.251.67    k8s-worker-9   <none>           <none><br>
elasticsearch-master-0                      1/1     Running   0          7h8m    10.100.230.0     k8s-worker-1   <none>           <none><br>
elasticsearch-master-1                      1/1     Running   0          7h8m    10.100.227.131   k8s-worker-6   <none>           <none><br>
elasticsearch-master-2                      1/1     Running   0          7h8m    10.100.69.206    k8s-worker-3   <none>           <none><br>
[root@k8s-master-1 ~]#</p>
<p>现在就比较清晰了，可以看到，在整个 namespace 中有两个 ES client，三个 ES data，三个 ES master。</p>
<p>我们来画一个细一点的架构图，以便在脑子里记下这个逻辑：</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E9%AB%98%E6%A5%BC%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E8%AF%BE/329e81dc420eb7e116b30c2641e93afb.png" alt=""></p>
<p>再结合我们在全局分析中看到的资源使用率图，现在判断至少有两个问题：</p>
<ol>
<li>ES client 请求不均衡；</li>
<li>ES data CPU 高。</li>
</ol>
<p>下面我们一个一个来分析。</p>
<h3 id="es-client-请求不均衡">ES client 请求不均衡</h3>
<p>从上面的架构图中可以看到，search 服务连两个 ES client，但是只有一个 ES client 的 CPU 使用率高。所以，我们需要查一下链路，看看 ES 的 service：</p>
<p>[root@k8s-master-1 ~]# kubectl get svc -o wide | grep search<br>
elasticsearch-client            NodePort    10.96.140.52    <none>        9200:30200/TCP,9300:31614/TCP         34d     app=elasticsearch-client,chart=elasticsearch,heritage=Helm,release=elasticsearch-client<br>
elasticsearch-client-headless   ClusterIP   None            <none>        9200/TCP,9300/TCP                     34d     app=elasticsearch-client<br>
elasticsearch-data              ClusterIP   10.96.16.151    <none>        9200/TCP,9300/TCP                     7h41m   app=elasticsearch-data,chart=elasticsearch,heritage=Helm,release=elasticsearch-data<br>
elasticsearch-data-headless     ClusterIP   None            <none>        9200/TCP,9300/TCP                     7h41m   app=elasticsearch-data<br>
elasticsearch-master            ClusterIP   10.96.207.238   <none>        9200/TCP,9300/TCP                     7h41m   app=elasticsearch-master,chart=elasticsearch,heritage=Helm,release=elasticsearch-master<br>
elasticsearch-master-headless   ClusterIP   None            <none>        9200/TCP,9300/TCP                     7h41m   app=elasticsearch-master<br>
svc-mall-search                 ClusterIP   10.96.27.150    <none>        8081/TCP                              44d     app=svc-mall-search<br>
[root@k8s-master-1 ~]#</p>
<p>你看，整个 namespace 中有一个 client service（解析出来的是 VIP，访问此服务时不会绕过 K8s 的转发机制），还有一个 client-headless service（解析出来的是 POD IP，访问这个服务时会绕过 K8s 的转发机制）。</p>
<p>接下来，我们查一下为什么会出现访问不均衡的情况。</p>
<p>通过查看 search 服务的 ES 配置，我们看到如下信息：</p>
<p>elasticsearch:<br>
rest:<br>
uris: elasticsearch-client:9200<br>
username: elastic<br>
password: admin@123</p>
<p>看到我们这里是用的 elasticsearch-client:9200，我们再来看一下 client service 的配置：</p>
<hr>
<p>apiVersion: v1<br>
kind: Service<br>
metadata:<br>
annotations:<br>
meta.helm.sh/release-name: elasticsearch-client<br>
meta.helm.sh/release-namespace: default<br>
creationTimestamp: &lsquo;2020-12-10T17:34:19Z&rsquo;<br>
labels:<br>
app: elasticsearch-client<br>
app.kubernetes.io/managed-by: Helm<br>
chart: elasticsearch<br>
heritage: Helm<br>
release: elasticsearch-client<br>
managedFields:<br>
- apiVersion: v1<br>
fieldsType: FieldsV1<br>
fieldsV1:<br>
&lsquo;f:metadata&rsquo;: {}<br>
&lsquo;f:spec&rsquo;:<br>
&lsquo;f:ports&rsquo;: {}<br>
manager: Go-http-client<br>
operation: Update<br>
time: &lsquo;2020-12-10T17:34:19Z&rsquo;<br>
name: elasticsearch-client<br>
namespace: default<br>
resourceVersion: &lsquo;4803428&rsquo;<br>
selfLink: /api/v1/namespaces/default/services/elasticsearch-client<br>
uid: 457e962e-bee0-49b7-9ec4-ebfbef0fecdd<br>
spec:<br>
clusterIP: 10.96.140.52<br>
externalTrafficPolicy: Cluster<br>
ports:<br>
- name: http<br>
nodePort: 30200<br>
port: 9200<br>
protocol: TCP<br>
targetPort: 9200<br>
- name: transport<br>
nodePort: 31614<br>
port: 9300<br>
protocol: TCP<br>
targetPort: 9300<br>
selector:<br>
app: elasticsearch-client<br>
chart: elasticsearch<br>
heritage: Helm<br>
release: elasticsearch-client<br>
sessionAffinity: None<br>
type: NodePort</p>
<p>从上面的配置来看，sessionAffinity 也配置为 None 了，也就是说这个 service 不以客户端的 IP 来保持 session。因为在这个环境配置中，Type 为 NodePort，而我们在 k8s 中配置的转发规则是 iptables。所以说，service 是依赖 iptables 的规则来做后端转发的。</p>
<p>接下来，我们检查一下 iptables 的转发规则。</p>
<p>我们先来看 iptables 中关于 ES client 的规则：</p>
<p>[root@k8s-master-1 ~]# iptables -S KUBE-SERVICES -t nat|grep elasticsearch-client|grep 9200<br>
-A KUBE-SERVICES ! -s 10.100.0.0/16 -d 10.96.140.52/32 -p tcp -m comment &ndash;comment &ldquo;default/elasticsearch-client:http cluster IP&rdquo; -m tcp &ndash;dport 9200 -j KUBE-MARK-MASQ<br>
-A KUBE-SERVICES -d 10.96.140.52/32 -p tcp -m comment &ndash;comment &ldquo;default/elasticsearch-client:http cluster IP&rdquo; -m tcp &ndash;dport 9200 -j KUBE-SVC-XCX4XZ2WPAE7BUZ4<br>
[root@k8s-master-1 ~]#</p>
<p>可以看到，service 的规则名是 KUBE-SVC-XCX4XZ2WPAE7BUZ4，那我们再去查它对应的 iptables 规则：</p>
<p>[root@k8s-master-1 ~]# iptables -S KUBE-SVC-XCX4XZ2WPAE7BUZ4 -t nat<br>
-N KUBE-SVC-XCX4XZ2WPAE7BUZ4<br>
-A KUBE-SVC-XCX4XZ2WPAE7BUZ4 -m comment &ndash;comment &ldquo;default/elasticsearch-client:http&rdquo; -j KUBE-SEP-LO263M5QW4XA6E3Q<br>
[root@k8s-master-1 ~]#<br>
[root@k8s-master-1 ~]# iptables -S KUBE-SEP-LO263M5QW4XA6E3Q -t nat<br>
-N KUBE-SEP-LO263M5QW4XA6E3Q<br>
-A KUBE-SEP-LO263M5QW4XA6E3Q -s 10.100.227.130/32 -m comment &ndash;comment &ldquo;default/elasticsearch-client:http&rdquo; -j KUBE-MARK-MASQ<br>
-A KUBE-SEP-LO263M5QW4XA6E3Q -p tcp -m comment &ndash;comment &ldquo;default/elasticsearch-client:http&rdquo; -m tcp -j DNAT &ndash;to-destination 10.100.227.130:9200</p>
<p>问题来了，这里好像没有负载均衡的配置（没有 probability 参数），并且根据 iptables 规则也只是转发到了一个 ES client 上。到这里，其实我们也就能理解，为什么在全局监控的时候，我们只看到一个 ES client 有那么高的 CPU 使用率，而另一个 ES client 却一点动静都没有。</p>
<p>但是，这里的 iptables 规则并不是自己来配置的，而是在部署 k8s 的时候自动刷进去的规则。现在只有一条规则了，所以只能转发到一个 POD 上去。</p>
<p>那我们就再刷一遍 ES 的 POD，重装一下 ES 的 POD，看 k8s 自己能不能刷出来负载均衡的 iptables 规则。重来一遍之后，我们再来看 iptables 规则：</p>
<p>[root@k8s-master-1 ~]# iptables -S KUBE-SVC-XCX4XZ2WPAE7BUZ4 -t nat<br>
-N KUBE-SVC-XCX4XZ2WPAE7BUZ4<br>
-A KUBE-SVC-XCX4XZ2WPAE7BUZ4 -m comment &ndash;comment &ldquo;default/elasticsearch-client:http&rdquo; -m statistic &ndash;mode random &ndash;probability 0.50000000000 -j KUBE-SEP-IFM4L7YNSTSJP4YT<br>
-A KUBE-SVC-XCX4XZ2WPAE7BUZ4 -m comment &ndash;comment &ldquo;default/elasticsearch-client:http&rdquo; -j KUBE-SEP-5RAP6F6FATXC4DFL<br>
[root@k8s-master-1 ~]#</p>
<p>哟，现在刷出来两条 iptables 规则了，看来之前在我们不断折腾的部署过程中，ES client 一直是有问题的。</p>
<p>在上面的 iptables 规则里，那两条 iptables 的上一条中有一个关键词“——probability 0.50000000000”。我们知道，iptables 的匹配规则是从上到下的，既然上一条的匹配是随机 0.5，也就是说只有 50% 的请求会走第一条规则，那下一条自然也是随机 0.5 了，因为总共只有两条规则嘛。这样一来就均衡了。</p>
<p>我们再接着做这个接口的压力场景，看到如下信息：</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E9%AB%98%E6%A5%BC%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E8%AF%BE/0087b43a9724667717f90431d8b3f5e5.png" alt=""></p>
<p>看起来 ES client 均衡了，对不对？</p>
<p>它对应的 TPS，如下：</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E9%AB%98%E6%A5%BC%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E8%AF%BE/854de02eb758b1132f7dfbb397262553.png" alt=""></p>
<p>明显 TPS 提升了 60 左右。</p>
<p>ES client 请求不均衡的问题解决了，现在，我们还要来看一下 ES data 单节点 CPU 高的问题。</p>
<h3 id="es-data-cpu-使用率高">ES Data CPU 使用率高</h3>
<ol>
<li><strong>第一阶段：加一个 CPU</strong></li>
</ol>
<p>在 TPS 提升之后，我们再来看一下全局监控数据。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E9%AB%98%E6%A5%BC%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E8%AF%BE/f1846a339f0015d590952668c3fe0a92.png" alt=""></p>
<p>看起来比一开始好多了。基于前面分析 ES client 的经验，我们就先来查一下 ES data 的 iptables 规则：</p>
<p>&ndash; 查看下有哪些 ES data 的 POD<br>
[root@k8s-master-1 ~]# kubectl get pods -o wide | grep data<br>
elasticsearch-data-0                        1/1     Running   0          10h     10.100.18.197    k8s-worker-5   <none>           <none><br>
elasticsearch-data-1                        1/1     Running   0          10h     10.100.5.5       k8s-worker-7   <none>           <none><br>
elasticsearch-data-2                        1/1     Running   0          10h     10.100.251.67    k8s-worker-9   <none>           <none></p>
<p>&ndash; 查看 ES data 对应的 iptables 规则<br>
[root@k8s-master-1 ~]# iptables -S KUBE-SERVICES -t nat|grep elasticsearch-data<br>
-A KUBE-SERVICES ! -s 10.100.0.0/16 -d 10.96.16.151/32 -p tcp -m comment &ndash;comment &ldquo;default/elasticsearch-data:http cluster IP&rdquo; -m tcp &ndash;dport 9200 -j KUBE-MARK-MASQ<br>
-A KUBE-SERVICES -d 10.96.16.151/32 -p tcp -m comment &ndash;comment &ldquo;default/elasticsearch-data:http cluster IP&rdquo; -m tcp &ndash;dport 9200 -j KUBE-SVC-4LU6GV7CN63XJXEQ<br>
-A KUBE-SERVICES ! -s 10.100.0.0/16 -d 10.96.16.151/32 -p tcp -m comment &ndash;comment &ldquo;default/elasticsearch-data:transport cluster IP&rdquo; -m tcp &ndash;dport 9300 -j KUBE-MARK-MASQ<br>
-A KUBE-SERVICES -d 10.96.16.151/32 -p tcp -m comment &ndash;comment &ldquo;default/elasticsearch-data:transport cluster IP&rdquo; -m tcp &ndash;dport 9300 -j KUBE-SVC-W4QKPGOO4JGYQZDQ</p>
<p>&ndash; 查看 9200（外部通信）对应的规则<br>
[root@k8s-master-1 ~]# iptables -S KUBE-SVC-4LU6GV7CN63XJXEQ -t nat<br>
-N KUBE-SVC-4LU6GV7CN63XJXEQ<br>
-A KUBE-SVC-4LU6GV7CN63XJXEQ -m comment &ndash;comment &ldquo;default/elasticsearch-data:http&rdquo; -m statistic &ndash;mode random &ndash;probability 0.33333333349 -j KUBE-SEP-ZHLKOYKJY5GV3ZVN<br>
-A KUBE-SVC-4LU6GV7CN63XJXEQ -m comment &ndash;comment &ldquo;default/elasticsearch-data:http&rdquo; -m statistic &ndash;mode random &ndash;probability 1 -j KUBE-SEP-6ILKZEZS3TMCB4VJ<br>
-A KUBE-SVC-4LU6GV7CN63XJXEQ -m comment &ndash;comment &ldquo;default/elasticsearch-data:http&rdquo; -j KUBE-SEP-JOYLBDPA3LNXKWUK</p>
<p>&ndash; 查看以上三条规则的转发目标<br>
[root@k8s-master-1 ~]# iptables -S KUBE-SEP-ZHLKOYKJY5GV3ZVN -t nat<br>
-N KUBE-SEP-ZHLKOYKJY5GV3ZVN<br>
-A KUBE-SEP-ZHLKOYKJY5GV3ZVN -s 10.100.18.197/32 -m comment &ndash;comment &ldquo;default/elasticsearch-data:http&rdquo; -j KUBE-MARK-MASQ<br>
-A KUBE-SEP-ZHLKOYKJY5GV3ZVN -p tcp -m comment &ndash;comment &ldquo;default/elasticsearch-data:http&rdquo; -m tcp -j DNAT &ndash;to-destination 10.100.18.197:9200<br>
[root@k8s-master-1 ~]# iptables -S KUBE-SEP-6ILKZEZS3TMCB4VJ -t nat<br>
-N KUBE-SEP-6ILKZEZS3TMCB4VJ<br>
-A KUBE-SEP-6ILKZEZS3TMCB4VJ -s 10.100.251.67/32 -m comment &ndash;comment &ldquo;default/elasticsearch-data:http&rdquo; -j KUBE-MARK-MASQ<br>
-A KUBE-SEP-6ILKZEZS3TMCB4VJ -p tcp -m comment &ndash;comment &ldquo;default/elasticsearch-data:http&rdquo; -m tcp -j DNAT &ndash;to-destination 10.100.251.67:9200<br>
[root@k8s-master-1 ~]# iptables -S KUBE-SEP-JOYLBDPA3LNXKWUK -t nat<br>
-N KUBE-SEP-JOYLBDPA3LNXKWUK<br>
-A KUBE-SEP-JOYLBDPA3LNXKWUK -s 10.100.5.5/32 -m comment &ndash;comment &ldquo;default/elasticsearch-data:http&rdquo; -j KUBE-MARK-MASQ<br>
-A KUBE-SEP-JOYLBDPA3LNXKWUK -p tcp -m comment &ndash;comment &ldquo;default/elasticsearch-data:http&rdquo; -m tcp -j DNAT &ndash;to-destination 10.100.5.5:9200<br>
[root@k8s-master-1 ~]</p>
<p>Everythins is perfect!！规则很合理。ES Data 总共有三个 pod，从逻辑上来看，它们各占了三分之一。</p>
<p>在前面的 ES client 分析中，我们讲到，第一个 POD 是 0.5，下一条自然也只剩下 0.5，这很容易理解。现在，ES data 的部分有三条 iptables 规则，我们来简单说明一下。</p>
<blockquote>
<p>通常我们理解的 iptables 就是一个防火墙。不过，要是从根本上来讲，它不算是一个防火墙，只是一堆规则列表，而通过 iptables 设计的规则列表，请求会对应到 netfilter 框架中去，而这个 netfilter 框架，才是真正的防火墙。其中，netfilter 是处于内核中的，iptables 就只是一个用户空间上的配置工具而已。</p>
</blockquote>
<blockquote>
<p>我们知道 iptables 有四表五链。四表是：fileter 表（负责过滤）、nat 表（负责地址转换）、mangle 表（负责解析）、raw 表（关闭 nat 表上启用的连接追踪）；五链是：prerouting 链（路由前）、input 链（输入规则）、forward 链（转发规则）、output 链（输出规则）、postrouting 链（路由后）。</p>
</blockquote>
<blockquote>
<p>而在这一部分，我们主要是看 nat 表以及其上的链。对于其他的部分，如果你想学习，可自行查阅 iptables 相关知识。毕竟，我还时刻记得自己写的是一个性能专栏，而不是计算机基础知识专栏，哈哈。</p>
</blockquote>
<p>从上面的信息可以看到，我们的这个集群中有三个 ES data 服务，对应着三条转发规则，其中，第一条规则的匹配比例是：0.33333333349；第二条比例：0.50000000000；第三条是 1。而这三条转发规则对应的 POD IP 和端口分别是：10.100.18.197:9200、10.100.251.67:9200、10.100.5.5:9200，这也就意味着，通过这三条 iptables 规则可以实现负载均衡，画图理解如下：</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E9%AB%98%E6%A5%BC%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E8%AF%BE/98410f4f455112d712c6198df49b7887.png" alt=""></p>
<p>我们假设有 30 个请求进来，那 ES Data 0 上就会有 30x0.33333333349=10 个请求；对于剩下的 20 个请求，在 ES Data 1 上就会有 20x0.50000000000=10 个请求；而最后剩下的 10 个请求，自然就到了 ES Data 2 上。这是一个非常均衡的逻辑，只是在 iptables 规则中，我看着这几个数据比例，实在是觉得别扭。</p>
<p>既然明白了这个逻辑，下面我们还是把查询商品接口的场景压起来看一下：</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E9%AB%98%E6%A5%BC%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E8%AF%BE/8fec37c16f96f7e8702709d0b42472d6.png" alt=""></p>
<p>从数据上来看，经常会出现 ES data 某个节点消耗 CPU 高的情况。可是，对应到我们前面看到的全局 worker 监控界面中，并没有哪个 worker 的 CPU 很高。所以，在这里，我们要查一下 ES Data 中的 cgroup 配置，看它的限制是多少。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E9%AB%98%E6%A5%BC%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E8%AF%BE/3447aebac18285db96c3d290f6cc598a.png" alt=""></p>
<p>也就是说，ES data 的每个 POD 都是配置了一颗 CPU，难怪 CPU 使用率动不动就红了。</p>
<p>还有一点你要记住，前面我们在查看 data 列表的时候发现，ES data 0 在 worker-5 上，ES data 1 在 worker-7 上，ES data 2 在 worker-9 上。而我们现在看到的却是，它们都各自分到了一个 CPU。既然如此，那我们就再添加一个 CPU，然后再回去看一下 worker-5/7/9 的反应。为什么只加一个 CPU 呢？因为从 worker-7 上来看，现在的 CPU 使用率已经在 50% 左右了，要是加多了，我怕它吃不消。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E9%AB%98%E6%A5%BC%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E8%AF%BE/43b18e189607e31da0defb2d6cec2b12.png" alt=""></p>
<p>看一下压力场景执行的效果：</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E9%AB%98%E6%A5%BC%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E8%AF%BE/31a2a8958b382321706afe0e01e0b2ff.png" alt=""></p>
<p>似乎……不怎么样？TPS 并没有增加。</p>
<ol>
<li><strong>第二阶段：加副本</strong></li>
</ol>
<p>我们再看加了 CPU 之后的全局 POD 监控：</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E9%AB%98%E6%A5%BC%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E8%AF%BE/9621e6741d069f7ed199b21d5b7b75b7.png" alt=""></p>
<p>还是只有一个 ES data 的 CPU 使用率高，所以我想查一下 ES 中的数据分布。因为负载均衡的问题解决了，并且知道有三个 ES data 节点。现在我们就要知道是不是每个节点都被访问到了。</p>
<p>pms                               0 p 10.100.18.199 _w   32   17690 18363   6.7mb  7820 true  true  8.5.1 false<br>
pms                               0 p 10.100.18.199 _15  41    2110     0 465.7kb  5500 true  true  8.5.1 true<br>
pms                               0 p 10.100.18.199 _16  42   21083 30255   9.5mb  5900 true  true  8.5.1 false<br>
pms                               0 p 10.100.18.199 _17  43    2572     0   568kb  5500 true  true  8.5.1 true<br>
pms                               0 p 10.100.18.199 _18  44    1403     0 322.9kb  5500 true  true  8.5.1 true<br>
pms                               0 p 10.100.18.199 _19  45    1856     0 414.1kb  5500 true  true  8.5.1 true<br>
pms                               0 p 10.100.18.199 _1a  46    1904     0   423kb  5500 true  true  8.5.1 true</p>
<p>为啥数据都在一个节点上（都是 10.100.18.199）？看起来只有一个数据副本的原因了。</p>
<p>green open pms                                            A&ndash;6O32bQaSBrJPJltOLHQ 1 0   48618 48618  55.1mb  18.3mb</p>
<p>所以，我们先把副本数加上去，因为我们有三个 data 节点，所以这里加三个副本：</p>
<p>PUT /pms/_settings<br>
{<br>
&ldquo;number_of_replicas&rdquo;: 3<br>
}</p>
<p>我们再次查看 ES 中的数据分布，如下所示：</p>
<p>pms                               0 r 10.100.18.200 _w   32   17690 18363   6.7mb  7820 true  true  8.5.1 false<br>
pms                               0 r 10.100.18.200 _15  41    2110     0 465.7kb  5500 true  true  8.5.1 true<br>
pms                               0 r 10.100.18.200 _16  42   21083 30255   9.5mb  5900 true  true  8.5.1 false<br>
pms                               0 r 10.100.18.200 _17  43    2572     0   568kb  5500 true  true  8.5.1 true<br>
pms                               0 r 10.100.18.200 _18  44    1403     0 322.9kb  5500 true  true  8.5.1 true<br>
pms                               0 r 10.100.18.200 _19  45    1856     0 414.1kb  5500 true  true  8.5.1 true<br>
pms                               0 r 10.100.18.200 _1a  46    1904     0   423kb  5500 true  true  8.5.1 true<br>
pms                               0 p 10.100.251.69 _w   32   17690 18363   6.7mb  7820 true  true  8.5.1 false<br>
pms                               0 p 10.100.251.69 _15  41    2110     0 465.7kb  5500 true  true  8.5.1 true<br>
pms                               0 p 10.100.251.69 _16  42   21083 30255   9.5mb  5900 true  true  8.5.1 false<br>
pms                               0 p 10.100.251.69 _17  43    2572     0   568kb  5500 true  true  8.5.1 true<br>
pms                               0 p 10.100.251.69 _18  44    1403     0 322.9kb  5500 true  true  8.5.1 true<br>
pms                               0 p 10.100.251.69 _19  45    1856     0 414.1kb  5500 true  true  8.5.1 true<br>
pms                               0 p 10.100.251.69 _1a  46    1904     0   423kb  5500 true  true  8.5.1 true<br>
pms                               0 r 10.100.140.10 _w   32   17690 18363   6.7mb  7820 true  true  8.5.1 false<br>
pms                               0 r 10.100.140.10 _15  41    2110     0 465.7kb  5500 true  true  8.5.1 true<br>
pms                               0 r 10.100.140.10 _16  42   21083 30255   9.5mb  5900 true  true  8.5.1 false<br>
pms                               0 r 10.100.140.10 _17  43    2572     0   568kb  5500 true  true  8.5.1 true<br>
pms                               0 r 10.100.140.10 _18  44    1403     0 322.9kb  5500 true  true  8.5.1 true<br>
pms                               0 r 10.100.140.10 _19  45    1856     0 414.1kb  5500 true  true  8.5.1 true<br>
pms                               0 r 10.100.140.10 _1a  46    1904     0   423kb  5500 true  true  8.5.1 true</p>
<p>我们接着压起来，看看 POD 的资源：</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E9%AB%98%E6%A5%BC%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E8%AF%BE/e728069526897faf40a4f946cefad465.png" alt=""></p>
<p>现在看着是不是开心多了？data 节点的 CPU 都用起来了。</p>
<p>我们再看一下 worker 的资源：</p>
<p>[root@k8s-master-1 ~]# kubectl get pods -o wide | grep data<br>
elasticsearch-data-0                        1/1     Running   0          16m     10.100.18.199    k8s-worker-5   <none>           <none><br>
elasticsearch-data-1                        1/1     Running   0          17m     10.100.251.68    k8s-worker-9   <none>           <none><br>
elasticsearch-data-2                        1/1     Running   0          18m     10.100.140.9     k8s-worker-2   <none>           <none></p>
<p>现在 ES Data 的 POD 分布到 2、5、9 三这个 worker 上去了，我们查看下全局监控：</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E9%AB%98%E6%A5%BC%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E8%AF%BE/f74e8f2800b76791e7aabed436dd5868.png" alt=""></p>
<p>嗯，不错，ES data 的 POD 把资源用起来了。其实这里要想继续调，还可以把 CPU 加大，ES 本来就是吃 CPU、内存的大户。不过，我们前面在配置的时候，给 ES data 的 CPU 也确实太小了。这个问题，并不是我故意设计出来的，而是当时在部署的时候，没考虑到这些。</p>
<p>最后，我们来看优化后的效果：</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E9%AB%98%E6%A5%BC%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E8%AF%BE/6d29d9ad2854aae47230ec54899969f0.png" alt=""></p>
<p>呀呀呀，你看 TPS 压都压不住呀，很快涨到 900 左右了！这个优化结果很好。</p>
<p>现在回过头来看第一个阶段，我们加 CPU 没有效果，主要还是因为副本数量太少。其实，在 ES 的优化中，还有很多细节可以玩。只不过，在我们这个课程中，我希望给你的是一个整体的分析思路和逻辑，而不是纠结于每个细节上的参数。所以，在这里，我们就不再说具体参数的调整了。</p>
<p>如果你想在 ES 上做更多的优化，可以在分析完业务之后，确定一下 ES 的架构、数据索引、分片等信息，然后再来设计一个合理的 ES 部署。</p>
<h2 id="总结">总结</h2>
<p>在这节课中，我们看到 APM 工具也有无能为力的地方。所以说，当我们分析到一个具体组件之后，要想再往下分析，就得靠自己的技术功底了。</p>
<p>在出现请求不均衡的时候，我们一定要先去看负载均衡的逻辑有没有问题。当看到 ES client 不均衡时，我们去看了 iptables 的原理，在发现 iptables 只有一个转发规则的时候，接下来要做的当然就是重刷转发规则了。</p>
<p>在 ES client 转发均衡了之后，我们在 ES data 单节点上又看到 CPU 使用率过高。由于 ES data 在 POD 当中，我们自然就要想到去看 cgroup 的限制。</p>
<p>而在添加了 CPU 之后，我们发现 TPS 并没有提高，这时候就要去看 ES 的逻辑了。ES 的强大之处就在于多副本多分片的查询能力，所以，我们增加了副本之后，CPU 就用起来了，这是一个合理的优化结果，TPS 也自然提高了。</p>
<p>经过一系列的动作，我们终于把资源给用起来了。这也是我一直在强调的，<strong>性能优化第一个阶段的目标，就是把资源给用起来，然后再考虑更细节的优化</strong>。</p>
<h2 id="课后作业">课后作业</h2>
<p>最后，我给你留两道题，请你思考一下：</p>
<ol>
<li>当负载出现不均衡时，主要的分析方向是什么？</li>
<li>什么时候才需要去看组件内部的实现逻辑？</li>
</ol>
<p>记得在留言区和我讨论、交流你的想法，每一次思考都会让你更进一步。</p>
<p>如果你读完这篇文章有所收获，也欢迎你分享给你的朋友，共同学习进步。我们下一讲再见！</p>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content"></span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
        10100-01-10
        
    </span>
  </p>
  
  
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/%E9%AB%98%E6%A5%BC%E7%9A%84%E6%80%A7%E8%83%BD%E5%B7%A5%E7%A8%8B%E5%AE%9E%E6%88%98%E8%AF%BE/">高楼的性能工程实战课</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E9%AB%98%E6%89%8B%E8%AF%BE/15__%E5%B9%B6%E5%8F%91%E5%AE%9E%E7%8E%B0%E6%8E%8C%E6%8F%A1%E4%B8%8D%E5%90%8C%E5%B9%B6%E5%8F%91%E6%A1%86%E6%9E%B6%E7%9A%84%E9%80%89%E6%8B%A9%E5%92%8C%E4%BD%BF%E7%94%A8%E7%A7%98%E8%AF%80/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">15__并发实现：掌握不同并发框架的选择和使用秘诀</span>
            <span class="prev-text nav-mobile">上一篇</span>
          </a>
        <a class="next" href="/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E6%80%A7%E8%83%BD%E5%B7%A5%E7%A8%8B%E9%AB%98%E6%89%8B%E8%AF%BE/15__%E5%B8%B8%E8%A7%81%E7%9A%84%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E4%B9%8Bcpu%E7%AF%87%E5%A6%82%E4%BD%95%E8%AE%A9cpu%E7%9A%84%E8%BF%90%E8%A1%8C%E4%B8%8D%E5%8F%97%E9%98%BB%E7%A2%8D/">
            <span class="next-text nav-default">15__常见的性能问题之CPU篇：如何让CPU的运行不受阻碍？</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
  
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2024 - 
    2025<span class="heart"><i class="iconfont icon-heart"></i></span><span></span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  <script src="/lib/highlight/highlight.pack.js?v=20171001"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js"></script>


      <script async src="https://www.googletagmanager.com/gtag/js?id=G-FVZ07KBD4X"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-FVZ07KBD4X');
        }
      </script>






</body>
</html>
