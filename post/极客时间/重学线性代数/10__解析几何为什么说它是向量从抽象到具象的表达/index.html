<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>10__解析几何：为什么说它是向量从抽象到具象的表达？ - Docs</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="" /><meta name="description" content="你好，我是朱维刚。欢迎你继续跟我学习线性代数，今天我们要讲的内容是“解析几何”。
前面所有章节我们都是围绕向量、矩阵，以及向量空间来展开的。但这一节课有点不一样，我要讲的是解析几何，它使得向量从抽象走向了具象，让向量具有了几何的含义。比如，计算向量的长度、向量之间的距离和角度，这在机器学习的主成分分析 PCA 中是非常有用的。
" /><meta name="keywords" content="技术文档, docs, 极客时间" />






<meta name="generator" content="Hugo 0.140.2 with theme even" />


<link rel="canonical" href="https://politcloud.org/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E9%87%8D%E5%AD%A6%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/10__%E8%A7%A3%E6%9E%90%E5%87%A0%E4%BD%95%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AF%B4%E5%AE%83%E6%98%AF%E5%90%91%E9%87%8F%E4%BB%8E%E6%8A%BD%E8%B1%A1%E5%88%B0%E5%85%B7%E8%B1%A1%E7%9A%84%E8%A1%A8%E8%BE%BE/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:url" content="https://politcloud.org/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E9%87%8D%E5%AD%A6%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/10__%E8%A7%A3%E6%9E%90%E5%87%A0%E4%BD%95%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AF%B4%E5%AE%83%E6%98%AF%E5%90%91%E9%87%8F%E4%BB%8E%E6%8A%BD%E8%B1%A1%E5%88%B0%E5%85%B7%E8%B1%A1%E7%9A%84%E8%A1%A8%E8%BE%BE/">
  <meta property="og:site_name" content="Docs">
  <meta property="og:title" content="10__解析几何：为什么说它是向量从抽象到具象的表达？">
  <meta property="og:description" content="你好，我是朱维刚。欢迎你继续跟我学习线性代数，今天我们要讲的内容是“解析几何”。
前面所有章节我们都是围绕向量、矩阵，以及向量空间来展开的。但这一节课有点不一样，我要讲的是解析几何，它使得向量从抽象走向了具象，让向量具有了几何的含义。比如，计算向量的长度、向量之间的距离和角度，这在机器学习的主成分分析 PCA 中是非常有用的。">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2024-01-10T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-01-10T00:00:00+00:00">
    <meta property="article:tag" content="重学线性代数">

  <meta itemprop="name" content="10__解析几何：为什么说它是向量从抽象到具象的表达？">
  <meta itemprop="description" content="你好，我是朱维刚。欢迎你继续跟我学习线性代数，今天我们要讲的内容是“解析几何”。
前面所有章节我们都是围绕向量、矩阵，以及向量空间来展开的。但这一节课有点不一样，我要讲的是解析几何，它使得向量从抽象走向了具象，让向量具有了几何的含义。比如，计算向量的长度、向量之间的距离和角度，这在机器学习的主成分分析 PCA 中是非常有用的。">
  <meta itemprop="datePublished" content="2024-01-10T00:00:00+00:00">
  <meta itemprop="dateModified" content="2024-01-10T00:00:00+00:00">
  <meta itemprop="wordCount" content="4614">
  <meta itemprop="keywords" content="重学线性代数">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="10__解析几何：为什么说它是向量从抽象到具象的表达？">
  <meta name="twitter:description" content="你好，我是朱维刚。欢迎你继续跟我学习线性代数，今天我们要讲的内容是“解析几何”。
前面所有章节我们都是围绕向量、矩阵，以及向量空间来展开的。但这一节课有点不一样，我要讲的是解析几何，它使得向量从抽象走向了具象，让向量具有了几何的含义。比如，计算向量的长度、向量之间的距离和角度，这在机器学习的主成分分析 PCA 中是非常有用的。">

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Docs</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Docs</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">10__解析几何：为什么说它是向量从抽象到具象的表达？</h1>

      <div class="post-meta">
        <span class="post-time"> 10100-01-10 </span>
        <div class="post-category">
            <a href="/categories/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/"> 极客时间 </a>
            </div>
          <span class="more-meta"> 约 4614 字 </span>
          <span class="more-meta"> 预计阅读 10 分钟 </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#范数">范数</a></li>
        <li><a href="#内积">内积</a>
          <ul>
            <li><a href="#点积">点积</a></li>
            <li><a href="#其他内积">其他内积</a></li>
            <li><a href="#内积空间">内积空间</a></li>
          </ul>
        </li>
        <li><a href="#对称正定矩阵">对称正定矩阵</a></li>
        <li><a href="#长度距离和角度">长度、距离和角度</a></li>
        <li><a href="#正交投影">正交投影</a>
          <ul>
            <li><a href="#投影到一维子空间上线">投影到一维子空间上（线）</a></li>
          </ul>
        </li>
        <li><a href="#本节小结">本节小结</a></li>
        <li><a href="#线性代数练习场">线性代数练习场</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p>你好，我是朱维刚。欢迎你继续跟我学习线性代数，今天我们要讲的内容是“解析几何”。</p>
<p>前面所有章节我们都是围绕向量、矩阵，以及向量空间来展开的。但这一节课有点不一样，我要讲的是解析几何，它使得向量从抽象走向了具象，让向量具有了几何的含义。比如，计算向量的长度、向量之间的距离和角度，这在机器学习的主成分分析 PCA 中是非常有用的。</p>
<h2 id="范数">范数</h2>
<p>讲解析几何我们得从“范数”开始讲起。</p>
<p>因为很多人看到几何向量的第一反应就是，它是从原点开始的有向线段，并且向量的长度是这个有向线段的终端和起始端之间的距离。而范数，就是被用来度量某个向量空间或矩阵中的每个向量的长度或大小的。</p>
<p>现在，我们先来看一下范数的数学定义：一个向量空间 V 上的一个范数就是一个函数，它计算 V 中的每一个向量 x 的长度，用符号来表示的话就是：∥x∥∈R，它满足三种性质：</p>
<ol>
<li>正齐次性：如果输入参数扩大正 λ 倍，其对应的函数也扩正大倍。设 λ∈R，x∈V，∥λx∥=∣λ∣∥x∥；</li>
<li>次可加性：类似三角不等式，两边之和大于第三边。设 x,y∈V，∥x+y∥≤∥x∥+∥y∥；</li>
<li>正定性：向量 x 的长度一定大于等于零。∥x∥≥0。</li>
</ol>
<p>看到这里，你也许会问，范数似乎和以前老师教的<strong>向量的模</strong>一样啊。先别急，它们还真有那么一点关系，你听我慢慢道来。由于范数是度量某个向量空间或矩阵中的每个向量的长度或大小的，所以它和向量空间维度是有关系的，于是，我们可以把范数写成这样的模式来区分不同维度的大小计算：L1​,L2​,…,L∞​。</p>
<ol>
<li>L1​ 范数：曼哈顿范数，也叫曼哈顿距离，设 x∈Rn，得到下面这个表达式。</li>
</ol>
<p>∥x∥1​=i=1∑n​∣xi​∣</p>
<ol>
<li>L2​ 范数：欧式范数，也叫欧式距离，设 x∈Rn，得到下面这个表达式。</li>
</ol>
<p>∥x∥2​=i=1∑n​xi2​​</p>
<ol>
<li>L∞​ 范数：切比雪夫范数，也叫切比雪夫距离，设 x∈Rn，得到下面这个表达式。</li>
</ol>
<p>∥x∥∞​=max(∣x1​∣,∣x2​∣,…,∣xn​∣)</p>
<p>我们发现，向量的模和 L2​ 范数的计算方式都是一样的，都表示的是欧氏距离，所以，我们可以简单地认为向量的模等于 L2​ 范数。而其他的范数模式和向量的模则没有任何关系。</p>
<h2 id="内积">内积</h2>
<p>学习解析几何时，我们必须掌握的第二个概念就是内积。</p>
<p>如果说范数是模式，是用来描述向量长度或大小的概念性表达，那么内积可以让我们很直观地了解一个向量的长度、两个向量之间的距离和角度，它的一个主要目的就是判断向量之间是否是正交的，正交这个概念我们会在后面讲解。</p>
<h3 id="点积">点积</h3>
<p>我们从特殊到一般，先来看点积，它和第三篇矩阵中说的“普通矩阵乘”形式一样，点积是特殊的内积，为什么说它特殊呢？那是因为在表示两个向量之间的距离时，它就是大家熟悉的欧式距离，点积可以表示成这样的形式：</p>
<p>xTy=i=1∑n​xi​yi​</p>
<h3 id="其他内积">其他内积</h3>
<p>除了点积外，我们再来看另一个不同的内积：设内积空间 V 是 R2，定义内积 ⟨x,y⟩=x1​y1​−(x1​y2​+x2​y1​)+2x2​y2​，一看便知这个和点积完全不同。</p>
<h3 id="内积空间">内积空间</h3>
<p>最后，我们再来看一般内积和内积空间。因为解析几何关注的是向量的长度、两个向量之间的距离和角度，所以，我们要在原来向量空间上加一个额外的结构，这个额外结构就是内积，而加了内积的向量空间，我们就叫做内积空间。</p>
<p>为了表达方便，我们可以把内积写成 ⟨ ⋅,⋅⟩ 这样的形式，那么内积空间 V 可以被表示成这样：(V,⟨ ⋅,⋅⟩)。这时，如果一般内积由点积来表达，那这个向量空间就变成了更具体的欧式向量空间。</p>
<p>接下来看下内积空间有什么性质？我们定义一个内积空间 V 和它的元素 x、y、z，以及一个 c∈R：</p>
<ol>
<li>满足对称性：x 和 y 的内积等于 y 和 x 的内积，⟨x,y⟩=⟨y,x⟩；</li>
<li>满足线性性：x 和 y+cz 的内积等于，x 和 y 的内积，与 x 和 z 的内积乘以 c 后的和，</li>
</ol>
<p>⟨x,y+cz⟩=⟨x,y⟩+c⟨x,z⟩；</p>
<ol start="3">
<li>满足正定性：x 和 y 的内积大于等于零，⟨x,y⟩≥0。</li>
</ol>
<h2 id="对称正定矩阵">对称正定矩阵</h2>
<p>内积还定义了一类矩阵，这类矩阵在机器学习中很重要，因为它可以被用来判定多元函数极值，而在深度学习中，它更是被用来获取最小化损失函数，我们把这类矩阵叫做对称正定矩阵。</p>
<p>对称正定矩阵的定义是：如果一个对称矩阵 A 属于方阵 Rn×n，对任意非零向量 x，都有 xTAx&gt;0，那么 A 就是对称正定矩阵。</p>
<p>我们来看两个例子，判断它们是不是对称正定矩阵。</p>
<p>第一个例子，请你回答下面这个矩阵是对称正定矩阵吗？</p>
<p>A=[96​65​]</p>
<p>答案：是的，它是对称正定矩阵。因为 xTAx&gt;0。</p>
<p>xTAx=[x1​​x2​​][96​65​][x1​x2​​]=(3x1​+2x2​)2+x22​&gt;0</p>
<p>第二个例子，请你看下面这个矩阵是对称正定矩阵吗？</p>
<p>A=[96​63​]</p>
<p>答案：不是的，它只是对称矩阵。因为 xTAx 可能小于 0。</p>
<p>xTAx=[x1​​x2​​][96​63​][x1​x2​​]=(3x1​+2x2​)2−x22​</p>
<h2 id="长度距离和角度">长度、距离和角度</h2>
<p>前面我们通过范数讲了向量的长度，但从内积的角度来看，我们发现，内积和范数之间有着千丝万缕的关系。我们来看看下面这个等式。</p>
<p>∥x∥=⟨x,x⟩​</p>
<p>从这个等式我们发现，内积可以用来产生范数，确实是这样。不过，不是每一个范数都能被一个内积产生的，比如：曼哈顿范数。接下来，我们还是来关注能由内积产生的范数上，从不同的角度来看看几何上的长度、距离和角度的概念。</p>
<p>我们先用内积来计算一个<strong>向量的长度</strong>，比如：向量 x=[1​1​]T，我们可以使用点积来计算，计算后得出 x 的范数是 2​，具体计算过程是这样的：∥x∥=xTx​=12+12​=2​。</p>
<p>接着，我们再来看一下<strong>向量之间的距离</strong>，一个内积空间 V，(V,⟨ ⋅,⋅⟩)，x 和 y 是它的两个向量，那么 x 和 y 之间的距离就可以表示成：d(x,y)=∥x−y∥=⟨x−y,x−y⟩​。</p>
<p>如果用点积来计算 x 和 y 之间的距离，那这个距离就叫做欧式距离。</p>
<p>再接着，来看看两个<strong>向量之间的角度</strong>。我们使用柯西 - 施瓦茨不等式（Cauchy-Schwarz Inequality）来表示内积空间中两个向量 x 和 y 之间的角度：a。</p>
<p>−1≤∥x∥∥y∥⟨x,y⟩​≤1</p>
<p>取值是从 −1 到 1 之间，那么角度就是从 0 到 π 之间，我们用 cos 来表示就是：</p>
<p>cos(a)=∥x∥∥y∥⟨x,y⟩​</p>
<p>其中 a 就是角度，a 的角度取值是 0 到 π 之间。我们很容易就能发现，其实两个向量之间的角度，就是告诉了我们两个向量之间方向的相似性。例如：x 和 y=4x，使用点积来计算它们之间的角度是 0，也就是说它们的方向是一样的，y 只是对 x 扩大了 4 倍而已。</p>
<p>现在，我们通过一个例子，再来更清楚地看下两个向量之间角度的计算，设 x=[1​1​]T，y=[1​2​]T，使用点积来计算，我们得出：</p>
<p>cos(a)=⟨x,x⟩⟨y,y⟩​⟨x,y⟩​=xTxyTy​xTy​=10​3​</p>
<p>那么，这两个向量之间的角度如下。</p>
<p>arccos(10​3​)≈0.32</p>
<p>我们可以用图来更直观地表达一下。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E9%87%8D%E5%AD%A6%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/49b8598405435168c42ef37124ba0863.png" alt=""></p>
<p>于是，我们最后可以引出一个概念，也就是我们在一开始提到的<strong>正交性</strong>。如果两个向量 x 和 y 内积等于 0，⟨x,y⟩=0，那么 x 和 y 是正交的，这可以写成：x⊥y。再如果，x 和 y 的范数都等于 1，∥x∥=∥y∥=1，也就是说，如果它们都是单位向量，那么 x 和 y 就是标准正交的。</p>
<p>我们用图来更直观地表达一下。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E9%87%8D%E5%AD%A6%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/5f07b371e9d114a0f6879a519993295e.png" alt=""></p>
<h2 id="正交投影">正交投影</h2>
<p>在理论讲解之后，我们要来了解一下解析几何在实践中经常运用的概念——正交投影，它是一种重要的线性变换，在图形图像、编码理论、统计和机器学习中扮演了重要角色。</p>
<p>在机器学习中，数据一般都是高维的。众所周知，高维数据是很难来分析和可视化的。而且，不是所有的高维数据都是有用的，可能只有一些数据包含了大部分的重要信息。</p>
<p>正交投影就是高维到低维的数据投影，在第 5 节课线性空间中，我简单介绍了高维数据投影到低维后，我们就能在低维空间更多地了解数据集、提炼相关模式。而在机器学习中，最普遍的降维算法——PCA 主成分分析，就是利用了降维的观点。</p>
<p>接下来，我开始讲解正交投影，在给出定义之前，先从一张图来了解会更直观。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E9%87%8D%E5%AD%A6%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/e72bac315081ef1f1ae51e6175fc0906.png" alt=""></p>
<p>图中的蓝点是原二维数据，黄点是它们的正交投影。所以，实际降维后，在一维空间中就形成了这条黑线表示，它近似地表达了原来二维数据表示的信息。</p>
<p>现在我们可以来看一下投影的定义：V 是一个向量空间，U 是 V 的一个向量子空间，一个从 V 到 U 的线性映射 Φ 是一个投影，如果它满足：Φ2=Φ∘Φ=Φ。因为线性映射能够被变换矩阵表示，所以，这个定义同样适用于一个特殊类型变换矩阵：投影矩阵 PΦ​，它也满足：PΦ2​=PΦ​。</p>
<h3 id="投影到一维子空间上线">投影到一维子空间上（线）</h3>
<p>接下来，我们来看看如何投影到一维子空间，也就是把内积空间的向量正交投影到子空间，这里我们使用点积作为内积。</p>
<p>假设有一条通过原点的线，这条线是由基向量 b 产生的一维子空间 U，当我们把一个向量 x 投影到 U 时，需要寻找另一个最靠近 x 的向量 ΦU​(x)。还是老样子，我们通过图来看一下。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E9%87%8D%E5%AD%A6%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/93dad17398477e22611ce6a22c6f31da.png" alt=""></p>
<p>首先，投影 ΦU​(x) 靠近 x，也就是要找出 x 和 ΦU​(x) 之间的 ∥x−ΦU​(x)∥ 最小距离，从几何角度来说，就是线段 ΦU​(x)−x 和 b 正交，满足等式：⟨ΦU​(x)−x,b⟩=0。其次，投影 ΦU​(x) 必须是 U 的一个元素，也就是，基向量 b 的一个乘来产生 U，ΦU​(x)=λb。</p>
<p>于是，我们可以通过三个步骤来分别得到 λ、投影 ΦU​(x) 和投影矩阵 PΦ​，来把任意 x 映射到子空间 U 上。</p>
<p>第一步，计算 λ，通过正交条件产生这样的等式：</p>
<p>⟨x−ΦU​(x),b⟩=0。因为 ΦU​(x)=λb，所以它可以转变成：⟨x−λb,b⟩=0。</p>
<p>利用内积的双线性：⟨x,b⟩−λ⟨b,b⟩=0，我们得到：</p>
<p>λ=⟨b,b⟩⟨x,b⟩​=∥b∥2⟨b,x⟩​</p>
<p>然后，我们通过点积得到：</p>
<p>λ=∥b∥2bTx​</p>
<p>如果 ∥b∥=1，那 λ 就等于 bTx。</p>
<p>接着第二步，是计算投影点 ΦU​(x)。从 ΦU​(x)=λb，我们得到：</p>
<p>ΦU​(x)=λb=∥b∥2⟨x,b⟩​b=∥b∥2bTx​b</p>
<p>通过点积来计算，我们就得到了 ΦU​(x) 的长度：</p>
<p>∥ΦU​(x)∥=∥b∥2∣∣​bTx∣∣​​∥b∥=∣cos(a)∣∥x∥∥b∥∥b∥2∥b∥​=∣cos(a)∥x∥</p>
<p>这里的 a，是 x 和 b 之间的夹角。</p>
<p>最后第三步，是计算投影矩阵 PΦ​，投影矩阵是一个线性映射。所以，我们可以得到：ΦU​(x)=PΦ​x，通过 ΦU​(x)=λb，我们可以得到：</p>
<p>ΦU​(x)=λb=bλ=b∥b∥2bTx​=∥b∥2bbT​x</p>
<p>这里，我们立即可以得到投影矩阵 PΦ​ 的计算等式：</p>
<p>PΦ​=∥b∥2bbT​</p>
<h2 id="本节小结">本节小结</h2>
<p>这一节课覆盖的知识点有点多，因为要把解析几何的知识点，浓缩到核心的几个点来讲解是一项艰巨的任务。不过不要怕，前面的几个知识点都是为这一节的重点“正交投影”来铺垫的。范数，被用来度量某个向量空间或矩阵中的每个向量的长度或大小，而内积让我们很直观地了解一个向量的长度、两个向量之间的距离和角度，以及判断向量之间是否是正交的。</p>
<p>所以，希望你能掌握范数和内积的理论知识，并把它和正交投影结合，运用在一些实践应用场景中，比如：3D 图形图像的坐标变换、数据压缩，以及机器学习的降维。</p>
<h2 id="线性代数练习场">线性代数练习场</h2>
<p>请用之前学到的正交投影的投影矩阵算法，来计算一条线上的投影矩阵 PΦ​。</p>
<p>这条线通过原点，由基 b=[1​2​2​]T 产生，PΦ​ 计算后，再通过一个 x 来验证一下它是否在 b 产生的子空间中，我们取 x=[1​1​1​]T。</p>
<p>欢迎在留言区晒出你的运算结果，我会及时回复。同时，也欢迎你把这篇文章分享给你的朋友，一起讨论、学习。</p>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content"></span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
        10100-01-10
        
    </span>
  </p>
  
  
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/%E9%87%8D%E5%AD%A6%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/">重学线性代数</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E5%A6%82%E4%BD%95%E8%90%BD%E5%9C%B0%E4%B8%9A%E5%8A%A1%E5%BB%BA%E6%A8%A1/10__%E5%B0%86%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0%E4%B8%BArestful_api%E4%B8%8A/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">10__将模型实现为RESTful_API（上）</span>
            <span class="prev-text nav-mobile">上一篇</span>
          </a>
        <a class="next" href="/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E8%B6%A3%E8%B0%88linux%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/10__%E8%BF%9B%E7%A8%8B%E5%85%AC%E5%8F%B8%E6%8E%A5%E8%BF%99%E4%B9%88%E5%A4%9A%E9%A1%B9%E7%9B%AE%E5%A6%82%E4%BD%95%E7%AE%A1/">
            <span class="next-text nav-default">10__进程：公司接这么多项目，如何管？</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
  
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2024 - 
    2025<span class="heart"><i class="iconfont icon-heart"></i></span><span></span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  <script src="/lib/highlight/highlight.pack.js?v=20171001"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js"></script>


      <script async src="https://www.googletagmanager.com/gtag/js?id=G-FVZ07KBD4X"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-FVZ07KBD4X');
        }
      </script>






</body>
</html>
