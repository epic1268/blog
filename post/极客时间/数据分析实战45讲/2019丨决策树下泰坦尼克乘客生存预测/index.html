<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>2019丨决策树下泰坦尼克乘客生存预测 - Docs</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="" /><meta name="description" content="19丨决策树（下）：泰坦尼克乘客生存预测
在前面的两篇文章中，我给你讲了决策树算法。决策树算法是经常使用的数据挖掘算法，这是因为决策树就像一个人脑中的决策模型一样，呈现出来非常直观。基于决策树还诞生了很多数据挖掘算法，比如随机森林（Random forest）。
" /><meta name="keywords" content="技术文档, docs, 极客时间" />






<meta name="generator" content="Hugo 0.140.2 with theme even" />


<link rel="canonical" href="https://politcloud.org/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/2019%E4%B8%A8%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8B%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E4%B9%98%E5%AE%A2%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:url" content="https://politcloud.org/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/2019%E4%B8%A8%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8B%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E4%B9%98%E5%AE%A2%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B/">
  <meta property="og:site_name" content="Docs">
  <meta property="og:title" content="2019丨决策树下泰坦尼克乘客生存预测">
  <meta property="og:description" content="19丨决策树（下）：泰坦尼克乘客生存预测
在前面的两篇文章中，我给你讲了决策树算法。决策树算法是经常使用的数据挖掘算法，这是因为决策树就像一个人脑中的决策模型一样，呈现出来非常直观。基于决策树还诞生了很多数据挖掘算法，比如随机森林（Random forest）。">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2024-01-10T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-01-10T00:00:00+00:00">
    <meta property="article:tag" content="数据分析实战45讲">

  <meta itemprop="name" content="2019丨决策树下泰坦尼克乘客生存预测">
  <meta itemprop="description" content="19丨决策树（下）：泰坦尼克乘客生存预测
在前面的两篇文章中，我给你讲了决策树算法。决策树算法是经常使用的数据挖掘算法，这是因为决策树就像一个人脑中的决策模型一样，呈现出来非常直观。基于决策树还诞生了很多数据挖掘算法，比如随机森林（Random forest）。">
  <meta itemprop="datePublished" content="2024-01-10T00:00:00+00:00">
  <meta itemprop="dateModified" content="2024-01-10T00:00:00+00:00">
  <meta itemprop="wordCount" content="4738">
  <meta itemprop="keywords" content="数据分析实战45讲">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="2019丨决策树下泰坦尼克乘客生存预测">
  <meta name="twitter:description" content="19丨决策树（下）：泰坦尼克乘客生存预测
在前面的两篇文章中，我给你讲了决策树算法。决策树算法是经常使用的数据挖掘算法，这是因为决策树就像一个人脑中的决策模型一样，呈现出来非常直观。基于决策树还诞生了很多数据挖掘算法，比如随机森林（Random forest）。">

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Docs</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Docs</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">2019丨决策树下泰坦尼克乘客生存预测</h1>

      <div class="post-meta">
        <span class="post-time"> 10100-01-10 </span>
        <div class="post-category">
            <a href="/categories/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/"> 极客时间 </a>
            </div>
          <span class="more-meta"> 约 4738 字 </span>
          <span class="more-meta"> 预计阅读 10 分钟 </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents"></nav>
  </div>
</div>
    <div class="post-content">
      <p>19丨决策树（下）：泰坦尼克乘客生存预测</p>
<p>在前面的两篇文章中，我给你讲了决策树算法。决策树算法是经常使用的数据挖掘算法，这是因为决策树就像一个人脑中的决策模型一样，呈现出来非常直观。基于决策树还诞生了很多数据挖掘算法，比如随机森林（Random forest）。</p>
<p>今天我来带你用决策树进行项目的实战。</p>
<p>决策树分类的应用场景非常广泛，在各行各业都有应用，比如在金融行业可以用决策树做贷款风险评估，医疗行业可以用决策树生成辅助诊断，电商行业可以用决策树对销售额进行预测等。</p>
<p>在了解决策树的原理后，今天我们用 sklearn 工具解决一个实际的问题：泰坦尼克号乘客的生存预测。</p>
<p>sklearn 中的决策树模型</p>
<p>首先，我们需要掌握 sklearn 中自带的决策树分类器 DecisionTreeClassifier，方法如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">clf = DecisionTreeClassifier(criterion=&#39;entropy&#39;)
</span></span></code></pre></td></tr></table>
</div>
</div><p>到目前为止，sklearn 中只实现了 ID3 与 CART 决策树，所以我们暂时只能使用这两种决策树，在构造 DecisionTreeClassifier 类时，其中有一个参数是 criterion，意为标准。它决定了构造的分类树是采用 ID3 分类树，还是 CART 分类树，对应的取值分别是 entropy 或者 gini：</p>
<p>entropy: 基于信息熵，也就是 ID3 算法，实际结果与 C4.5 相差不大；</p>
<p>gini：默认参数，基于基尼系数。CART 算法是基于基尼系数做属性划分的，所以 criterion=gini 时，实际上执行的是 CART 算法。</p>
<p>我们通过设置 criterion=&lsquo;entropy’可以创建一个 ID3 决策树分类器，然后打印下 clf，看下决策树在 sklearn 中是个什么东西？</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">DecisionTreeClassifier(class_weight=None, criterion=&#39;entropy&#39;, max_depth=None,
</span></span><span class="line"><span class="cl">            max_features=None, max_leaf_nodes=None,
</span></span><span class="line"><span class="cl">            min_impurity_decrease=0.0, min_impurity_split=None,
</span></span><span class="line"><span class="cl">            min_samples_leaf=1, min_samples_split=2,
</span></span><span class="line"><span class="cl">            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
</span></span><span class="line"><span class="cl">            splitter=&#39;best&#39;)
</span></span></code></pre></td></tr></table>
</div>
</div><p>这里我们看到了很多参数，除了设置 criterion 采用不同的决策树算法外，一般建议使用默认的参数，默认参数不会限制决策树的最大深度，不限制叶子节点数，认为所有分类的权重都相等等。当然你也可以调整这些参数，来创建不同的决策树模型。</p>
<p>我整理了这些参数代表的含义：</p>
<p>在构造决策树分类器后，我们可以使用 fit 方法让分类器进行拟合，使用 predict 方法对新数据进行预测，得到预测的分类结果，也可以使用 score 方法得到分类器的准确率。</p>
<p>下面这个表格是 fit 方法、predict 方法和 score 方法的作用。</p>
<p>Titanic 乘客生存预测</p>
<p>问题描述</p>
<p>泰坦尼克海难是著名的十大灾难之一，究竟多少人遇难，各方统计的结果不一。现在我们可以得到部分的数据，具体数据你可以从 GitHub 上下载：https://github.com/cystanford/Titanic_Data</p>
<p>其中数据集格式为 csv，一共有两个文件：</p>
<p>train.csv 是训练数据集，包含特征信息和存活与否的标签；</p>
<p>test.csv: 测试数据集，只包含特征信息。</p>
<p>现在我们需要用决策树分类对训练集进行训练，针对测试集中的乘客进行生存预测，并告知分类器的准确率。</p>
<p>在训练集中，包括了以下字段，它们具体为：</p>
<p>生存预测的关键流程</p>
<p>我们要对训练集中乘客的生存进行预测，这个过程可以划分为两个重要的阶段：</p>
<p>准备阶段：我们首先需要对训练集、测试集的数据进行探索，分析数据质量，并对数据进行清洗，然后通过特征选择对数据进行降维，方便后续分类运算；</p>
<p>分类阶段：首先通过训练集的特征矩阵、分类结果得到决策树分类器，然后将分类器应用于测试集。然后我们对决策树分类器的准确性进行分析，并对决策树模型进行可视化。</p>
<p>下面，我分别对这些模块进行介绍。</p>
<p>模块 1：数据探索</p>
<p>数据探索这部分虽然对分类器没有实质作用，但是不可忽略。我们只有足够了解这些数据的特性，才能帮助我们做数据清洗、特征选择。</p>
<p>那么如何进行数据探索呢？这里有一些函数你需要了解：</p>
<p>使用 info() 了解数据表的基本情况：行数、列数、每列的数据类型、数据完整度；</p>
<p>使用 describe() 了解数据表的统计情况：总数、平均值、标准差、最小值、最大值等；</p>
<p>使用 describe(include=[‘O’]) 查看字符串类型（非数字）的整体情况；</p>
<p>使用 head 查看前几行数据（默认是前 5 行）；</p>
<p>使用 tail 查看后几行数据（默认是最后 5 行）。</p>
<p>我们可以使用 Pandas 便捷地处理这些问题：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">import pandas as pd
</span></span><span class="line"><span class="cl"># 数据加载
</span></span><span class="line"><span class="cl">train_data = pd.read_csv(&#39;./Titanic_Data/train.csv&#39;)
</span></span><span class="line"><span class="cl">test_data = pd.read_csv(&#39;./Titanic_Data/test.csv&#39;)
</span></span><span class="line"><span class="cl"># 数据探索
</span></span><span class="line"><span class="cl">print(train_data.info())
</span></span><span class="line"><span class="cl">print(&#39;-&#39;*30)
</span></span><span class="line"><span class="cl">print(train_data.describe())
</span></span><span class="line"><span class="cl">print(&#39;-&#39;*30)
</span></span><span class="line"><span class="cl">print(train_data.describe(include=[&#39;O&#39;]))
</span></span><span class="line"><span class="cl">print(&#39;-&#39;*30)
</span></span><span class="line"><span class="cl">print(train_data.head())
</span></span><span class="line"><span class="cl">print(&#39;-&#39;*30)
</span></span><span class="line"><span class="cl">print(train_data.tail())
</span></span></code></pre></td></tr></table>
</div>
</div><p>运行结果：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
</span></span><span class="line"><span class="cl">RangeIndex: 891 entries, 0 to 890
</span></span><span class="line"><span class="cl">Data columns (total 12 columns):
</span></span><span class="line"><span class="cl">PassengerId    891 non-null int64
</span></span><span class="line"><span class="cl">Survived       891 non-null int64
</span></span><span class="line"><span class="cl">Pclass         891 non-null int64
</span></span><span class="line"><span class="cl">Name           891 non-null object
</span></span><span class="line"><span class="cl">Sex            891 non-null object
</span></span><span class="line"><span class="cl">Age            714 non-null float64
</span></span><span class="line"><span class="cl">SibSp          891 non-null int64
</span></span><span class="line"><span class="cl">Parch          891 non-null int64
</span></span><span class="line"><span class="cl">Ticket         891 non-null object
</span></span><span class="line"><span class="cl">Fare           891 non-null float64
</span></span><span class="line"><span class="cl">Cabin          204 non-null object
</span></span><span class="line"><span class="cl">Embarked       889 non-null object
</span></span><span class="line"><span class="cl">dtypes: float64(2), int64(5), object(5)
</span></span><span class="line"><span class="cl">memory usage: 83.6+ KB
</span></span><span class="line"><span class="cl">None
</span></span><span class="line"><span class="cl">------------------------------
</span></span><span class="line"><span class="cl">       PassengerId    Survived     ...           Parch        Fare
</span></span><span class="line"><span class="cl">count   891.000000  891.000000     ...      891.000000  891.000000
</span></span><span class="line"><span class="cl">mean    446.000000    0.383838     ...        0.381594   32.204208
</span></span><span class="line"><span class="cl">std     257.353842    0.486592     ...        0.806057   49.693429
</span></span><span class="line"><span class="cl">min       1.000000    0.000000     ...        0.000000    0.000000
</span></span><span class="line"><span class="cl">25%     223.500000    0.000000     ...        0.000000    7.910400
</span></span><span class="line"><span class="cl">50%     446.000000    0.000000     ...        0.000000   14.454200
</span></span><span class="line"><span class="cl">75%     668.500000    1.000000     ...        0.000000   31.000000
</span></span><span class="line"><span class="cl">max     891.000000    1.000000     ...        6.000000  512.329200
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">[8 rows x 7 columns]
</span></span><span class="line"><span class="cl">------------------------------
</span></span><span class="line"><span class="cl">                                          Name   Sex   ...       Cabin Embarked
</span></span><span class="line"><span class="cl">count                                      891   891   ...         204      889
</span></span><span class="line"><span class="cl">unique                                     891     2   ...         147        3
</span></span><span class="line"><span class="cl">top     Peter, Mrs. Catherine (Catherine Rizk)  male   ...     B96 B98        S
</span></span><span class="line"><span class="cl">freq                                         1   577   ...           4      644
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">[4 rows x 5 columns]
</span></span><span class="line"><span class="cl">------------------------------
</span></span><span class="line"><span class="cl">   PassengerId  Survived  Pclass    ...        Fare Cabin  Embarked
</span></span><span class="line"><span class="cl">0            1         0       3    ...      7.2500   NaN         S
</span></span><span class="line"><span class="cl">1            2         1       1    ...     71.2833   C85         C
</span></span><span class="line"><span class="cl">2            3         1       3    ...      7.9250   NaN         S
</span></span><span class="line"><span class="cl">3            4         1       1    ...     53.1000  C123         S
</span></span><span class="line"><span class="cl">4            5         0       3    ...      8.0500   NaN         S
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">[5 rows x 12 columns]
</span></span><span class="line"><span class="cl">------------------------------
</span></span><span class="line"><span class="cl">     PassengerId  Survived  Pclass    ...      Fare Cabin  Embarked
</span></span><span class="line"><span class="cl">886          887         0       2    ...     13.00   NaN         S
</span></span><span class="line"><span class="cl">887          888         1       1    ...     30.00   B42         S
</span></span><span class="line"><span class="cl">888          889         0       3    ...     23.45   NaN         S
</span></span><span class="line"><span class="cl">889          890         1       1    ...     30.00  C148         C
</span></span><span class="line"><span class="cl">890          891         0       3    ...      7.75   NaN         Q
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">[5 rows x 12 columns]
</span></span></code></pre></td></tr></table>
</div>
</div><p>模块 2：数据清洗</p>
<p>通过数据探索，我们发现 Age、Fare 和 Cabin 这三个字段的数据有所缺失。其中 Age 为年龄字段，是数值型，我们可以通过平均值进行补齐；Fare 为船票价格，是数值型，我们也可以通过其他人购买船票的平均值进行补齐。</p>
<p>具体实现的代码如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"># 使用平均年龄来填充年龄中的 nan 值
</span></span><span class="line"><span class="cl">train_data[&#39;Age&#39;].fillna(train_data[&#39;Age&#39;].mean(), inplace=True)
</span></span><span class="line"><span class="cl">test_data[&#39;Age&#39;].fillna(test_data[&#39;Age&#39;].mean(),inplace=True)
</span></span><span class="line"><span class="cl"># 使用票价的均值填充票价中的 nan 值
</span></span><span class="line"><span class="cl">train_data[&#39;Fare&#39;].fillna(train_data[&#39;Fare&#39;].mean(), inplace=True)
</span></span><span class="line"><span class="cl">test_data[&#39;Fare&#39;].fillna(test_data[&#39;Fare&#39;].mean(),inplace=True)
</span></span></code></pre></td></tr></table>
</div>
</div><p>Cabin 为船舱，有大量的缺失值。在训练集和测试集中的缺失率分别为 77% 和 78%，无法补齐；Embarked 为登陆港口，有少量的缺失值，我们可以把缺失值补齐。</p>
<p>首先观察下 Embarked 字段的取值，方法如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">print(train_data[&#39;Embarked&#39;].value_counts())
</span></span></code></pre></td></tr></table>
</div>
</div><p>结果如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">S    644
</span></span><span class="line"><span class="cl">C    168
</span></span><span class="line"><span class="cl">Q     77
</span></span></code></pre></td></tr></table>
</div>
</div><p>我们发现一共就 3 个登陆港口，其中 S 港口人数最多，占到了 72%，因此我们将其余缺失的 Embarked 数值均设置为 S：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"># 使用登录最多的港口来填充登录港口的 nan 值
</span></span><span class="line"><span class="cl">train_data[&#39;Embarked&#39;].fillna(&#39;S&#39;, inplace=True)
</span></span><span class="line"><span class="cl">test_data[&#39;Embarked&#39;].fillna(&#39;S&#39;,inplace=True)
</span></span></code></pre></td></tr></table>
</div>
</div><p>模块 3：特征选择</p>
<p>特征选择是分类器的关键。特征选择不同，得到的分类器也不同。那么我们该选择哪些特征做生存的预测呢？</p>
<p>通过数据探索我们发现，PassengerId 为乘客编号，对分类没有作用，可以放弃；Name 为乘客姓名，对分类没有作用，可以放弃；Cabin 字段缺失值太多，可以放弃；Ticket 字段为船票号码，杂乱无章且无规律，可以放弃。其余的字段包括：Pclass、Sex、Age、SibSp、Parch 和 Fare，这些属性分别表示了乘客的船票等级、性别、年龄、亲戚数量以及船票价格，可能会和乘客的生存预测分类有关系。具体是什么关系，我们可以交给分类器来处理。</p>
<p>因此我们先将 Pclass、Sex、Age 等这些其余的字段作特征，放到特征向量 features 里。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"># 特征选择
</span></span><span class="line"><span class="cl">features = [&#39;Pclass&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;, &#39;Embarked&#39;]
</span></span><span class="line"><span class="cl">train_features = train_data[features]
</span></span><span class="line"><span class="cl">train_labels = train_data[&#39;Survived&#39;]
</span></span><span class="line"><span class="cl">test_features = test_data[features]
</span></span></code></pre></td></tr></table>
</div>
</div><p>特征值里有一些是字符串，这样不方便后续的运算，需要转成数值类型，比如 Sex 字段，有 male 和 female 两种取值。我们可以把它变成 Sex=male 和 Sex=female 两个字段，数值用 0 或 1 来表示。</p>
<p>同理 Embarked 有 S、C、Q 三种可能，我们也可以改成 Embarked=S、Embarked=C 和 Embarked=Q 三个字段，数值用 0 或 1 来表示。</p>
<p>那该如何操作呢，我们可以使用 sklearn 特征选择中的 DictVectorizer 类，用它将可以处理符号化的对象，将符号转成数字 0/1 进行表示。具体方法如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">from sklearn.feature_extraction import DictVectorizer
</span></span><span class="line"><span class="cl">dvec=DictVectorizer(sparse=False)
</span></span><span class="line"><span class="cl">train_features=dvec.fit_transform(train_features.to_dict(orient=&#39;record&#39;))
</span></span></code></pre></td></tr></table>
</div>
</div><p>你会看到代码中使用了 fit_transform 这个函数，它可以将特征向量转化为特征值矩阵。然后我们看下 dvec 在转化后的特征属性是怎样的，即查看 dvec 的 feature_names_ 属性值，方法如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">print(dvec.feature_names_)
</span></span></code></pre></td></tr></table>
</div>
</div><p>运行结果：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">[&#39;Age&#39;, &#39;Embarked=C&#39;, &#39;Embarked=Q&#39;, &#39;Embarked=S&#39;, &#39;Fare&#39;, &#39;Parch&#39;, &#39;Pclass&#39;, &#39;Sex=female&#39;, &#39;Sex=male&#39;, &#39;SibSp&#39;]
</span></span></code></pre></td></tr></table>
</div>
</div><p>你可以看到原本是一列的 Embarked，变成了“Embarked=C”“Embarked=Q”“Embarked=S”三列。Sex 列变成了“Sex=female”“Sex=male”两列。</p>
<p>这样 train_features 特征矩阵就包括 10 个特征值（列），以及 891 个样本（行），即 891 行，10 列的特征矩阵。</p>
<p>模块 4：决策树模型</p>
<p>刚才我们已经讲了如何使用 sklearn 中的决策树模型。现在我们使用 ID3 算法，即在创建 DecisionTreeClassifier 时，设置 criterion=‘entropy’，然后使用 fit 进行训练，将特征值矩阵和分类标识结果作为参数传入，得到决策树分类器。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">from sklearn.tree import DecisionTreeClassifier
</span></span><span class="line"><span class="cl"># 构造 ID3 决策树
</span></span><span class="line"><span class="cl">clf = DecisionTreeClassifier(criterion=&#39;entropy&#39;)
</span></span><span class="line"><span class="cl"># 决策树训练
</span></span><span class="line"><span class="cl">clf.fit(train_features, train_labels)
</span></span></code></pre></td></tr></table>
</div>
</div><p>模块 5：模型预测 &amp; 评估</p>
<p>在预测中，我们首先需要得到测试集的特征值矩阵，然后使用训练好的决策树 clf 进行预测，得到预测结果 pred_labels：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">test_features=dvec.transform(test_features.to_dict(orient=&#39;record&#39;))
</span></span><span class="line"><span class="cl"># 决策树预测
</span></span><span class="line"><span class="cl">pred_labels = clf.predict(test_features)
</span></span></code></pre></td></tr></table>
</div>
</div><p>在模型评估中，决策树提供了 score 函数可以直接得到准确率，但是我们并不知道真实的预测结果，所以无法用预测值和真实的预测结果做比较。我们只能使用训练集中的数据进行模型评估，可以使用决策树自带的 score 函数计算下得到的结果：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"># 得到决策树准确率
</span></span><span class="line"><span class="cl">acc_decision_tree = round(clf.score(train_features, train_labels), 6)
</span></span><span class="line"><span class="cl">print(u&#39;score 准确率为 %.4lf&#39; % acc_decision_tree)
</span></span></code></pre></td></tr></table>
</div>
</div><p>运行结果：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">score 准确率为 0.9820
</span></span></code></pre></td></tr></table>
</div>
</div><p>你会发现你刚用训练集做训练，再用训练集自身做准确率评估自然会很高。但这样得出的准确率并不能代表决策树分类器的准确率。</p>
<p>这是为什么呢？</p>
<p>因为我们没有测试集的实际结果，因此无法用测试集的预测结果与实际结果做对比。如果我们使用 score 函数对训练集的准确率进行统计，正确率会接近于 100%（如上结果为 98.2%），无法对分类器的在实际环境下做准确率的评估。</p>
<p>那么有什么办法，来统计决策树分类器的准确率呢？</p>
<p>这里可以使用 K 折交叉验证的方式，交叉验证是一种常用的验证分类准确率的方法，原理是拿出大部分样本进行训练，少量的用于分类器的验证。K 折交叉验证，就是做 K 次交叉验证，每次选取 K 分之一的数据作为验证，其余作为训练。轮流 K 次，取平均值。</p>
<p>K 折交叉验证的原理是这样的：</p>
<p>将数据集平均分割成 K 个等份；</p>
<p>使用 1 份数据作为测试数据，其余作为训练数据；</p>
<p>计算测试准确率；</p>
<p>使用不同的测试集，重复 2、3 步骤。</p>
<p>在 sklearn 的 model_selection 模型选择中提供了 cross_val_score 函数。cross_val_score 函数中的参数 cv 代表对原始数据划分成多少份，也就是我们的 K 值，一般建议 K 值取 10，因此我们可以设置 CV=10，我们可以对比下 score 和 cross_val_score 两种函数的正确率的评估结果：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">import numpy as np
</span></span><span class="line"><span class="cl">from sklearn.model_selection import cross_val_score
</span></span><span class="line"><span class="cl"># 使用 K 折交叉验证 统计决策树准确率
</span></span><span class="line"><span class="cl">print(u&#39;cross_val_score 准确率为 %.4lf&#39; % np.mean(cross_val_score(clf, train_features, train_labels, cv=10)))
</span></span></code></pre></td></tr></table>
</div>
</div><p>运行结果：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">cross_val_score 准确率为 0.7835
</span></span></code></pre></td></tr></table>
</div>
</div><p>你可以看到，score 函数的准确率为 0.9820，cross_val_score 准确率为 0.7835。</p>
<p>这里很明显，对于不知道测试集实际结果的，要使用 K 折交叉验证才能知道模型的准确率。</p>
<p>模块 6：决策树可视化</p>
<p>sklearn 的决策树模型对我们来说，还是比较抽象的。我们可以使用 Graphviz 可视化工具帮我们把决策树呈现出来。</p>
<p>安装 Graphviz 库需要下面的几步：</p>
<p>安装 graphviz 工具，这里是它的下载地址；http://www.graphviz.org/download/</p>
<p>将 Graphviz 添加到环境变量 PATH 中；</p>
<p>需要 Graphviz 库，如果没有可以使用 pip install graphviz 进行安装。</p>
<p>这样你就可以在程序里面使用 Graphviz 对决策树模型进行呈现，最后得到一个决策树可视化的 PDF 文件，可视化结果文件 Source.gv.pdf 你可以在 GitHub 上下载：https://github.com/cystanford/Titanic_Data</p>
<p>决策树模型使用技巧总结</p>
<p>今天我用泰坦尼克乘客生存预测案例把决策树模型的流程跑了一遍。在实战中，你需要注意一下几点：</p>
<p>特征选择是分类模型好坏的关键。选择什么样的特征，以及对应的特征值矩阵，决定了分类模型的好坏。通常情况下，特征值不都是数值类型，可以使用 DictVectorizer 类进行转化；</p>
<p>模型准确率需要考虑是否有测试集的实际结果可以做对比，当测试集没有真实结果可以对比时，需要使用 K 折交叉验证 cross_val_score；</p>
<p>Graphviz 可视化工具可以很方便地将决策模型呈现出来，帮助你更好理解决策树的构建。</p>
<p>我上面讲了泰坦尼克乘客生存预测的六个关键模块，请你用 sklearn 中的决策树模型独立完成这个项目，对测试集中的乘客是否生存进行预测，并给出模型准确率评估。数据从 GitHub 上下载即可。</p>
<p>最后给你留一个思考题吧，我在构造特征向量时使用了 DictVectorizer 类，使用 fit_transform 函数将特征向量转化为特征值矩阵。DictVectorizer 类同时也提供 transform 函数，那么这两个函数有什么区别？</p>
<p>欢迎你在评论区留言与我分享你的答案，也欢迎点击“请朋友读”把这篇文章分享给你的朋友或者同事，一起交流一下。</p>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content"></span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
        10100-01-10
        
    </span>
  </p>
  
  
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2/">数据分析实战45讲</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/sql%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A/2019%E4%B8%A8%E5%9F%BA%E7%A1%80%E7%AF%87%E6%80%BB%E7%BB%93%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96%E9%80%9A%E9%85%8D%E7%AC%A6%E4%BB%A5%E5%8F%8A%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">2019丨基础篇总结如何理解查询优化通配符以及存储过程</span>
            <span class="prev-text nav-mobile">上一篇</span>
          </a>
        <a class="next" href="/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E5%A6%82%E4%BD%95%E6%88%90%E4%B8%BA%E5%AD%A6%E4%B9%A0%E9%AB%98%E6%89%8B/2020%E5%A6%82%E4%BD%95%E5%81%9A%E5%88%B0%E6%AF%8F%E5%A4%A9%E9%AB%98%E6%95%88%E5%AD%A6%E4%B9%A012%E5%B0%8F%E6%97%B6/">
            <span class="next-text nav-default">2020｜如何做到每天高效学习12小时</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
  
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2024 - 
    2025<span class="heart"><i class="iconfont icon-heart"></i></span><span></span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  <script src="/lib/highlight/highlight.pack.js?v=20171001"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js"></script>


      <script async src="https://www.googletagmanager.com/gtag/js?id=G-FVZ07KBD4X"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-FVZ07KBD4X');
        }
      </script>






</body>
</html>
