<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>20_流处理案例实战：分析纽约市出租车载客信息 - Docs</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="" /><meta name="description" content="你好，我是蔡元楠。
今天我要与你分享的主题是“流处理案例实战：分析纽约市出租车载客信息”。
在上一讲中，我们结合加州房屋信息的真实数据集，构建了一个基本的预测房价的线性回归模型。通过这个实例，我们不仅学习了处理大数据问题的基本流程，而且还进一步熟练了对 RDD 和 DataFrame API 的使用。
" /><meta name="keywords" content="技术文档, docs, 极客时间" />






<meta name="generator" content="Hugo 0.140.2 with theme even" />


<link rel="canonical" href="https://politcloud.org/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E6%88%98/20_%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%88%98%E5%88%86%E6%9E%90%E7%BA%BD%E7%BA%A6%E5%B8%82%E5%87%BA%E7%A7%9F%E8%BD%A6%E8%BD%BD%E5%AE%A2%E4%BF%A1%E6%81%AF/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:url" content="https://politcloud.org/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E6%88%98/20_%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%88%98%E5%88%86%E6%9E%90%E7%BA%BD%E7%BA%A6%E5%B8%82%E5%87%BA%E7%A7%9F%E8%BD%A6%E8%BD%BD%E5%AE%A2%E4%BF%A1%E6%81%AF/">
  <meta property="og:site_name" content="Docs">
  <meta property="og:title" content="20_流处理案例实战：分析纽约市出租车载客信息">
  <meta property="og:description" content="你好，我是蔡元楠。
今天我要与你分享的主题是“流处理案例实战：分析纽约市出租车载客信息”。
在上一讲中，我们结合加州房屋信息的真实数据集，构建了一个基本的预测房价的线性回归模型。通过这个实例，我们不仅学习了处理大数据问题的基本流程，而且还进一步熟练了对 RDD 和 DataFrame API 的使用。">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2024-01-10T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-01-10T00:00:00+00:00">
    <meta property="article:tag" content="大规模数据处理实战">

  <meta itemprop="name" content="20_流处理案例实战：分析纽约市出租车载客信息">
  <meta itemprop="description" content="你好，我是蔡元楠。
今天我要与你分享的主题是“流处理案例实战：分析纽约市出租车载客信息”。
在上一讲中，我们结合加州房屋信息的真实数据集，构建了一个基本的预测房价的线性回归模型。通过这个实例，我们不仅学习了处理大数据问题的基本流程，而且还进一步熟练了对 RDD 和 DataFrame API 的使用。">
  <meta itemprop="datePublished" content="2024-01-10T00:00:00+00:00">
  <meta itemprop="dateModified" content="2024-01-10T00:00:00+00:00">
  <meta itemprop="wordCount" content="4040">
  <meta itemprop="keywords" content="大规模数据处理实战">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="20_流处理案例实战：分析纽约市出租车载客信息">
  <meta name="twitter:description" content="你好，我是蔡元楠。
今天我要与你分享的主题是“流处理案例实战：分析纽约市出租车载客信息”。
在上一讲中，我们结合加州房屋信息的真实数据集，构建了一个基本的预测房价的线性回归模型。通过这个实例，我们不仅学习了处理大数据问题的基本流程，而且还进一步熟练了对 RDD 和 DataFrame API 的使用。">

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Docs</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Docs</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">20_流处理案例实战：分析纽约市出租车载客信息</h1>

      <div class="post-meta">
        <span class="post-time"> 10100-01-10 </span>
        <div class="post-category">
            <a href="/categories/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/"> 极客时间 </a>
            </div>
          <span class="more-meta"> 约 4040 字 </span>
          <span class="more-meta"> 预计阅读 9 分钟 </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#数据集介绍">数据集介绍</a></li>
        <li><a href="#流数据输入">流数据输入</a></li>
        <li><a href="#数据清洗">数据清洗</a></li>
        <li><a href="#stream-stream-join">Stream-stream Join</a></li>
        <li><a href="#计算结果并输出">计算结果并输出</a></li>
        <li><a href="#小结">小结</a></li>
        <li><a href="#思考题">思考题</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p>你好，我是蔡元楠。</p>
<p>今天我要与你分享的主题是“流处理案例实战：分析纽约市出租车载客信息”。</p>
<p>在上一讲中，我们结合加州房屋信息的真实数据集，构建了一个基本的预测房价的线性回归模型。通过这个实例，我们不仅学习了处理大数据问题的基本流程，而且还进一步熟练了对 RDD 和 DataFrame API 的使用。</p>
<p>你应该已经发现，上一讲的实例是一个典型的批处理问题，因为处理的数据是静态而有边界的。今天让我们来一起通过实例，更加深入地学习用 Spark 去解决实际的流处理问题。</p>
<p>相信你还记得，在前面的章节中我们介绍过 Spark 两个用于流处理的组件——Spark Streaming 和 Structured Streaming。其中 Spark Streaming 是 Spark 2.0 版本前的的流处理库，在 Spark 2.0 之后，集成了 DataFrame/DataSet API 的 Structured Streaming 成为 Spark 流处理的主力。</p>
<p>今天就让我们一起用 Structured Streaming 对纽约市出租车的载客信息进行处理，建立一个实时流处理的 pipeline，实时输出各个区域内乘客小费的平均数来帮助司机决定要去哪里接单。</p>
<h2 id="数据集介绍">数据集介绍</h2>
<p>今天的数据集是纽约市 2009～2015 年出租车载客的信息。每一次出行包含了两个事件，一个事件代表出发，另一个事件代表到达。每个事件都有 11 个属性，它的 schema 如下所示：</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E6%88%98/895f2166ef86e528fde002d1650c516b.png" alt=""></p>
<p>这部分数据有个不太直观的地方，那就是同一次出行会有两个记录，而且代表出发的事件没有任何意义，因为到达事件已经涵盖了所有必要的信息。现实世界中的数据都是这样复杂，不可能像学校的测试数据一样简单直观，所以处理之前，我们要先对数据进行清洗，只留下必要的信息。</p>
<p>这个数据还包含有另外一部分信息，就是所有出租车的付费信息，它有 8 个属性，schema 如下所示。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E6%88%98/68c0db754093a5dc4a6a58a56bc754f1.png" alt=""></p>
<p>这个数据集可以从<a href="./taxiData.md">网上</a>下载到，数据集的规模在 100MB 左右，它只是节选了一部分出租车的载客信息，所以在本机运行就可以了。详细的纽约出租车数据集超过了 500GB，同样在<a href="./tlc-trip-record-data.page.md">网上</a>可以下载，感兴趣的同学可以下载来实践一下。</p>
<h2 id="流数据输入">流数据输入</h2>
<p>你可能要问，这个数据同样是静态、有边界的，为什么要用流处理？</p>
<p>因为我们手里没有实时更新的流数据源。我也没有权限去公开世界上任何一个上线产品的数据流。所以，这里只能将有限的数据经过 Kafka 处理，输出为一个伪流数据，作为我们要构建的 pipeline 的输入。</p>
<p>在模块二中，我们曾经初步了解过 Apache Kafka，知道它是基于 Pub/Sub 模式的流数据处理平台。由于我们的专栏并不涉及 Apache Kafka 的具体内容，所以我在这里就不讲如何把这个数据输入到 Kafka 并输出的细节了。你只要知道，在这个例子中，Consumer 是之后要写的 Spark 流处理程序，这个消息队列有两个 Topic，一个包含出行的地理位置信息，一个包含出行的收费信息。Kafka 会<strong>按照时间顺序</strong>，向这两个 Topic 中发布事件，从而模拟一个实时的流数据源。</p>
<p>相信你还记得，写 Spark 程序的第一步就是创建 SparkSession 对象，并根据输入数据创建对应的 RDD 或者 DataFrame。你可以看下面的代码。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-gdscript3" data-lang="gdscript3"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">from</span> <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span> <span class="n">import</span> <span class="n">SparkSession</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> <span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   <span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&#34;Spark Structured Streaming for taxi ride info&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> <span class="n">rides</span> <span class="o">=</span> <span class="n">spark</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   <span class="o">.</span><span class="n">readStream</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&#34;kafka&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&#34;kafka.bootstrap.servers&#34;</span><span class="p">,</span> <span class="s2">&#34;localhost:xxxx&#34;</span><span class="p">)</span> <span class="o">//</span> <span class="err">取决于</span> <span class="n">Kafka</span> <span class="err">的配置</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&#34;subscribe&#34;</span><span class="p">,</span> <span class="s2">&#34;taxirides&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&#34;startingOffsets&#34;</span><span class="p">,</span> <span class="s2">&#34;latest&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   <span class="o">.</span><span class="n">load</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   <span class="o">.</span><span class="n">selectExpr</span><span class="p">(</span><span class="s2">&#34;CAST(value AS STRING)&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> <span class="n">fares</span> <span class="o">=</span> <span class="n">spark</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   <span class="o">.</span><span class="n">readStream</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&#34;kafka&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&#34;kafka.bootstrap.servers&#34;</span><span class="p">,</span> <span class="s2">&#34;localhost:xxxx&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&#34;subscribe&#34;</span><span class="p">,</span> <span class="s2">&#34;taxifares&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   <span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&#34;startingOffsets&#34;</span><span class="p">,</span> <span class="s2">&#34;latest&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   <span class="o">.</span><span class="n">load</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   <span class="o">.</span><span class="n">selectExpr</span><span class="p">(</span><span class="s2">&#34;CAST(value AS STRING)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>在这段代码里，我们创建了两个 Streaming DataFrame，并订阅了对应的 Kafka topic，一个代表出行位置信息，另一个代表收费信息。Kafka 对数据没有做任何修改，所以流中的每一个数据都是一个长 String，属性之间是用逗号分割的。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">417986,END,2013-01-02 00:43:52,2013-01-02  00:39:56,-73.984528,40.745377,-73.975967,40.765533,1,2013007646,2013007642
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="数据清洗">数据清洗</h2>
<p>现在，我们要开始做数据清洗了。要想分离出我们需要的位置和付费信息，我们首先要把数据分割成一个个属性，并创建对应的 DataFrame 中的列。为此，我们首先要根据数据类型创建对应的 schema。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">ridesSchema = StructType([
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   StructField(&#34;rideId&#34;, LongType()), StructField(&#34;isStart&#34;, StringType()),
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   StructField(&#34;endTime&#34;, TimestampType()), StructField(&#34;startTime&#34;, TimestampType()),
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   StructField(&#34;startLon&#34;, FloatType()), StructField(&#34;startLat&#34;, FloatType()),
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   StructField(&#34;endLon&#34;, FloatType()), StructField(&#34;endLat&#34;, FloatType()),
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   StructField(&#34;passengerCnt&#34;, ShortType()), StructField(&#34;taxiId&#34;, LongType()),
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   StructField(&#34;driverId&#34;, LongType())])
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> faresSchema = StructType([
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   StructField(&#34;rideId&#34;, LongType()), StructField(&#34;taxiId&#34;, LongType()),
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   StructField(&#34;driverId&#34;, LongType()), StructField(&#34;startTime&#34;, TimestampType()),
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   StructField(&#34;paymentType&#34;, StringType()), StructField(&#34;tip&#34;, FloatType()),
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   StructField(&#34;tolls&#34;, FloatType()), StructField(&#34;totalFare&#34;, FloatType())])
</span></span></code></pre></td></tr></table>
</div>
</div><p>接下来，我们将每个数据都用逗号分割，并加入相应的列。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-gdscript3" data-lang="gdscript3"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">parse_data_from_kafka_message</span><span class="p">(</span><span class="n">sdf</span><span class="p">,</span> <span class="n">schema</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   <span class="n">from</span> <span class="n">pyspark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">functions</span> <span class="n">import</span> <span class="n">split</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   <span class="nb">assert</span> <span class="n">sdf</span><span class="o">.</span><span class="n">isStreaming</span> <span class="o">==</span> <span class="n">True</span><span class="p">,</span> <span class="s2">&#34;DataFrame doesn&#39;t receive streaming data&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   <span class="n">col</span> <span class="o">=</span> <span class="n">split</span><span class="p">(</span><span class="n">sdf</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">],</span> <span class="s1">&#39;,&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">field</span> <span class="ow">in</span> <span class="n">enumerate</span><span class="p">(</span><span class="n">schema</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">       <span class="n">sdf</span> <span class="o">=</span> <span class="n">sdf</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="n">field</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">col</span><span class="o">.</span><span class="n">getItem</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">field</span><span class="o">.</span><span class="n">dataType</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   <span class="k">return</span> <span class="n">sdf</span><span class="o">.</span><span class="n">select</span><span class="p">([</span><span class="n">field</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">field</span> <span class="ow">in</span> <span class="n">schema</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> <span class="n">rides</span> <span class="o">=</span> <span class="n">parse_data_from_kafka_message</span><span class="p">(</span><span class="n">rides</span><span class="p">,</span> <span class="n">ridesSchema</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">fares</span> <span class="o">=</span> <span class="n">parse_data_from_kafka_message</span><span class="p">(</span><span class="n">fares</span><span class="p">,</span> <span class="n">faresSchema</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>在上面的代码中，我们定义了函数 parse_data_from_kafka_message，用来把 Kafka 发来的 message 根据 schema 拆成对应的属性，转换类型，并加入到 DataFrame 的表中。</p>
<p>正如我们之前提到的，读入的数据包含了一些无用信息。</p>
<p>首先，所有代表出发的事件都已被删除，因为到达事件已经包含了出发事件的所有信息，而且只有到达之后才会付费。</p>
<p>其次，出发地点和目的地在纽约范围外的数据，也可以被删除。因为我们的目标是找出纽约市内小费较高的地点。DataFrame 的 filter 函数可以很容易地做到这些。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">MIN_LON, MAX_LON, MIN_LAT, MAX_LAT = -73.7, -74.05, 41.0, 40.5
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">rides = rides.filter(
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   rides[&#34;startLon&#34;].between(MIN_LON, MAX_LON) &amp;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   rides[&#34;startLat&#34;].between(MIN_LAT, MAX_LAT) &amp;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   rides[&#34;endLon&#34;].between(MIN_LON, MAX_LON) &amp;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   rides[&#34;endLat&#34;].between(MIN_LAT, MAX_LAT))
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">rides = rides.filter(rides[&#34;isStart&#34;] == &#34;END&#34;)
</span></span></code></pre></td></tr></table>
</div>
</div><p>上面的代码中首先定义了纽约市的经纬度范围，然后把所有起点和终点在这个范围之外的数据都过滤掉了。最后，把所有代表出发事件的数据也移除掉。</p>
<p>当然，除了前面提到的清洗方案，可能还会有别的可以改进的地方，比如把不重要的信息去掉，例如乘客数量、过路费等，你可以自己思考一下。</p>
<h2 id="stream-stream-join">Stream-stream Join</h2>
<p>我们的目标是找出小费较高的地理区域，而现在收费信息和地理位置信息还在两个 DataFrame 中，无法放在一起分析。那么要用怎样的方式把它们联合起来呢？</p>
<p>你应该还记得，DataFrame 本质上是把数据当成一张关系型的表。在我们这个例子中，rides 所对应的表的键值（Key）是 rideId，其他列里我们关心的就是起点和终点的位置；fares 所对应的表键值也是 rideId，其他列里我们关心的就是小费信息（tips）。</p>
<p>说到这里，你可能会自然而然地想到，如果可以像关系型数据表一样，根据共同的键值 rideId 把两个表 inner join 起来，就可以同时分析这两部分信息了。但是这里的 DataFrame 其实是两个数据流，Spark 可以把两个流 Join 起来吗？</p>
<p>答案是肯定的。在 Spark 2.3 中，流与流的 Join（Stream-stream join）被正式支持。这样的 Join 难点就在于，在任意一个时刻，流数据都不是完整的，流 A 中后面还没到的数据有可能要和流 B 中已经有的数据 Join 起来再输出。为了解决这个问题，我们就要引入<strong>数据水印</strong>（Watermark）的概念。</p>
<p>数据水印定义了我们可以对数据延迟的最大容忍限度。</p>
<p>比如说，如果定义水印是 10 分钟，数据 A 的事件时间是 1:00，数据 B 的事件时间是 1:10，由于数据传输发生了延迟，我们在 1:15 才收到了 A 和 B，那么我们将只处理数据 B 并更新结果，A 会被无视。在 Join 操作中，好好利用水印，我们就知道什么时候可以不用再考虑旧数据，什么时候必须把旧数据保留在内存中。不然，我们就必须把所有旧数据一直存在内存里，导致数据不断增大，最终可能会内存泄漏。</p>
<p>在这个例子中，为什么我们做这样的 Join 操作需要水印呢？</p>
<p>这是因为两个数据流并不保证会同时收到同一次出行的数据，因为收费系统需要额外的时间去处理，而且这两个数据流是独立的，每个都有可能产生数据延迟。所以要对时间加水印，以免出现内存中数据无限增长的情况。</p>
<p>那么下一个问题就是，究竟要对哪个时间加水印，出发时间还是到达时间？</p>
<p>前面说过了，我们其实只关心到达时间，所以对 rides 而言，我们只需要对到达时间加水印。但是，在 fares 这个 DataFrame 里并没有到达时间的任何信息，所以我们没法选择，只能对出发时间加水印。因此，我们还需要额外定义一个时间间隔的限制，出发时间和到达时间的间隔要在一定的范围内。具体内容你可以看下面的代码。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">faresWithWatermark = fares
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   .selectExpr(&#34;rideId AS rideId_fares&#34;, &#34;startTime&#34;, &#34;totalFare&#34;, &#34;tip&#34;)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   .withWatermark(&#34;startTime&#34;, &#34;30 minutes&#34;)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> ridesWithWatermark = rides
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> .selectExpr(&#34;rideId&#34;, &#34;endTime&#34;, &#34;driverId&#34;, &#34;taxiId&#34;, &#34;startLon&#34;, &#34;startLat&#34;, &#34;endLon&#34;, &#34;endLat&#34;)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> .withWatermark(&#34;endTime&#34;, &#34;30 minutes&#34;)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> joinDF = faresWithWatermark
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   .join(ridesWithWatermark,
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">     expr(&#34;&#34;&#34;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      rideId_fares = rideId AND
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">       endTime &gt; startTime AND
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">       endTime &lt;= startTime + interval 2 hours
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">       &#34;&#34;&#34;)
</span></span></code></pre></td></tr></table>
</div>
</div><p>在这段代码中，我们对 fares 和 rides 分别加了半小时的水印，然后把两个 DataFrame 根据 rideId 和时间间隔的限制 Join 起来。这样，joinDF 就同时包含了地理位置和付费信息。</p>
<p>接下来，就让我们开始计算实时的小费最高区域。</p>
<h2 id="计算结果并输出">计算结果并输出</h2>
<p>到现在为止，我们还没有处理地点信息。原生的经纬度信息显然并没有很大用处。我们需要做的是把纽约市分割成几个区域，把数据中所有地点的经纬度信息转化成区域信息，这样司机们才可以知道大概哪个地区的乘客比较可能给高点的小费。</p>
<p>纽约市的区域信息以及坐标可以从网上找到，这部分处理比较容易。每个接收到的数据我们都可以判定它在哪个区域内，然后对 joinDF 增加一个列“area”来代表终点的区域。现在，让我们假设 area 已经加到现有的 DataFrame 里。接下来我们需要把得到的信息告诉司机了。</p>
<p>还记得第 16 讲和第 17 讲中提到的滑动窗口操作吗？这是流处理中常见的输出形式，即输出每隔一段时间内，特定时间窗口的特征值。在这个例子中，我们可以每隔 10 分钟，输出过去半小时内每个区域内的平均小费。这样的话，司机可以每隔 10 分钟查看一下数据，决定下一步去哪里接单。这个查询（Query）可以由以下代码产生。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">tips = joinDF
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   .groupBy(
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">       window(&#34;endTime&#34;, &#34;30 minutes&#34;, &#34;10 minutes&#34;),
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">       &#34;area&#34;)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   .agg(avg(&#34;tip&#34;))
</span></span></code></pre></td></tr></table>
</div>
</div><p>最后，我们把 tips 这个流式 DataFrame 输出。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">query.writeStream
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   .outputMode(&#34;append&#34;)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   .format(&#34;console&#34;)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   .option(&#34;truncate&#34;, False
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   .start()
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">   .awaitTermination()
</span></span></code></pre></td></tr></table>
</div>
</div><p>你可能会问，为什么我们不可以把输出结果按小费多少进行排序呢？</p>
<p>这是因为两个流的 inner-join 只支持附加输出模式（Append Mode），而现在 Structured Streaming 不支持在附加模式下进行排序操作。希望将来 Structured Streaming 可以提供这个功能，但是现在，司机们只能扫一眼所有的输出数据来大概判断哪个地方的小费最高了。</p>
<h2 id="小结">小结</h2>
<p>流处理和批处理都是非常常见的应用场景，而且相较而言流处理更加复杂，对延迟性要求更高。今天我们再次通过一个实例帮助你了解要如何利用 Structured Streaming 对真实数据集进行流处理。Spark 最大的好处之一就是它拥有统一的批流处理框架和 API，希望你在课下要进一步加深对 DataSet/DataFrame 的熟练程度。</p>
<h2 id="思考题">思考题</h2>
<p>今天的主题是“案例实战”，不过我留的是思考题，而不是实践题。因为我不确定你是否会使用 Kafka。如果你的工作中会接触到流数据，那么你可以参考今天这个案例的思路和步骤来解决问题，多加练习以便熟悉 Spark 的使用。如果你还没有接触过流数据，但却想往这方面发展的话，我就真的建议你去学习一下 Kafka，这是个能帮助我们更好地做流处理应用开发和部署的利器。</p>
<p>现在，来说一下今天的思考题吧。</p>
<ol>
<li>为什么流的 Inner-Join 不支持完全输出模式？</li>
<li>对于 Inner-Join 而言，加水印是否是必须的？Outer-Join 呢？</li>
</ol>
<p>欢迎你把答案写在留言区，与我和其他同学一起讨论。</p>
<p>如果你觉得有所收获，也欢迎把文章分享给你的朋友。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E6%88%98/a0f5b0b51dbfc98740637837dc0ae117.png" alt="unpreview"></p>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content"></span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
        10100-01-10
        
    </span>
  </p>
  
  
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E6%88%98/">大规模数据处理实战</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E6%89%93%E9%80%A0%E7%88%86%E6%AC%BE%E7%9F%AD%E8%A7%86%E9%A2%91/20_%E6%8B%89%E7%89%87%E6%B3%95%E7%94%A8%E7%94%B5%E5%BD%B1%E5%88%9B%E4%BD%9C%E7%9A%84%E6%96%B9%E5%BC%8F%E5%AD%A6%E4%B9%A0%E7%88%86%E6%AC%BE%E7%9F%AD%E8%A7%86%E9%A2%91%E8%BF%90%E8%90%A5%E6%8A%80%E5%B7%A7/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">20_拉片法：用电影创作的方式学习爆款短视频运营技巧</span>
            <span class="prev-text nav-mobile">上一篇</span>
          </a>
        <a class="next" href="/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E6%88%90%E4%B8%BAai%E4%BA%A7%E5%93%81%E7%BB%8F%E7%90%86/20_%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0%E4%BA%8C%E4%BB%8E%E4%BF%A1%E7%94%A8%E8%AF%84%E5%88%86%E4%BA%A7%E5%93%81%E7%9C%8B%E4%BB%80%E4%B9%88%E6%98%AFksauc/">
            <span class="next-text nav-default">20_模型性能评估（二）：从信用评分产品看什么是KS、AUC？</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
  
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2024 - 
    2025<span class="heart"><i class="iconfont icon-heart"></i></span><span></span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  <script src="/lib/highlight/highlight.pack.js?v=20171001"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js"></script>


      <script async src="https://www.googletagmanager.com/gtag/js?id=G-FVZ07KBD4X"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-FVZ07KBD4X');
        }
      </script>






</body>
</html>
