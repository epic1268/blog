<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>15_Spark_SQL：Spark数据查询的利器 - Docs</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="" /><meta name="description" content="你好，我是蔡元楠。
上一讲中，我介绍了弹性分布式数据集的特性和它支持的各种数据操作。
不过在实际的开发过程中，我们并不是总需要在 RDD 的层次进行编程。
就好比编程刚发明的年代，工程师只能用汇编语言，到后来才慢慢发展出高级语言，如 Basic、C、Java 等。使用高级语言大大提升了开发者的效率。
" /><meta name="keywords" content="技术文档, docs, 极客时间" />






<meta name="generator" content="Hugo 0.140.2 with theme even" />


<link rel="canonical" href="https://politcloud.org/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E6%88%98/15_spark_sqlspark%E6%95%B0%E6%8D%AE%E6%9F%A5%E8%AF%A2%E7%9A%84%E5%88%A9%E5%99%A8/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:url" content="https://politcloud.org/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E6%88%98/15_spark_sqlspark%E6%95%B0%E6%8D%AE%E6%9F%A5%E8%AF%A2%E7%9A%84%E5%88%A9%E5%99%A8/">
  <meta property="og:site_name" content="Docs">
  <meta property="og:title" content="15_Spark_SQL：Spark数据查询的利器">
  <meta property="og:description" content="你好，我是蔡元楠。
上一讲中，我介绍了弹性分布式数据集的特性和它支持的各种数据操作。
不过在实际的开发过程中，我们并不是总需要在 RDD 的层次进行编程。
就好比编程刚发明的年代，工程师只能用汇编语言，到后来才慢慢发展出高级语言，如 Basic、C、Java 等。使用高级语言大大提升了开发者的效率。">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2024-01-10T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-01-10T00:00:00+00:00">
    <meta property="article:tag" content="大规模数据处理实战">

  <meta itemprop="name" content="15_Spark_SQL：Spark数据查询的利器">
  <meta itemprop="description" content="你好，我是蔡元楠。
上一讲中，我介绍了弹性分布式数据集的特性和它支持的各种数据操作。
不过在实际的开发过程中，我们并不是总需要在 RDD 的层次进行编程。
就好比编程刚发明的年代，工程师只能用汇编语言，到后来才慢慢发展出高级语言，如 Basic、C、Java 等。使用高级语言大大提升了开发者的效率。">
  <meta itemprop="datePublished" content="2024-01-10T00:00:00+00:00">
  <meta itemprop="dateModified" content="2024-01-10T00:00:00+00:00">
  <meta itemprop="wordCount" content="3099">
  <meta itemprop="keywords" content="大规模数据处理实战">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="15_Spark_SQL：Spark数据查询的利器">
  <meta name="twitter:description" content="你好，我是蔡元楠。
上一讲中，我介绍了弹性分布式数据集的特性和它支持的各种数据操作。
不过在实际的开发过程中，我们并不是总需要在 RDD 的层次进行编程。
就好比编程刚发明的年代，工程师只能用汇编语言，到后来才慢慢发展出高级语言，如 Basic、C、Java 等。使用高级语言大大提升了开发者的效率。">

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Docs</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Docs</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">15_Spark_SQL：Spark数据查询的利器</h1>

      <div class="post-meta">
        <span class="post-time"> 10100-01-10 </span>
        <div class="post-category">
            <a href="/categories/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/"> 极客时间 </a>
            </div>
          <span class="more-meta"> 约 3099 字 </span>
          <span class="more-meta"> 预计阅读 7 分钟 </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#spark-sql-发展历史">Spark SQL 发展历史</a></li>
        <li><a href="#spark-sql-的架构">Spark SQL 的架构</a></li>
        <li><a href="#dataset">DataSet</a></li>
        <li><a href="#dataframe">DataFrame</a></li>
        <li><a href="#rdddataframedataset-对比">RDD、DataFrame、DataSet 对比</a>
          <ul>
            <li><a href="#发展历史">发展历史</a></li>
            <li><a href="#不变性与分区">不变性与分区</a></li>
            <li><a href="#性能">性能</a></li>
            <li><a href="#错误检测">错误检测</a></li>
          </ul>
        </li>
        <li><a href="#小结">小结</a></li>
        <li><a href="#思考题">思考题</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p>你好，我是蔡元楠。</p>
<p>上一讲中，我介绍了弹性分布式数据集的特性和它支持的各种数据操作。</p>
<p>不过在实际的开发过程中，我们并不是总需要在 RDD 的层次进行编程。</p>
<p>就好比编程刚发明的年代，工程师只能用汇编语言，到后来才慢慢发展出高级语言，如 Basic、C、Java 等。使用高级语言大大提升了开发者的效率。</p>
<p>同样的，Spark 生态系统也提供很多库，让我们在不同的场景中使用。</p>
<p>今天，让我们来一起探讨 Spark 最常用的数据查询模块——Spark SQL。</p>
<h2 id="spark-sql-发展历史">Spark SQL 发展历史</h2>
<p>几年前，Hadoop/MapReduce 在企业生产中的大量使用，HDFS 上积累了大量数据。</p>
<p>由于 MapReduce 对于开发者而言使用难度较大，大部分开发人员最熟悉的还是传统的关系型数据库。</p>
<p>为了方便大多数开发人员使用 Hadoop，Hive 应运而生。</p>
<p>Hive 提供类似 SQL 的编程接口，HQL 语句经过语法解析、逻辑计划、物理计划转化成 MapReduce 程序执行，使得开发人员很容易对 HDFS 上存储的数据进行查询和分析。</p>
<p>在 Spark 刚问世的时候，Spark 团队也开发了一个 Shark 来支持用 SQL 语言来查询 Spark 的数据。</p>
<p>Shark 的本质就是 Hive，它修改了 Hive 的内存管理模块，大幅优化了运行速度，是 Hive 的 10 倍到 100 倍之多。</p>
<p>但是，Shark 对于 Hive 的依赖严重影响了 Spark 的发展。Spark 想要定义的是一个统一的技术栈和完整的生态，不可能允许有这样的外在依赖。</p>
<p>试想，如果 Spark 想发布新的功能还需要等 Hive 的更新，那么势必会很难执行。此外，依赖于 Hive 还制约了 Spark 各个组件的相互集成，Shark 也无法利用 Spark 的特性进行深度优化。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E6%88%98/17c0863ad7c9035e417b42941c606815.png" alt=""></p>
<p>所以，2014 年 7 月 1 日，Spark 团队就将 Shark 交给 Hive 进行管理，转而开发了 SparkSQL。</p>
<p>SparkSQL 摒弃了 Shark 的（将 SQL 语句转化为 Spark RDD 的）执行引擎，换成自己团队重新开发的执行引擎。</p>
<p>Spark SQL 不仅将关系型数据库的处理模式和 Spark 的函数式编程相结合，还兼容多种数据格式，包括 Hive、RDD、JSON 文件、CSV 文件等。</p>
<p>可以说，Spark SQL 的问世大大加快了 Spark 生态的发展。</p>
<h2 id="spark-sql-的架构">Spark SQL 的架构</h2>
<p>Spark SQL 本质上是一个库。它运行在 Spark 的核心执行引擎之上。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E6%88%98/32ba006042626d2e44d3d15a0a085bfc.png" alt=""></p>
<p>如上图所示，它提供类似于 SQL 的操作接口，允许数据仓库应用程序直接获取数据，允许使用者通过命令行操作来交互地查询数据，还提供两个 API：DataFrame API 和 DataSet API。</p>
<p>Java、Python 和 Scala 的应用程序可以通过这两个 API 来读取和写入 RDD。</p>
<p>此外，正如我们在上一讲介绍的，应用程序还可以直接操作 RDD。</p>
<p>使用 Spark SQL 会让开发者觉得好像是在操作一个关系型数据库一样，而不是在操作 RDD。这是它优于原生的 RDD API 的地方。</p>
<p>与基本的 Spark RDD API 不同，Spark SQL 提供的接口为 Spark 提供了关于数据结构和正在执行的计算的更多信息。</p>
<p>在内部，Spark SQL 使用这些额外的信息来执行额外的优化。虽然 Spark SQL 支持多种交互方式，但是在计算结果时均使用相同的执行引擎。</p>
<p>这种统一意味着开发人员可以轻松地在不同的 API 之间来回切换，基于这些 API 提供了表达给定转换的最自然的方式。</p>
<p>接下来让我们进一步了解 DataSet 和 DataFrame。</p>
<h2 id="dataset">DataSet</h2>
<p>DataSet，顾名思义，就是数据集的意思，它是 Spark 1.6 新引入的接口。</p>
<p>同弹性分布式数据集类似，DataSet 也是不可变分布式的数据单元，它既有与 RDD 类似的各种转换和动作函数定义，而且还享受 Spark SQL 优化过的执行引擎，使得数据搜索效率更高。</p>
<p>DataSet 支持的转换和动作也和 RDD 类似，比如 map、filter、select、count、show 及把数据写入文件系统中。</p>
<p>同样地，DataSet 上的转换操作也不会被立刻执行，只是先生成新的 DataSet，只有当遇到动作操作，才会把之前的转换操作一并执行，生成结果。</p>
<p>所以，DataSet 的内部结构包含了逻辑计划，即生成该数据集所需要的运算。</p>
<p>当动作操作执行时，Spark SQL 的查询优化器会优化这个逻辑计划，并生成一个可以分布式执行的、包含分区信息的物理计划。</p>
<p>那么，DataSet 和 RDD 的区别是什么呢？</p>
<p>通过之前的叙述，我们知道 DataSet API 是 Spark SQL 的一个组件。那么，你应该能很容易地联想到，DataSet 也具有关系型数据库中表的特性。</p>
<p>是的，DataSet 所描述的数据都被组织到有名字的列中，就像关系型数据库中的表一样。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E6%88%98/75067fe7eac8e9e0e0578ca4c6a4f454.png" alt=""></p>
<p>如上图所示，左侧的 RDD 虽然以 People 为类型参数，但 Spark 框架本身不了解 People 类的内部结构。所有的操作都以 People 为单位执行。</p>
<p>而右侧的 DataSet 却提供了详细的结构信息与每列的数据类型。</p>
<p>这让 Spark SQL 可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。也就是说，DataSet 提供数据表的 schema 信息。这样的结构使得 DataSet API 的执行效率更高。</p>
<p>试想，如果我们要查询 People 的年龄信息，Spark SQL 执行的时候可以依靠查询优化器仅仅把需要的那一列取出来，其他列的信息根本就不需要去读取了。所以，有了这些信息以后在编译的时候能够做更多的优化。</p>
<p>其次，由于 DataSet 存储了每列的数据类型。所以，在程序编译时可以执行类型检测。</p>
<h2 id="dataframe">DataFrame</h2>
<p>DataFrame 可以被看作是一种特殊的 DataSet。它也是关系型数据库中表一样的结构化存储机制，也是分布式不可变的数据结构。</p>
<p>但是，它的每一列并不存储类型信息，所以在编译时并不能发现类型错误。DataFrame 每一行的类型固定为 Row，他可以被当作 DataSet[Row] 来处理，我们必须要通过解析才能获取各列的值。</p>
<p>所以，对于 DataSet 我们可以用类似 people.name 来访问一个人的名字，而对于 DataFrame 我们一定要用类似 people.get As [String] (“name”) 来访问。</p>
<h2 id="rdddataframedataset-对比">RDD、DataFrame、DataSet 对比</h2>
<p>学习 Spark 到现在，我们已经接触了三种基本的数据结构：RDD、DataFrame 和 DataSet。接下来你的表格中，你可以看到它们的异同点，思考一下怎样在实际工程中选择。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E6%88%98/f39d375985ad186b994d8b0b0e9538db.png" alt=""></p>
<h3 id="发展历史">发展历史</h3>
<p>从发展历史上来看，RDD API 在第一代 Spark 中就存在，是整个 Spark 框架的基石。</p>
<p>接下来，为了方便熟悉关系型数据库和 SQL 的开发人员使用，在 RDD 的基础上，Spark 创建了 DataFrame API。依靠它，我们可以方便地对数据的列进行操作。</p>
<p>DataSet 最早被加入 Spark SQL 是在 Spark 1.6，它在 DataFrame 的基础上添加了对数据的每一列的类型的限制。</p>
<p>在 Spark 2.0 中，DataFrame 和 DataSet 被统一。DataFrame 作为 DataSet[Row] 存在。在弱类型的语言，如 Python 中，DataFrame API 依然存在，但是在 Java 中，DataFrame API 已经不复存在了。</p>
<h3 id="不变性与分区">不变性与分区</h3>
<p>由于 DataSet 和 DataFrame 都是基于 RDD 的，所以它们都拥有 RDD 的基本特性，在此不做赘述。而且我们可以通过简单的 API 在 DataFrame 或 Dataset 与 RDD 之间进行无缝切换。</p>
<h3 id="性能">性能</h3>
<p>DataFrame 和 DataSet 的性能要比 RDD 更好。</p>
<p>Spark 程序运行时，Spark SQL 中的查询优化器会对语句进行分析，并生成优化过的 RDD 在底层执行。</p>
<p>举个例子，如果我们想先对一堆数据进行 GroupBy 再进行 Filter 操作，这无疑是低效的，因为我们并不需要对所有数据都 GroupBy。</p>
<p>如果用 RDD API 实现这一语句，在执行时它只会机械地按顺序执行。而如果用 DataFrame/DataSet API，Spark SQL 的 Catalyst 优化器会将 Filter 操作和 GroupBy 操作调换顺序，从而提高执行效率。</p>
<p>下图反映了这一优化过程。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E6%88%98/656b0226cc1a91d70943dd6942e82ea4.png" alt=""></p>
<h3 id="错误检测">错误检测</h3>
<p>RDD 和 DataSet 都是类型安全的，而 DataFrame 并不是类型安全的。这是因为它不存储每一列的信息如名字和类型。</p>
<p>使用 DataFrame API 时，我们可以选择一个并不存在的列，这个错误只有在代码被执行时才会抛出。如果使用 DataSet API，在编译时就会检测到这个错误。</p>
<h2 id="小结">小结</h2>
<p>DataFrame 和 DataSet 是 Spark SQL 提供的基于 RDD 的结构化数据抽象。</p>
<p>它既有 RDD 不可变、分区、存储依赖关系等特性，又拥有类似于关系型数据库的结构化信息。</p>
<p>所以，基于 DataFrame 和 DataSet API 开发出的程序会被自动优化，使得开发人员不需要操作底层的 RDD API 来进行手动优化，大大提升开发效率。</p>
<p>但是 RDD API 对于非结构化的数据处理有独特的优势，比如文本流数据，而且更方便我们做底层的操作。所以在开发中，我们还是需要根据实际情况来选择使用哪种 API。</p>
<h2 id="思考题">思考题</h2>
<p>什么场景适合使用 DataFrame API，什么场景适合使用 DataSet API？</p>
<p>欢迎你把答案写在留言区，与我和其他同学一起讨论。</p>
<p>如果你觉得有所收获，也欢迎把文章分享给你的朋友。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E6%88%98/a0f5b0b51dbfc98740637837dc0ae117.png" alt="unpreview"></p>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content"></span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
        10100-01-10
        
    </span>
  </p>
  
  
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E6%88%98/">大规模数据处理实战</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%9845%E8%AE%B2/15_linux%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%8B%E4%BB%8E_start%E5%88%B0%E7%AC%AC%E4%B8%80%E4%B8%AA%E8%BF%9B%E7%A8%8B/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">15_Linux初始化（下）：从_start到第一个进程</span>
            <span class="prev-text nav-mobile">上一篇</span>
          </a>
        <a class="next" href="/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E4%BB%8E0%E6%89%93%E9%80%A0%E9%9F%B3%E8%A7%86%E9%A2%91%E7%9B%B4%E6%92%AD%E7%B3%BB%E7%BB%9F/15_webrtc%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%9F%E8%AE%A1%E5%8E%9F%E6%9D%A5%E8%BF%99%E4%B9%88%E5%BC%BA%E5%A4%A7%E4%B8%8A/">
            <span class="next-text nav-default">15_WebRTC中的数据统计原来这么强大（上）</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
  
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2024 - 
    2025<span class="heart"><i class="iconfont icon-heart"></i></span><span></span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  <script src="/lib/highlight/highlight.pack.js?v=20171001"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js"></script>


      <script async src="https://www.googletagmanager.com/gtag/js?id=G-FVZ07KBD4X"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-FVZ07KBD4X');
        }
      </script>






</body>
</html>
