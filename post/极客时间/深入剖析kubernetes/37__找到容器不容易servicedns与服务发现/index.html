<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>37__找到容器不容易：Service、DNS与服务发现 - Docs</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="" /><meta name="description" content="你好，我是张磊。今天我和你分享的主题是：找到容器不容易之 Service、DNS 与服务发现。
在前面的文章中，我们已经多次使用到了 Service 这个 Kubernetes 里重要的服务对象。而 Kubernetes 之所以需要 Service，一方面是因为 Pod 的 IP 不是固定的，另一方面则是因为一组 Pod 实例之间总会有负载均衡的需求。
" /><meta name="keywords" content="技术文档, docs, 极客时间" />






<meta name="generator" content="Hugo 0.140.2 with theme even" />


<link rel="canonical" href="https://politcloud.org/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90kubernetes/37__%E6%89%BE%E5%88%B0%E5%AE%B9%E5%99%A8%E4%B8%8D%E5%AE%B9%E6%98%93servicedns%E4%B8%8E%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:url" content="https://politcloud.org/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90kubernetes/37__%E6%89%BE%E5%88%B0%E5%AE%B9%E5%99%A8%E4%B8%8D%E5%AE%B9%E6%98%93servicedns%E4%B8%8E%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0/">
  <meta property="og:site_name" content="Docs">
  <meta property="og:title" content="37__找到容器不容易：Service、DNS与服务发现">
  <meta property="og:description" content="你好，我是张磊。今天我和你分享的主题是：找到容器不容易之 Service、DNS 与服务发现。
在前面的文章中，我们已经多次使用到了 Service 这个 Kubernetes 里重要的服务对象。而 Kubernetes 之所以需要 Service，一方面是因为 Pod 的 IP 不是固定的，另一方面则是因为一组 Pod 实例之间总会有负载均衡的需求。">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2024-01-10T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-01-10T00:00:00+00:00">
    <meta property="article:tag" content="深入剖析Kubernetes">

  <meta itemprop="name" content="37__找到容器不容易：Service、DNS与服务发现">
  <meta itemprop="description" content="你好，我是张磊。今天我和你分享的主题是：找到容器不容易之 Service、DNS 与服务发现。
在前面的文章中，我们已经多次使用到了 Service 这个 Kubernetes 里重要的服务对象。而 Kubernetes 之所以需要 Service，一方面是因为 Pod 的 IP 不是固定的，另一方面则是因为一组 Pod 实例之间总会有负载均衡的需求。">
  <meta itemprop="datePublished" content="2024-01-10T00:00:00+00:00">
  <meta itemprop="dateModified" content="2024-01-10T00:00:00+00:00">
  <meta itemprop="wordCount" content="4091">
  <meta itemprop="keywords" content="深入剖析Kubernetes">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="37__找到容器不容易：Service、DNS与服务发现">
  <meta name="twitter:description" content="你好，我是张磊。今天我和你分享的主题是：找到容器不容易之 Service、DNS 与服务发现。
在前面的文章中，我们已经多次使用到了 Service 这个 Kubernetes 里重要的服务对象。而 Kubernetes 之所以需要 Service，一方面是因为 Pod 的 IP 不是固定的，另一方面则是因为一组 Pod 实例之间总会有负载均衡的需求。">

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Docs</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Docs</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">37__找到容器不容易：Service、DNS与服务发现</h1>

      <div class="post-meta">
        <span class="post-time"> 10100-01-10 </span>
        <div class="post-category">
            <a href="/categories/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/"> 极客时间 </a>
            </div>
          <span class="more-meta"> 约 4091 字 </span>
          <span class="more-meta"> 预计阅读 9 分钟 </span>
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">文章目录</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#总结">总结</a></li>
        <li><a href="#思考题">思考题</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p>你好，我是张磊。今天我和你分享的主题是：找到容器不容易之 Service、DNS 与服务发现。</p>
<p>在前面的文章中，我们已经多次使用到了 Service 这个 Kubernetes 里重要的服务对象。而 Kubernetes 之所以需要 Service，一方面是因为 Pod 的 IP 不是固定的，另一方面则是因为一组 Pod 实例之间总会有负载均衡的需求。</p>
<p>一个最典型的 Service 定义，如下所示：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">apiVersion: v1
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">kind: Service
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">metadata:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  name: hostnames
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">spec:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  selector:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    app: hostnames
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  ports:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  - name: default
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    protocol: TCP
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    port: 80
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    targetPort: 9376
</span></span></code></pre></td></tr></table>
</div>
</div><p>这个 Service 的例子，相信你不会陌生。其中，我使用了 selector 字段来声明这个 Service 只代理携带了 app=hostnames 标签的 Pod。并且，这个 Service 的 80 端口，代理的是 Pod 的 9376 端口。</p>
<p>然后，我们的应用的 Deployment，如下所示：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">apiVersion: apps/v1
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">kind: Deployment
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">metadata:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  name: hostnames
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">spec:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  selector:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    matchLabels:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      app: hostnames
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  replicas: 3
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  template:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    metadata:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      labels:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        app: hostnames
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    spec:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      containers:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      - name: hostnames
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        image: k8s.gcr.io/serve_hostname
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        ports:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        - containerPort: 9376
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">          protocol: TCP
</span></span></code></pre></td></tr></table>
</div>
</div><p>这个应用的作用，就是每次访问 9376 端口时，返回它自己的 hostname。</p>
<p>而被 selector 选中的 Pod，就称为 Service 的 Endpoints，你可以使用 kubectl get ep 命令看到它们，如下所示：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ kubectl get endpoints hostnames
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">NAME        ENDPOINTS
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">hostnames   10.244.0.5:9376,10.244.0.6:9376,10.244.0.7:9376
</span></span></code></pre></td></tr></table>
</div>
</div><p>需要注意的是，只有处于 Running 状态，且 readinessProbe 检查通过的 Pod，才会出现在 Service 的 Endpoints 列表里。并且，当某一个 Pod 出现问题时，Kubernetes 会自动把它从 Service 里摘除掉。</p>
<p>而此时，通过该 Service 的 VIP 地址 10.0.1.175，你就可以访问到它所代理的 Pod 了：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ kubectl get svc hostnames
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">NAME        TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">hostnames   ClusterIP   10.0.1.175   &lt;none&gt;        80/TCP    5s
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> $ curl 10.0.1.175:80
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">hostnames-0uton
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> $ curl 10.0.1.175:80
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">hostnames-yp2kp
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> $ curl 10.0.1.175:80
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">hostnames-bvc05
</span></span></code></pre></td></tr></table>
</div>
</div><p>这个 VIP 地址是 Kubernetes 自动为 Service 分配的。而像上面这样，通过三次连续不断地访问 Service 的 VIP 地址和代理端口 80，它就为我们依次返回了三个 Pod 的 hostname。这也正印证了 Service 提供的是 Round Robin 方式的负载均衡。对于这种方式，我们称为：ClusterIP 模式的 Service。</p>
<p>你可能一直比较好奇，Kubernetes 里的 Service 究竟是如何工作的呢？</p>
<p>实际上，<strong>Service 是由 kube-proxy 组件，加上 iptables 来共同实现的。</strong></p>
<p>举个例子，对于我们前面创建的名叫 hostnames 的 Service 来说，一旦它被提交给 Kubernetes，那么 kube-proxy 就可以通过 Service 的 Informer 感知到这样一个 Service 对象的添加。而作为对这个事件的响应，它就会在宿主机上创建这样一条 iptables 规则（你可以通过 iptables-save 看到它），如下所示：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">-A KUBE-SERVICES -d 10.0.1.175/32 -p tcp -m comment --comment &#34;default/hostnames: cluster IP&#34; -m tcp --dport 80 -j KUBE-SVC-NWV5X2332I4OT4T3
</span></span></code></pre></td></tr></table>
</div>
</div><p>可以看到，这条 iptables 规则的含义是：凡是目的地址是 10.0.1.175、目的端口是 80 的 IP 包，都应该跳转到另外一条名叫 KUBE-SVC-NWV5X2332I4OT4T3</p>
<p>的 iptables 链进行处理。</p>
<p>而我们前面已经看到，10.0.1.175 正是这个 Service 的 VIP。所以这一条规则，就为这个 Service 设置了一个固定的入口地址。并且，由于 10.0.1.175 只是一条 iptables 规则上的配置，并没有真正的网络设备，所以你 ping 这个地址，是不会有任何响应的。</p>
<p>那么，我们即将跳转到的 KUBE-SVC-NWV5X2332I4OT4T3 规则，又有什么作用呢？</p>
<p>实际上，它是一组规则的集合，如下所示：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment &#34;default/hostnames:&#34; -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-WNBA2IHDGP2BOBGZ
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment &#34;default/hostnames:&#34; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-X3P2623AGDH6CDF3
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment &#34;default/hostnames:&#34; -j KUBE-SEP-57KPRZ3JQVENLNBR
</span></span></code></pre></td></tr></table>
</div>
</div><p>可以看到，这一组规则，实际上是一组随机模式（–mode random）的 iptables 链。</p>
<p>而随机转发的目的地，分别是 KUBE-SEP-WNBA2IHDGP2BOBGZ、KUBE-SEP-X3P2623AGDH6CDF3 和 KUBE-SEP-57KPRZ3JQVENLNBR。</p>
<p>而这三条链指向的最终目的地，其实就是这个 Service 代理的三个 Pod。所以这一组规则，就是 Service 实现负载均衡的位置。</p>
<p>需要注意的是，iptables 规则的匹配是从上到下逐条进行的，所以为了保证上述三条规则每条被选中的概率都相同，我们应该将它们的 probability 字段的值分别设置为 1/3（0.333…）、1/2 和 1。</p>
<p>这么设置的原理很简单：第一条规则被选中的概率就是 1/3；而如果第一条规则没有被选中，那么这时候就只剩下两条规则了，所以第二条规则的 probability 就必须设置为 1/2；类似地，最后一条就必须设置为 1。</p>
<p>你可以想一下，如果把这三条规则的 probability 字段的值都设置成 1/3，最终每条规则被选中的概率会变成多少。</p>
<p>通过查看上述三条链的明细，我们就很容易理解 Service 进行转发的具体原理了，如下所示：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">-A KUBE-SEP-57KPRZ3JQVENLNBR -s 10.244.3.6/32 -m comment --comment &#34;default/hostnames:&#34; -j MARK --set-xmark 0x00004000/0x00004000
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">-A KUBE-SEP-57KPRZ3JQVENLNBR -p tcp -m comment --comment &#34;default/hostnames:&#34; -m tcp -j DNAT --to-destination 10.244.3.6:9376
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> -A KUBE-SEP-WNBA2IHDGP2BOBGZ -s 10.244.1.7/32 -m comment --comment &#34;default/hostnames:&#34; -j MARK --set-xmark 0x00004000/0x00004000
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">-A KUBE-SEP-WNBA2IHDGP2BOBGZ -p tcp -m comment --comment &#34;default/hostnames:&#34; -m tcp -j DNAT --to-destination 10.244.1.7:9376
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> -A KUBE-SEP-X3P2623AGDH6CDF3 -s 10.244.2.3/32 -m comment --comment &#34;default/hostnames:&#34; -j MARK --set-xmark 0x00004000/0x00004000
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">-A KUBE-SEP-X3P2623AGDH6CDF3 -p tcp -m comment --comment &#34;default/hostnames:&#34; -m tcp -j DNAT --to-destination 10.244.2.3:9376
</span></span></code></pre></td></tr></table>
</div>
</div><p>可以看到，这三条链，其实是三条 DNAT 规则。但在 DNAT 规则之前，iptables 对流入的 IP 包还设置了一个“标志”（–set-xmark）。这个“标志”的作用，我会在下一篇文章再为你讲解。</p>
<p>而 DNAT 规则的作用，就是在 PREROUTING 检查点之前，也就是在路由之前，将流入 IP 包的目的地址和端口，改成–to-destination 所指定的新的目的地址和端口。可以看到，这个目的地址和端口，正是被代理 Pod 的 IP 地址和端口。</p>
<p>这样，访问 Service VIP 的 IP 包经过上述 iptables 处理之后，就已经变成了访问具体某一个后端 Pod 的 IP 包了。不难理解，这些 Endpoints 对应的 iptables 规则，正是 kube-proxy 通过监听 Pod 的变化事件，在宿主机上生成并维护的。</p>
<p>以上，就是 Service 最基本的工作原理。</p>
<p>此外，你可能已经听说过，Kubernetes 的 kube-proxy 还支持一种叫作 IPVS 的模式。这又是怎么一回事儿呢？</p>
<p>其实，通过上面的讲解，你可以看到，kube-proxy 通过 iptables 处理 Service 的过程，其实需要在宿主机上设置相当多的 iptables 规则。而且，kube-proxy 还需要在控制循环里不断地刷新这些规则来确保它们始终是正确的。</p>
<p>不难想到，当你的宿主机上有大量 Pod 的时候，成百上千条 iptables 规则不断地被刷新，会大量占用该宿主机的 CPU 资源，甚至会让宿主机“卡”在这个过程中。所以说，<strong>一直以来，基于 iptables 的 Service 实现，都是制约 Kubernetes 项目承载更多量级的 Pod 的主要障碍。</strong></p>
<p>而 IPVS 模式的 Service，就是解决这个问题的一个行之有效的方法。</p>
<p>IPVS 模式的工作原理，其实跟 iptables 模式类似。当我们创建了前面的 Service 之后，kube-proxy 首先会在宿主机上创建一个虚拟网卡（叫作：kube-ipvs0），并为它分配 Service VIP 作为 IP 地址，如下所示：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"># ip addr
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  ...
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  73：kube-ipvs0：&lt;BROADCAST,NOARP&gt;  mtu 1500 qdisc noop state DOWN qlen 1000
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  link/ether  1a:ce:f5:5f:c1:4d brd ff:ff:ff:ff:ff:ff
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  inet 10.0.1.175/32  scope global kube-ipvs0
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  valid_lft forever  preferred_lft forever
</span></span></code></pre></td></tr></table>
</div>
</div><p>而接下来，kube-proxy 就会通过 Linux 的 IPVS 模块，为这个 IP 地址设置三个 IPVS 虚拟主机，并设置这三个虚拟主机之间使用轮询模式 (rr) 来作为负载均衡策略。我们可以通过 ipvsadm 查看到这个设置，如下所示：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"># ipvsadm -ln
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> IP Virtual Server version 1.2.1 (size=4096)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  Prot LocalAddress:Port Scheduler Flags
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    -&gt;  RemoteAddress:Port           Forward  Weight ActiveConn InActConn     
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  TCP  10.102.128.4:80 rr
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    -&gt;  10.244.3.6:9376    Masq    1       0          0         
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    -&gt;  10.244.1.7:9376    Masq    1       0          0
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    -&gt;  10.244.2.3:9376    Masq    1       0          0
</span></span></code></pre></td></tr></table>
</div>
</div><p>可以看到，这三个 IPVS 虚拟主机的 IP 地址和端口，对应的正是三个被代理的 Pod。</p>
<p>这时候，任何发往 10.102.128.4:80 的请求，就都会被 IPVS 模块转发到某一个后端 Pod 上了。</p>
<p>而相比于 iptables，IPVS 在内核中的实现其实也是基于 Netfilter 的 NAT 模式，所以在转发这一层上，理论上 IPVS 并没有显著的性能提升。但是，IPVS 并不需要在宿主机上为每个 Pod 设置 iptables 规则，而是把对这些“规则”的处理放到了内核态，从而极大地降低了维护这些规则的代价。这也正印证了我在前面提到过的，“将重要操作放入内核态”是提高性能的重要手段。</p>
<blockquote>
<p>备注：这里你可以再回顾下第 33 篇文章<a href="./65287.md">《深入解析容器跨主机网络》</a>中的相关内容。</p>
</blockquote>
<p>不过需要注意的是，IPVS 模块只负责上述的负载均衡和代理功能。而一个完整的 Service 流程正常工作所需要的包过滤、SNAT 等操作，还是要靠 iptables 来实现。只不过，这些辅助性的 iptables 规则数量有限，也不会随着 Pod 数量的增加而增加。</p>
<p>所以，在大规模集群里，我非常建议你为 kube-proxy 设置–proxy-mode=ipvs 来开启这个功能。它为 Kubernetes 集群规模带来的提升，还是非常巨大的。</p>
<p><strong>此外，我在前面的文章中还介绍过 Service 与 DNS 的关系。</strong></p>
<p>在 Kubernetes 中，Service 和 Pod 都会被分配对应的 DNS A 记录（从域名解析 IP 的记录）。</p>
<p>对于 ClusterIP 模式的 Service 来说（比如我们上面的例子），它的 A 记录的格式是：..svc.cluster.local。当你访问这条 A 记录的时候，它解析到的就是该 Service 的 VIP 地址。</p>
<p>而对于指定了 clusterIP=None 的 Headless Service 来说，它的 A 记录的格式也是：..svc.cluster.local。但是，当你访问这条 A 记录的时候，它返回的是所有被代理的 Pod 的 IP 地址的集合。当然，如果你的客户端没办法解析这个集合的话，它可能会只会拿到第一个 Pod 的 IP 地址。</p>
<p>此外，对于 ClusterIP 模式的 Service 来说，它代理的 Pod 被自动分配的 A 记录的格式是：..pod.cluster.local。这条记录指向 Pod 的 IP 地址。</p>
<p>而对 Headless Service 来说，它代理的 Pod 被自动分配的 A 记录的格式是：&hellip;svc.cluster.local。这条记录也指向 Pod 的 IP 地址。</p>
<p>但如果你为 Pod 指定了 Headless Service，并且 Pod 本身声明了 hostname 和 subdomain 字段，那么这时候 Pod 的 A 记录就会变成：&lt;pod 的 hostname&gt;&hellip;svc.cluster.local，比如：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">apiVersion: v1
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">kind: Service
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">metadata:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  name: default-subdomain
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">spec:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  selector:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    name: busybox
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  clusterIP: None
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  ports:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  - name: foo
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    port: 1234
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    targetPort: 1234
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">---
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">apiVersion: v1
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">kind: Pod
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">metadata:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  name: busybox1
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  labels:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    name: busybox
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">spec:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  hostname: busybox-1
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  subdomain: default-subdomain
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  containers:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  - image: busybox
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    command:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      - sleep
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">      - &#34;3600&#34;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    name: busybox
</span></span></code></pre></td></tr></table>
</div>
</div><p>在上面这个 Service 和 Pod 被创建之后，你就可以通过 busybox-1.default-subdomain.default.svc.cluster.local 解析到这个 Pod 的 IP 地址了。</p>
<p>需要注意的是，在 Kubernetes 里，/etc/hosts 文件是单独挂载的，这也是为什么 kubelet 能够对 hostname 进行修改并且 Pod 重建后依然有效的原因。这跟 Docker 的 Init 层是一个原理。</p>
<h2 id="总结">总结</h2>
<p>在这篇文章里，我为你详细讲解了 Service 的工作原理。实际上，Service 机制，以及 Kubernetes 里的 DNS 插件，都是在帮助你解决同样一个问题，即：如何找到我的某一个容器？</p>
<p>这个问题在平台级项目中，往往就被称作服务发现，即：当我的一个服务（Pod）的 IP 地址是不固定的且没办法提前获知时，我该如何通过一个固定的方式访问到这个 Pod 呢？</p>
<p>而我在这里讲解的、ClusterIP 模式的 Service 为你提供的，就是一个 Pod 的稳定的 IP 地址，即 VIP。并且，这里 Pod 和 Service 的关系是可以通过 Label 确定的。</p>
<p>而 Headless Service 为你提供的，则是一个 Pod 的稳定的 DNS 名字，并且，这个名字是可以通过 Pod 名字和 Service 名字拼接出来的。</p>
<p>在实际的场景里，你应该根据自己的具体需求进行合理选择。</p>
<h2 id="思考题">思考题</h2>
<p>请问，Kubernetes 的 Service 的负载均衡策略，在 iptables 和 ipvs 模式下，都有哪几种？具体工作模式是怎样的？</p>
<p>感谢你的收听，欢迎你给我留言，也欢迎分享给更多的朋友一起阅读。</p>
<p><img src="https://raw.githubusercontent.com/epic1268/images/master/%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90Kubernetes/e870b7df0db49509e735e6becd4a9a9a.png" alt=""></p>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content"></span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">
        10100-01-10
        
    </span>
  </p>
  
  
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90kubernetes/">深入剖析Kubernetes</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B%E4%B9%8B%E7%BE%8E/37__%E9%81%87%E5%88%B0%E7%BA%BF%E4%B8%8A%E6%95%85%E9%9A%9C%E4%BD%A0%E5%92%8C%E9%AB%98%E6%89%8B%E7%9A%84%E5%B7%AE%E8%B7%9D%E5%9C%A8%E5%93%AA%E9%87%8C/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">37__遇到线上故障，你和高手的差距在哪里？</span>
            <span class="prev-text nav-mobile">上一篇</span>
          </a>
        <a class="next" href="/post/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/kafka%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98/37__%E4%B8%BB%E6%B5%81%E7%9A%84kafka%E7%9B%91%E6%8E%A7%E6%A1%86%E6%9E%B6/">
            <span class="next-text nav-default">37__主流的Kafka监控框架</span>
            <span class="next-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
  
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2024 - 
    2025<span class="heart"><i class="iconfont icon-heart"></i></span><span></span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  <script src="/lib/highlight/highlight.pack.js?v=20171001"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js"></script>


      <script async src="https://www.googletagmanager.com/gtag/js?id=G-FVZ07KBD4X"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-FVZ07KBD4X');
        }
      </script>






</body>
</html>
